---
title: "Was liest und schreibt man über Inklusion?"
subtitle: "Web-Scraping und Text-Mining mit R am Beispiel einer Online-Nachrichten- und Diskussionsseite für Lehrkräfte"
author: "Pawel R. Kulawiak"
date: last-modified
abstract: "Preprint in Progress"
abstract-title: "Status"
lang: de
bibliography: references.bib
theme: sandstone
toc: true
number-sections: true
toc-depth: 90
toc-expand: true
title-block-banner: "#2A5B60"
code-overflow: wrap
code-block-bg: "#F8F8F8"
code-block-border-left: "#769E3C"
mermaid-format: png
language: 
  title-block-author-single: "Autor"
  title-block-published: "Datum der letzten Veränderung"
format:
  html:
    embed-resources: false
    output-file: "index"
  #pdf:
    #include-in-header: 
      #text: |
        #\usepackage{lscape}
        #\newcommand{\blandscape}{\begin{landscape}}
        #\newcommand{\elandscape}{\end{landscape}}
    #header-includes: |
      #\titlehead{\includegraphics[width=6.5in]{images/owl.png}}
    #geometry:
      #- top=30mm
      #- left=30mm
      #- right=30mm
      #- bottom=30mm
      #- heightrounded
    #fig-pos: H
---

```{r}
#| echo: false
#| include: false
options(width = 75)
library(tidyverse)
library(rvest)
library(flextable)
```

![](images/owl.png){width="60%"}

{{< downloadthis text_mining.pdf dname=text_mining label="PDF-Download" icon=file-earmark-pdf type=success >}} 

\newpage

# Vorwort

TBA -

# Einleitung

Mein wertgeschätzter Kollege Timo Lüke[^1] hat einst im Rahmen einer Medieninhaltsanalyse deutschsprachiger Printmedien [@lüke2014] folgende Forschungsfragen aufgeworfen:

[^1]: <https://timolueke.de/>

-   Welches Verständnis von Inklusion wird in den deutschen meinungsführenden Medien kommuniziert?
-   Welche Argumente für und gegen die Umsetzung von Inklusion werden genannt?
-   Welche Fallbeispiele werden als Belege angeführt?

> *"Im Rahmen einer systematischen Inhaltsanalyse (Rössler, 2010) deutscher Printmedien untersuchen wir die öffentliche Berichterstattung zum Thema „Inklusion". Dabei wollen wir verbreitete Definitionen, Argumente und Fallbeispiele systematisch erfassen. So sollen langfristig die Analyse des medialen Diskurses und in der Folge eine Versachlichung der kontroversen Debatte über Inklusion ermöglicht werden."* [@lüke2014]

Erste Ergebnisse der Medieninhaltsanalyse sind in Form einer Posterpräsentation verfügbar [@lüke2014] und ich erlaube mir die Darstellung des interessanten Posters (@fig-poster).

::: column-body-outset-left
![Posterpräsentation von @lüke2014: Was liest man über Inklusion?](images/poster.png){#fig-poster fig-alt="Posterpräsentation von @lüke2014: Was liest man über Inklusion?"}
:::

# Ziele

## Allgemeine Zielsetzung

Ich möchte die Medieninhaltsanalyse von @lüke2014 replizieren sowie erweitern und mich dabei auf die Textinhalte einer Online-Nachrichten- und Diskussionsseite für Lehrkräfte fokussieren, nämlich News4teachers [@N4T_2022].

## Zielsetzung mit R: Web-Scraping und Text-Mining

Ich möchte exemplarisch aufzeigen, wie die einzelnen Projektphasen der Medieninhaltsanalyse mit der Programmiersprache R umgesetzt werden können. Hierfür werden wir uns auf zwei wichtige Arbeitsschritte fokussieren:

-   **Web-Scraping**, also eine automatisierte Methode zum Extrahieren der Textinformationen von der Webseite News4teachers. Eine Einführung in das Thema Web-Scraping mit R bieten @Wickham2023-hx [Kapitel 25]^[<https://r4ds.hadley.nz/webscraping>].

-   **Text-Mining**: Die mittels Web-Scraping gesammelten Textdaten sollen mit Methoden des Text-Minings analysiert werden. Methoden des Text-Minings fokussieren sich auf die Extraktion von nützlichen Informationen aus unstrukturierten Textdaten. Unstrukturierte Textdaten sind Texte, die nicht in einer festen Datenbankstruktur vorliegen, also z.B. Textinhalte von Webseiten. Mit Methoden des Text-Minings kann auch der sentimentale Ton eines Textinhalts bzw. die im Text vermittelte subjektive Meinung analysiert werden. Das Hauptziel der sogenannten Sentimentanalyse besteht also darin, die in einem Textdokument geäußerten Emotionen und Ansichten bezüglich eines bestimmten Themas zu identifizieren, in unserem Fall also z.B. geäußerte Meinungen zum Thema Inklusion. Eine Einführung in das Thema Text-Mining mit R bieten @Silge2017-sp.

# News4teachers: Online-Nachrichten- und Diskussionsseite für Lehrkräfte

Bevor wir mit dem Web-Scraping und Text-Mining beginnen, betrachten wir zunächst das Arbeitsmaterial, also die Webinhalte der Webseite News4teachers, und die entsprechende Selbstbeschreibung der Webseite [@N4T_2022]:

> "**Wer steckt hinter News4teachers?**
>
> News4teachers wird von einer Redaktion aus Lehrern und Journalisten betrieben. Die Seite ist ein gemeinsames Projekt von [4teachers](http://www.4teachers.de/), der Service-Plattform von Lehrern für Lehrer, sowie [der Agentur für Bildungsjournalismus](http://www.xn--agentur-fr-bildungsjournalismus-wid.de/).
>
> **Was ist News4teachers?**
>
> News4teachers ist eine Nachrichten- und Diskussionsseite, die sich mit seriösen Berichten, Analysen und Kommentaren an pädagogische Profis und die an Bildungsthemen interessierte Öffentlichkeit richtet. Die Redaktion sichtet täglich die Meldungen aus Politik, Forschung und Gesellschaft. Auf die Seite gelangt alles, was für die Bildung wichtig ist. News4teachers bietet also einen aktuellen Überblick über die relevanten Informationen für Lehrer, Erzieher, Schüler und Eltern. Und zwar: unabhängig und überparteilich.
>
> **Was ist die Idee hinter News4teachers?**
>
> News4teachers fühlt sich dem klassischen Journalismus verpflichtet. Das heißt konkret: Wir unterwerfen uns den publizistischen Grundsätzen des Deutschen Presserats, dem [Pressekodex](https://www.presserat.de/pressekodex/pressekodex/). Informationen, die auf die Seite gelangen, wurden zuvor von der Redaktion mit der gebotenen Sorgfalt geprüft. Quellen werden stets genannt, Meinung und Bericht voneinander getrennt. News4teachers unterliegt zudem einer Chronistenpflicht: Alles, was für die Bildungsdebatte in Deutschland von Bedeutung ist, wird aktuell berichtet. Regelmäßige Nutzer von News4teachers sind also immer im Bild." [@N4T_2022]

Die Redaktion besteht aus folgenden Personen [@N4T_impressum]: Anna Hückelheim, Sonja Mankowsky, Laura Millmann, Nina Odenius, Thomas Zab und Milla Priboschek (Podcast-Redaktion).

```{r}
#| echo: false
#| include: false
options(scipen=999)
BESUCHER <- 55000
LK <- 975000
```

## Inhalte von News4teachers und potenzielle Leserschaft aus Lehrkräften

News4teachers verspricht eine unabhängige und überparteiliche Berichterstattung zu Bildungsthemen, wahrscheinlich auch zum Thema Inklusion. Die Inhalte sind für die Leserschaft kostenfrei (werbefinanziertes Angebot). Die Inhalte von News4teachers sind außerdem speziell auf Lehrkräfte ausgerichtet. Somit kann angenommen werden, dass ein großer Teil der Leserschaft aus Lehrkräften besteht. Die Internetseite News4teachers hatte folgende Besucherzahlen (Jahr 2023): Mai (54000 Personen), Juni (60000 Personen) und Juli und August jeweils 55000 Personen (Zahlen ermittelt mit: <https://neilpatel.com/website-traffic-checker/>). Nehmen wir an, dass die Leserschaft von News4teachers zu 75% aus Lehrkräften aus Deutschland bestünde, dann hätten wir bei einer monatlichen Besucherzahl von `r BESUCHER` Personen eine monatliche Leserschaft von ca. `r BESUCHER * 0.75` Lehrkräften (`r BESUCHER` \* 0,75 = `r BESUCHER * 0.75`). In Deutschland gibt es aber laut Mikrozensus 2022 rund `r LK` Lehrkräfte an allgemeinbildenden Schulen [@census]. Die potenzielle News4teachers-Leserschaft aus Lehrkräften (`r BESUCHER * 0.75` Personen) entspräche dann einem Anteil von ca. `r (BESUCHER/LK*100) |> round(2)`% aller Lehrkräfte an allgemeinbildenden Schulen (`r BESUCHER` / `r LK` \* 100 = `r (BESUCHER/LK*100) |> round(2)`%). Im dargestellten Szenario würden die Inhalte von News4teachers also pro Monat ca. `r (BESUCHER/LK*100) |> round(2)`% der Lehrkräfte an allgemeinbildenden Schulen in Deutschland erreichen (5 von 100 Lehrkräften lesen News4teachers). Dies sind aber nur vage Vermutungen zur Reichweite von News4teachers unter Lehrkräften an allgemeinbildenden Schulen in Deutschland, unter der Annahme, dass 75% der Leserschaft von News4teachers aus Lehrkräften bestünde.

AUFGREIFEN: \[<https://www.news4teachers.de/2021/12/liebe-leserin-lieber-leser-ein-wort-zum-jahreswechsel-in-eigener-sache/>\]

### Kommentare und Diskussionen

Die Webseite News4teachers bieten der Leserschaft die Möglichkeit die Inhalte zu kommentieren und zu diskutieren (@fig-beitrag und @fig-struktur3). Hierfür formuliert die Redaktion spezifische Richtlinien [@N4T_2022]:

> "**Gibt's Regeln für die Leserzuschriften in den Foren?**
>
> Grundsätzlich gilt: Niemand hat einen Anspruch darauf, in den Foren zu den einzelnen Artikeln eine eigene Wortmeldung zu veröffentlichen. Die Redaktion legt Wert darauf, nur Leserzuschriften zu veröffentlichen, die erkennbar darauf abzielen, einen inhaltlichen Beitrag zur Diskussion des darüberstehenden Artikels zu leisten. Das bedeutet konkret: Auch für Leserzuschriften gelten die publizistischen Grundsätze des Deutschen Presserats, gilt also [der Pressekodex](https://www.presserat.de/pressekodex/pressekodex/).
>
> Kurzgefasst:
>
> -   Wir veröffentlichen keine Leserbeiträge, in denen ungeprüfte, unbelegte oder falsche Tatsachenbehauptungen verbreitet werden.
> -   Wir veröffentlichen keine Hetze gegen Menschen oder Menschengruppen.
> -   Wir veröffentlichen keine Werbung, ob nun für Produkte oder Parteien.
> -   Und wir veröffentlichen keine Links auf unseriöse Quellen.
>
> Wir behalten uns darüber hinaus vor, Leserbriefe, die lediglich der Stimmungsmache dienen, zu löschen. Oder Leserbriefe sinnwahrend zu kürzen." [@N4T_2022]

\[Hier weitere Erläuterungen einfügen\]

##### Facebook und Twitter

Die Beiträge werden aber nicht nur unmittelbar auf der Seite von News4teachers kommentiert und diskutiert (@fig-struktur3). Die Diskussion der Beiträge erfolgt auch auf einer externen Seite, nämlich bei Facebook. Innerhalb der Beiträge wird auch auf die externe Diskussion bei Facebook verwiesen (@fig-facebook). Außerdem werden die Beiträge von News4teachers ebenfalls bei Twitter geteilt und diskutiert (<https://twitter.com/News4teachers>). Die Kommentare und Diskussionen bei Facebook und Twitter sollen daher auch bei der vorliegenden Medieninhaltsanalyse Berücksichtigung finden. 

![Beitrag zum Thema Inklusion mit 152 Leserkommentaren auf der Internetseite News4teachers [@N4T_2023]](images/beitrag.png){#fig-beitrag fig-alt="Inhalt zum Thema Inklusion mit 152 Leserkommentaren auf der Internetseite News4teachers"}

![Verweis bei News4teachers auf die Diskussion bei Facebook](images/facebook.png){#fig-facebook fig-alt="Verweis bei News4teachers auf die Diskussion bei Facebook"}

# Explorative Forschungsfragen

Die Inhalte von der Webseite News4teachers und die Kommentare und Diskussionen der Leserschaft eignen sich eventuell zur Beantwortung folgender Forschungsfragen:

-   Auf welche Art und Weise wird das Thema Inklusion auf der Online-Nachrichten- und Diskussionsseite für Lehrkräfte dargestellt?
-   Auf welche Art und Weise werden die Inhalte zum Thema Inklusion von der Leserschaft kommentiert und diskutiert?

# Web-Scraping

Der erste Arbeitsschritt, hin zum Text-Mining, also hin zur Medieninhaltsanalyse, wird nun das Web-Scraping sein, also die automatisierte Extraktion der Webinhalte (z.B. Textinformationen) von der Webseite News4teachers. Traditionellerweise bzw. altmodischerweise würde man Webinhalte mit der Methode "*copy-and-paste*" in einen Datensatz übertragen, also z.B. Text von einer Webseite kopieren und anschließend die kopierte Textinformation in einen Datensatz einfügen (z.B. bei Excel). Dieses Verfahren ist aber fehleranfällig, da z.B. die Gefahr besteht, dass aufgrund mangelnder Konzentration falsche oder unvollständige Textinhalte übertragen werden. Web-Scraping ist daher als automatisierte Methode der Extraktion von Webinhalten weniger anfällig für Fehler und somit die Methode der Wahl. Eine Einführung in das Thema Web-Scraping mit R bieten @Wickham2023-hx [Kapitel 25]^[<https://r4ds.hadley.nz/webscraping>].

## R-Zusatzpakete

### R-Zusatzpaket *rvest*

Für das Web-Scraping nutzen wir nun das R-Zusatzpaket *rvest* [@rvest]. Der Name des R-Zusatzpaketes ist eine gelungene Anspielung auf das englische Wort *harvest* (ernten, sammeln), denn wir wollen ja Informationen aus dem Internet sammeln (mit R). Das kreative Wortspiel ist auch im Logo des R-Zusatzpaketes visualisiert (@fig-logo). Zunächst müssen wird das R-Zusatzpaket installieren und laden.

```{r}
#| eval: false
install.packages("rvest")
library(rvest)
```

![Logo des R-Zusatzpaketes *rvest*](images/logo.png){#fig-logo fig-alt="Logo des R-Zusatzpaketes rvest"}

Das R-Zusatzpaket *rvest* verfügt über eine umfassende und hilfreiche Online-Dokumentation:

-   <https://rvest.tidyverse.org/>
-   <https://r4ds.hadley.nz/webscraping> [@Wickham2023-hx, Kapitel 25]

### R-Zusatzpaket *tidyverse*

Das R-Zusatzpaket *tidyverse* [@tidyverse] ist eine Zusammenstellung unterschiedlicher R-Zusatzpakete. Auch das R-Zusatzpaket *rvest* ist Bestandteil des R-Zusatzpakets *tidyverse*. Wir werden an diversen Stellen die herausragende Funktionalität des R-Zusatzpaketes *tidyverse* nutzen. An den entsprechenden Stellen wird ein Verweis auf die R-Zusatzpakete erfolgen. Informationen zum R-Zusatzpaket *tidyverse* findet man hier:

<https://www.tidyverse.org/>

Wir installieren und laden das R-Zusatzpaket:

```{r}
#| eval: false
install.packages("tidyverse")
library(tidyverse)
```

## Struktur und Inhalte der Webseite

Ziel des Web-Scapings wird es sein, die relevanten Webinhalte von News4teachers automatisiert zu extrahieren. Hierfür müssen wir uns erstmal einen Überblick über die Struktur und Inhalte der Webseite verschaffen. Die Beiträge auf den Internetseiten von News4teachers haben eine spezifische Struktur mit spezifischen Webinhalten. Wir betrachten den Beitrag mit dem Titel *"„Schämt Euch!" -- Deutschland steht vor den Vereinten Nationen am Pranger, weil es die Inklusion an Schulen praktisch verweigert"* [@N4T_2023]. Für unsere Forschungsfragen mehr oder weniger interessante Webinhalte sind in den Abbildungen kenntlich gemacht (@fig-struktur1, @fig-struktur2 und @fig-struktur3).

![(a) Beitrag auf der Internetseite News4teachers und Struktur der Webinhalte [@N4T_2023]](images/struktur1.png){#fig-struktur1 fig-alt="Beitrag auf der Internetseite News4teachers und Struktur der Webinhalte"}

![(b) Beitrag auf der Internetseite News4teachers und Struktur der Webinhalte [@N4T_2023]](images/struktur2.png){#fig-struktur2 fig-alt="Beitrag auf der Internetseite News4teachers und Struktur der Webinhalte"}

![(c) Beitrag auf der Internetseite News4teachers und Struktur der Webinhalte [@N4T_2023]](images/struktur3.png){#fig-struktur3 fig-alt="Beitrag auf der Internetseite News4teachers und Struktur der Webinhalte"}

\[Erläuterungen zu den Abbildungen und Inhalten hinzufügen\]

## Erster Web-Scraping-Versuch

Zuvor haben wir uns einen Überblick über die zu extrahierenden Webinhalte verschafft. Für den ersten Web-Scraping-Versuch nutzen wir weiterhin den Beitrag mit dem Titel *"„Schämt Euch!" -- Deutschland steht vor den Vereinten Nationen am Pranger, weil es die Inklusion an Schulen praktisch verweigert"* (@fig-struktur1). Dies ist der Link zum Beitrag:

<https://www.news4teachers.de/2023/08/schaemt-euch-deutschland-steht-vor-den-vereinten-nationen-am-pranger-weil-es-die-inklusion-an-schulen-verweigert/>

Wir nutzen den Befehl `read_html()` und den entsprechenden Link, um sämtliche Informationen von der Webseite zu extrahieren:

```{r}
html <-
  read_html("https://www.news4teachers.de/2023/08/schaemt-euch-deutschland-steht-vor-den-vereinten-nationen-am-pranger-weil-es-die-inklusion-an-schulen-verweigert/")

#html <- read_html("https://www.news4teachers.de/2012/02/im-kern-sind-wir-uns-einig-kein-streit-mehr-uber-die-struktur/")

#html <- read_html("https://www.news4teachers.de/2013/03/studie-hochbegabte-sind-besser-unter-sich/")

#html <- read_html("https://www.news4teachers.de/2019/03/mobbing-ritual-unter-grundschuelern-bringt-politik-in-bewegung/")
```

Alle Webinhalte sind nun im Objekt `html` hinterlegt. Wir sind allerdings nur an spezifischen Webinhalten interessiert und möchten daher im nächsten Schritt einen spezifischen Textinhalt aus dem Objekt `html` auslesen. Beginnen wir mit einem Textinhalt, welcher sich relativ leicht extrahieren lässt. Wir wollen den Titel des Beitrages extrahieren: *"„Schämt Euch!" -- Deutschland steht vor den Vereinten Nationen am Pranger, weil es die Inklusion an Schulen praktisch verweigert"*. Dabei ist es gar nicht so leicht, einen spezifischen Inhalt wie den Titel zu lokalisieren und auszulesen. Hierfür ist HTML-[^4] und CSS-Selector-Grundlagenwissen[^5] hilfreich. Die eigentlichen Textinhalte sind nämlich im HTML-Dokument der Webseite hinterlegt (HTML-Quelltext). Ist eine Internetseite im Browser geöffnet, so gelangen wir mit einem Rechtsklick i.d.R. zur Option *"Seitenquelltext anzeigen"* (@fig-quelltext). Dies führt uns zum HTML-Dokument der Webseite (@fig-html1).

[^4]: <https://developer.mozilla.org/en-US/docs/Web/HTML>

[^5]: <https://developer.mozilla.org/en-US/docs/Learn/CSS/Building_blocks/Selectors>; *"CSS includes a miniature language for selecting elements on a page called CSS selectors. CSS selectors define patterns for locating HTML elements, and are useful for scraping because they provide a concise way of describing which elements you want to extract."*, Quelle: <https://rvest.tidyverse.org/articles/rvest.html>

![Seitenquelltext (HTML) anzeigen](images/quelltext.png){#fig-quelltext fig-alt="Seitenquelltext (HTML) anzeigen"}

::: column-body-outset-left
![HTML-Dokument/Seitenquelltext (Ausschnitt)](images/html1.png){#fig-html1 fig-alt="HTML-Dokument (Ausschnitt)"}
:::

Das HTML-Dokument (@fig-html1) ist riesig (mehr als 10000 Zeilen) und wir müssen etwas stöbern, um den passenden Webinhalt zu lokalisieren. Der HTML-Code aus @fig-html1 ist zwecks besserer Lesbarkeit auch nachfolgend dargestellt:

```{html}
<article id="post-132285" class="post-132285 post type-post status-publish format-standard has-post-thumbnail category-leben category-titelthema category-wissenschaft tag-forderschulen tag-inklusion tag-un-behindertenrechtskonvention" itemscope itemtype="https://schema.org/Article">
    <div class="td-post-header">
        <!-- category -->
        <ul class="td-category">
            <li class="entry-category"><a href="https://www.news4teachers.de/bildung/leben/">Leben</a></li>
            <li class="entry-category"><a href="https://www.news4teachers.de/bildung/titelthema/">Titelthema</a></li>
            <li class="entry-category"><a href="https://www.news4teachers.de/bildung/wissenschaft/">Wissen</a></li>
        </ul>
        <header class="td-post-title">
            <h1 class="entry-title">&#8222;Schämt Euch!&#8220; &#8211; Deutschland steht vor den Vereinten Nationen am Pranger, weil es die Inklusion an Schulen praktisch verweigert</h1>
            <div class="td-module-meta-info">
                <!-- author -->
                <!-- date -->
                <span class="td-post-date">
                    <time class="entry-date updated td-module-date" datetime="2023-08-29T12:46:06+02:00">29. August 2023</time>
                </span>
                <!-- comments -->
                <div class="td-post-comments">
                    <a href="https://www.news4teachers.de/2023/08/schaemt-euch-deutschland-steht-vor-den-vereinten-nationen-am-pranger-weil-es-die-inklusion-an-schulen-verweigert/#comments">
                        <i class="td-icon-comments"></i>150
                    </a>
                </div>
                <!-- views -->
            </div>
        </header>
    </div>
</article>
```

Wir sehen z.B. in der Zeile 1297 (@fig-html1), dass der Titel des Beitrages ein `h1`-HTML-Element[^6] ist (header 1: Überschrift erster Ebene):

[^6]: <https://developer.mozilla.org/en-US/docs/Web/HTML/Element/Heading_Elements>

```{html}
<h1 class="entry-title">&#8222;Schämt Euch!&#8220; &#8211; Deutschland steht vor den Vereinten Nationen am Pranger, weil es die Inklusion an Schulen praktisch verweigert</h1>
```

Diese Information benötigen wir, um den Titel des Beitrags gezielt auszulesen. Hierfür nutzen wir den Befehl `html_elements("h1")` und übergeben das Objekt `html` an diesen Befehl.

```{r}
html |> html_elements("h1")
```

Die Information `<h1 class="entry-title">` ist überflüßig, da wir nur am HTML-Textinhalt interessiert sind. Daher extrahieren wir den reinen Textinhalt, also den Titel, mit dem Befehl `html_text()`. Die Befehlskette wird entsprechend erweitert:

```{r}
html |>
  html_elements("h1") |>
  html_text()
```

Herzlichen Glückwunsch! 🥳 Somit haben wir erfolgreich alle Informationen von der Webseite extrahiert und eine relevante Textstelle (den Titel) ausgelesen.

### Datenstruktur

Im HTML-Seitenquelltext (@fig-html1) sehen wir, dass anscheinend jeder Beitrag über eine ID verfügt (`id="post-132285"`). Wenn wir in unserem zukünftigen Datensatz mehrere Beiträge abspeichern wollen, dann wird eine ID-Variable zwecks Unterscheidung der Beiträge eine hilfreiche Sache sein. @tbl-idee ist eine erste Idee bezüglich einer möglichen/sinnvollen Datenstruktur. Bei dieser Datenstruktur ignorieren wir der Einfachheit halber vorerst ein paar relevante Webinhalte, z.B. Kommentare und Anzahl der Likes ("*Gefällt mir*").

\newpage

\blandscape

```{r}
#| echo: false
#| label: tbl-idee
#| tbl-cap: Erste Idee bezüglich einer möglichen/sinnvollen Datenstruktur
ID <- c(132285, "...", "...")
link <- c("https://...", "...", "...")
datum <- c("29.\nAugust\n2023", "...", "...")
n_kommentare <- c("150", "...", "...")
ort <- c("GENF", "...", "...")
titel <- c("'Schämt Euch!' --\nDeutschland steht\nvor den Vereinten\nNationen am Pranger...", "...", "...")
zusammenfassung <- c("„Schämt Euch!“ --\nso heißt es auf\neinem Transparent...", "...", "...")
haupttext <- c("Der offizielle Beitrag\nDeutschlands fällt\ndünn aus...", "...", "...")
usw. <- c("...", "...", "...")

data.frame(ID, link, datum, n_kommentare, ort, titel, zusammenfassung, haupttext, usw.) |>
  flextable() |>
  #autofit() |>
  bold(part = "header") |>
  rotate(rotation = "tbrl", align = "bottom", part = "header") |>
  width(j = 1:5, width = 0.6) |>
  width(j = 2, width = 0.8) |>
  width(j = 6:8, width = 1.5) |>
  width(j = 9, width = 0.3) |>
  height(height = 5, part = "header")
```

\elandscape

[Gefällt mir???]

### Weitere Web-Scraping-Schritte {#sec-schritte}

Um die Datenstruktur aus @tbl-idee zu realisieren, müssen wir nun die ID des Beitrags, den Link, das Erscheinungsdatum, die Anzahl der Kommentare, die Zusammenfassung und den eigentlichen Haupttext des Beitrages auslesen (den Titel haben wir ja bereits erfolgreich extrahiert). Beginnen wir mit der ID.

#### ID {#sec-link}

In @fig-html1 sehen wir, das die ID des Beitrages (`id="post-132285"`) ein Attribut[^7] eines HTML-Elements ist (HTML-Element: `article`[^8]):

[^7]: <https://developer.mozilla.org/en-US/docs/Web/HTML/Global_attributes/id>

[^8]: <https://developer.mozilla.org/en-US/docs/Web/HTML/Element/article>

```{html}
<article id="post-132285" class="post-132285 post type-post status-publish format-standard has-post-thumbnail category-leben category-titelthema category-wissenschaft tag-forderschulen tag-inklusion tag-un-behindertenrechtskonvention" itemscope itemtype="https://schema.org/Article">
```

Daher übergeben wir das Objekt `html` zwecks Auslesung der ID zunächst an den Befehl `html_elements("article")` und dann an den Befehl `html_attr("id")`:

```{r}
html |>
  html_elements("article") |>
  html_attr("id")
```

Die ID des Beitrags erscheint mit dem Präfix `"post-"`, eine nicht notwendigerweise nützliche Information. Das Präfix entfernen wir daher mit dem Befehl `str_remove("post-")` und überführen die ID mit dem Befehl `as.numeric()` in ein nummerisches Format. Somit erhalten wir die nummerische ID `132285`:

```{r}
html |>
  html_elements("article") |>
  html_attr("id") |>
  str_remove("post-") |> # R-Zusatzpaket stringr (tidyverse)
  as.numeric()
```

#### Link

Der Beitrag verfügt über einen langen Link:

<https://www.news4teachers.de/2023/08/schaemt-euch-deutschland-steht-vor-den-vereinten-nationen-am-pranger-weil-es-die-inklusion-an-schulen-verweigert/>

Im HTML-Quelltext ist allerdings auch ein kurzer Link, also ein `shortlink`, ausgewiesen:

```{html}
<link rel='shortlink' href='https://www.news4teachers.de/?p=132285' />
```

Die ID des Beitrags (`132285`) ist Bestandteil des kurzen Links. Wir können also den ersten Teil des kurzen Links (`"https://www.news4teachers.de/?p="`) mit der ID (`132285`) verbinden, um den gewünschten Kurzlink zu generieren. Hierfür nutzen wir nach der Auslesung der ID den Befehl `paste0("https://www.news4teachers.de/?p=", .)`. Mit dem magrittr-Pipe-Operator (`%>%`[^9]) wird die ID an das zweite Argument des Befehls `paste0("https://www.news4teachers.de/?p=", .)` übergeben, also an die Stelle mit dem Punkt (`.`). Eine Übergabe an das zweite Argument wäre mit der sogenannten base-Pipe (`|>`) nicht möglich, daher nutzen wir die magrittr-Pipe (`%>%`). Die Befehlskette zur Erstellung des Links gestaltet sich somit folgendermaßen:

[^9]: <https://magrittr.tidyverse.org/>

```{r}
html |>
  html_elements("article") |>
  html_attr("id") |>
  str_remove("post-") |> # R-Zusatzpaket stringr (tidyverse)
  as.numeric() %>% # Pipe-Operator, R-Zusatzpaket magrittr (tidyverse)
  paste0("https://www.news4teachers.de/?p=", .) 
```

#### Erscheinungsdatum

Fahren wir fort mit dem Auslesen des Erscheinungsdatums des Beitrages. Im HTML-Quelltext (@fig-html1, Zeile 1301) erscheint folgende Information:

```{html}
<span class="td-post-date"><time class="entry-date updated td-module-date" datetime="2023-08-29T12:46:06+02:00" >29. August 2023</time></span> 
```

Wir sehen, dass das Datum ein HTML-Element ist, nämlich ein `time`-Element[^10]. Dieses `time`-Element ist innerhalb eines `span`-Elements[^11] geschachtelt. Wir können hier also von einer hierarchischen Schachtelung der HTML-Elemente sprechen (`span -> time`, @fig-schachtelung_1).

[^10]: <https://developer.mozilla.org/en-US/docs/Web/HTML/Element/time>

[^11]: <https://developer.mozilla.org/en-US/docs/Web/HTML/Element/span>

```{mermaid}
%%| label: fig-schachtelung_1
%%| fig-cap: Hierarchische Schachtelung der HTML-Elemente `span` und `time`
%%{init: {'theme':'forest'}}%%
flowchart TB
  id1(span) --> id2(time)
```

Entsprechend erfolgt die Extraktion des Datums mit der Übergabe des Objektes `html`, zunächst an den Befehl `html_elements("span")`, und anschließend an den Befehl `html_elements("time")`:

```{r}
html |>
  html_elements("span") |>
  html_elements("time")
```

Das Ergebnis ist aber nicht ganz befriedigend, da mehrere Datumsangaben extrahiert worden sind, unter anderem das gewünschte Erscheinugsdatum des Beitrages (`2023-08-29`), aber auch andere, nicht relevate Datumsangaben (z.B. `2023-09-17`), welche ebenfalls auf der Webseite erscheinen (@fig-datum).

![Verweis auf einen anderen Beitrag mit nicht relevanter Datumsangabe](images/datum.png){#fig-datum fig-alt="Verweis auf einen anderen Beitrag mit nicht relevanter Datumsangabe"}

Wir müssen daher beim Auslesen noch genauer die hierarchische Position des Erscheinungsdatums definieren. Ein Blick auf @fig-html1 offenbart, dass die beiden HTML-Elemente `span` und `time` innerhalb des bereits bekannten HTML-Elements `article` geschachtelt sind (`article -> span -> time`, @fig-schachtelung_2).

```{mermaid}
%%| label: fig-schachtelung_2
%%| fig-cap: Hierarchische Schachtelung der HTML-Elemente `article`, `span` und `time`
%%{init: {'theme':'forest'}}%%
flowchart TB
  id1(article) --> id2(span) --> id3(time)
```

Diese hierarchische Schachtelung (`article -> span -> time`) muss daher beim Auslesen des Erscheingsdatums beachtet werden:

```{r}
html |>
  html_elements("article") |>
  html_elements("span") |>
  html_elements("time")
```

Das Erscheinungsdatum ist in diesem Falle das einzige `time`-Element innerhalb des `article`-Elements. Daher führt auch das Weglassen des `span`-Elements und somit die Anwendung einer reduzierten hierarchischen Schachtelung der HTML-Elemente (`article -> time`, @fig-schachtelung_3) zum gewünschten Erfolg:

```{mermaid}
%%| label: fig-schachtelung_3
%%| fig-cap: Reduzierte hierarchische Schachtelung der HTML-Elemente `article` und `time`
%%{init: {'theme':'forest'}}%%
flowchart TB
  id1(article) --> id2(time)
```

```{r}
html |>
  html_elements("article") |>
  html_elements("time")
```

Auch bei der Datumsangabe wollen wir uns auf die wesentliche Information fokussieren und extrahieren daher die reine Datumsangabe, die dem Attribut `"datetime"` zugeordnet ist. Die Befehlskette wird daher um den Befehl `"html_attr("datetime")"` ergänzt:

```{r}
html |>
  html_elements("article") |>
  html_elements("time") |>
  html_attr("datetime")
```

Die Datumsangabe (`"2023-08-29T12:46:06+02:00"`) beinhaltet eine für uns nicht relevante Zeitangabe, also die genaue Uhrzeit der Beitragserscheinung (`T12:46:06+02:00`). Die ersten 10 Zeichen (inkl. Bindestriche: `JJJJ-MM-TT`/`2023-08-29`) beibehalten die relevante Datumsangabe. Die nicht relevante Zeitangabe entfernen wir, indem wir lediglich die ersten 10 Zeichen der Datumsangabe beibehalten. Hierfür ergänzen wir die Befehlskette um den Befehl `str_sub(end = 10)`:

```{r}
html |>
  html_elements("article") |>
  html_elements("time") |>
  html_attr("datetime") |>
  str_sub(end = 10) # R-Zusatzpaket stringr (tidyverse)
```

#### Anzahl der Kommentare

Die Anzahl der Kommentare ist im HTML-Element `div`^[<https://developer.mozilla.org/en-US/docs/Web/HTML/Element/div>] hinterlegt. Und dieses HTML-Element `div` ist durch eine CSS-Klasse[^100] gekennzeichnet (`class="wpd-thread-info"`), wobei die eigentliche Anzahl der Kommentare ein HTML-Attribut ist (`data-comments-count="150"`). Im HTML-Seitenquelltext sieht dies folgendermaßen aus:

[^100]: <https://developer.mozilla.org/en-US/docs/Learn/CSS/Building_blocks/Selectors>; *"CSS includes a miniature language for selecting elements on a page called CSS selectors. CSS selectors define patterns for locating HTML elements, and are useful for scraping because they provide a concise way of describing which elements you want to extract."*, Quelle: <https://rvest.tidyverse.org/articles/rvest.html>

```{html}
<div class="wpd-thread-info" data-comments-count="150"><span class='wpdtc' title='150'>150</span> Kommentare </div>
```

Das Objekt `html` wird daher an den Befehl `html_elements("div")` übergeben. Die anschließende Angabe der CSS-Klasse `"wpd-thread-info"` erfolgt mit einem vorangestellten Punkt (`".wpd-thread-info"`) innerhalb des Befehls `html_elements(".wpd-thread-info")`:

```{r}
html |>
  html_elements("div") |>
  html_elements(".wpd-thread-info")
```
Die eigentliche Extraktion der Anzahl der Kommentare erfolgt mit der Angabe des entsprechenden HTML-Attributes innerhalb des Befehls `html_attr("data-comments-count")`. Die Anzahl der Kommentare wird mit dem Befehl `as.numeric()` in ein nummerisches Format überführt. Die Befehlskette gestaltet sich daher folgendermaßen:

```{r}
html |>
  html_elements("div") |>
  html_elements(".wpd-thread-info") |>
  html_attr("data-comments-count") |>
  as.numeric()
```

#### Ort der Berichterstattung

Die Ortsangabe ist Bestandteil der Zusammenfassung (siehe @fig-struktur1) und die Zusammenfassung ist ein Absatz, i.d.R. der erste Absatz des Beitrages. Für die Extraktion der Ortsangabe ist daher das HTML-Element für Absätze notwendig (`p`[^12]; `p` steht für "paragraph"). Dieses HTML-Element (`p`) ist wie gewohnt innerhalb des HTML-Elements `article` geschachtelt. Zur Extraktion des ersten Absatzes wird diesmal der Befehl `html_element("p")` anstatt `html_elements("p")` genutzt. Der Befehl `html_elements("p")` würde alle Absätze des Beitrages extrahieren. Wir benötigen aber nur den ersten Absatz mit der Ortsangabe und daher nutzen wir diesmal den Befehl `html_element("p")` anstatt `html_elements("p")`. Die Befehlskette gestaltet sich daher wie folgt:

[^12]: <https://developer.mozilla.org/en-US/docs/Web/HTML/Element/p>

```{r}
html |>
  html_elements("article") |>
  html_element("p") |> # html_element anstatt html_elements
  html_text()
```

Somit sehen wir den Absatz mit der Ortsangabe. Wir benötigen allerdings nur die Ortsangabe, also das erste Wort des Absatzes. Hinter der gewünschten Ortsangabe steht ein Punkt (`GENF.`). Mit dem Befehl `str_extract("[^.]+")`[^13] extrahieren wir alle Zeichen vor dem ersten Punkt, also die Ortsangabe `GENF`. Die Befehlskette gestaltet sich daher wie folgt:

[^13]: Bei so einer kryptischen Formel (`"[^.]+"`) handelt es sich um eine sogenannte "*regular expression*" (*regex*). Eine Einführung in diese Thematik findet man hier: <https://r4ds.hadley.nz/regexps> [@Wickham2023-hx, Kapitel 16].

```{r}
html |>
  html_elements("article") |>
  html_element("p") |> # html_element anstatt html_elements
  html_text() |>
  str_extract("[^\\.]+") # R-Zusatzpaket stringr (tidyverse)
```

#### Zusammenfassung des Beitrages

Wie soeben bei der Extraktion der Ortsangabe erwähnt, ist die Zusammenfassung des Beitrages der erste Absatz des Textes (siehe @fig-struktur1). Der erste Absatz wurde soeben folgendermaßen extrahiert:

```{r}
html |>
  html_elements("article") |>
  html_element("p") |> # html_element anstatt html_elements
  html_text()
```

Somit erhalten wir die Zusammenfassung mit der Ortsangabe inkl. Punkt (`GENF.`). Nun wollen wir die überflüssige Ortsangabe entfernen und nur die eigentliche Zusammenfassung beibehalten. Dies erreichen wir mit dem Befehl `str_extract("\\.[\\s](.*)")`. Die regex-Formel `"\\.[\\s](.*)"` hat folgende Bedeutung:

-   `\\.` Suche und extrahiere Zeichen nach dem ersten Punkt (einschließlich des ersten Punktes)
-   `[\\s]` Die extrahierten Zeichen können Leerzeichen sein ("*s*" steht für "*space*")
-   `(.*)` Extrahiere außerdem alle weiteren Zeichen

Die Befehlskette gestaltet sich daher wie folgt:

```{r}
html |>
  html_elements("article") |>
  html_element("p") |> # html_element anstatt html_elements
  html_text() |>
  str_extract("\\.[\\s](.*)") # R-Zusatzpaket stringr (tidyverse)
```

Die Ortsangabe (`GENF`) wurde erfolgreich entfernt. Der Punkt hinter der Ortsangabe (`GENF.`) wurde allerdings nicht entfernt und bleibt bestehen. Die Zusammenfassung beginnt daher nun mit einem Punkt (`.`) gefolgt von einem Leerzeichen. Wir entfernen den Punkt und das Leerzeichen (`". "`) mit dem Befehl `str_remove(". ")`. Die Befehlskette zur Extraktion der Zusammenfassung gestaltet sich daher folgendermaßen:

```{r}
html |>
  html_elements("article") |>
  html_element("p") |> # html_element anstatt html_elements
  html_text() |>
  str_extract("\\.[\\s](.*)") |> # R-Zusatzpaket stringr (tidyverse)
  str_remove(". ") # R-Zusatzpaket stringr (tidyverse)
```

#### Haupttext

Kommen wir nun zum Filetstück, also zum eigentlichen Haupttext des Beitrages. Der Beitragstext besteht aus Absätzen. Also können wir, wie bereits gewohnt, das HTML-Element `p` berücksichtigen. Und dieses HTML-Element `p` ist bekannterweise innerhalb des HTML-Elements `article` geschachtelt:

[X-PATH erklären und anwenden Überall]

```{r}
# Seitenquelltext (HTML)
html |>
  html_elements(xpath = "//article/div/p[not(descendant::blockquote or iframe)]")

# Text
html |>
  html_elements(xpath = "//article/div/p[not(descendant::blockquote or iframe)]") |>
  html_text()
```

Mit der obigen Befehlskette werden 25 Absätze extrahiert. Aber nur 11 Absätze gehören zum eigentlichen Haupttext (4 bis 11, 14, 18 und 19). Es ist also auch eine Menge Gedöns dabei, also primär nicht relevante Informationen, verstreut innerhalb des Beitrages, z.B. Verweise auf externe Quellen:

`[2] "#InklusiveBildungJETZT\nWir sind auf dem Place de Nations angekommen! Die #Staatenpruefung für Deutschland beginnt in Kürze.#WirFahrenNachGenf pic.twitter.com/XoBPh69iUO"`

`[20] "Hier geht es zum vollständigen „Gemeinsamen Bericht der Zivilgesellschaft zum 2. und 3. Bericht der Bundesregierung zur Umsetzung der UN-Behindertenrechtskonvention durch Deutschland“."`

`[21] "Der Beitrag wird auch auf der Facebook-Seite von News4teachers heiß diskutiert."`

Eigentlich müsste man hier eine Systematik entwickeln, um die inhaltlich nicht relevanten Absätze zu entfernen. Man könnte z.B. alle Absätze entfernen, die die Wörter "*Twitter*" oder "*Facebook*" enthalten. Mit dieser Systematik würde man aber Gefahr laufen, dass man auch fälschlicherweise inhaltlich relevante Absätze entfernt, z.B. einen Absatz, der darauf verweist, dass sich ein Bildungspolitiker bei "*Twitter*" kritisch zum Thema Inklusion geäußert hat. Wir müssen also Absätze mit weniger oder nicht relevanten Inhalten in Kauf nehmen.

Aber wir haben auch drei Absätze ohne sinnvollen Inhalt (`[22] ""`, `[24] ""` und `[25] " "`), da diese Absätze keine Zeichen oder ausschließlich Leerzeichen enthalten. Wir wollen daher Absätze, die keine Zeichen oder ausschließlich Leerzeichen enthalten, entfernen. Hierfür speichern wir vorübergehend alle Absätze im Textformat als Objekt `all_p` ab. Wir Überprüfen anschließend, ob der Textinhalt der Absätze  keine Zeichen oder ausschließlich Leerzeichen enthält (`all_p |> str_detect("^\\s*$")`). Die regex-Formel `"^\\s*$"` hat folgende Bedeutung:

-   `^` Beginne die Suche am Anfang des Absatzes
-   `\\s*` Suche Leerzeichen bzw. keine Zeichen ("*s*" steht für "*space*")
-   `$` Führe diese Suche bis zum Ende des Absatzes durch

Das Ergebnis der Überprüfung ist eine `TRUE`-`FALSE`-Aussage für jeden Absatz (`TRUE`: Keine Zeichen oder ausschließlich Leerzeichen; `FALSE`: Andere Zeichen). Diese `TRUE`-`FALSE`-Aussage speichern wir als Objekt `spaces` ab und lassen uns abschließend Absätze anzeigen, welche nicht ausschließlich aus Leerzeichen oder keinen Zeichen bestehen (`all_p[!spaces]`):

```{r}
all_p <-
  html |>
  html_elements("article") |>
  html_elements("p") |>
  html_text()

all_p |>
  str_detect("^\\s*$") # R-Zusatzpaket stringr (tidyverse)

spaces <-
  all_p |>
  str_detect("^\\s*$") # R-Zusatzpaket stringr (tidyverse)

all_p[!spaces]
```

Fast geschafft! Wir müssen nur noch den ersten Absatz entfernen (`all_p[!spaces] %>% .[-1]`), da der erste Absatz die Zusammenfassung mit Ortsangabe darstellt (`[1] "GENF. „Schämt Euch!“ – so heißt es auf einem Transparent..."`) und daher nicht zum Haupttext gezählt werden kann. Der Punkt vor der eckigen Klammer (`.[-1]`) symbolisiert die Absätze (`all_p[!spaces]`). Bei der Übergabe dieser Absätze nutzen wir, wie zuvor beim Auslesen des Links (@sec-link), die magrittr-Pipe (`%>%`), da nur diese Pipe, und nicht die base-Pipe (`|>`), eine Übergabe an eine Bedingung in eckigen Klammern ermöglicht (`.[-1]`). Alle Absätze werden anschließend mit dem Befehl `list()` in eine Liste überführt. Dies ist daher die schlussendliche Befehlskette:

```{r}
all_p[!spaces] %>% # Pipe-Operator, R-Zusatzpaket magrittr (tidyverse)
  .[-1] |>
  list()
```

<!-- 

Wir müssen diese nicht relevanten Absätze also mittels einer geeigneten Systematik entfernen. Woran erkennen wir die nicht relevanten Absätze? Es sind Verweise auf externe Quellen und diese Verweise enthalten Links auf externe Quellen. Im nächsten Schritt wollen wir daher Absätze mit Links entfernen. Ob in einem Absatz ein Link vorhanden ist, erkennen wir am HTML-Element `a`[^15] (`a` steht für "*anchor*"), z.B. beim Verweis auf externe Inhalte bei Twitter (@fig-struktur1 und @fig-struktur2):

[^15]: <https://developer.mozilla.org/en-US/docs/Web/HTML/Element/a>

`"<p dir=\"ltr\" lang=\"de\"><a href=\"https://twitter.com/hashtag/InklusiveBildungJETZT?src=hash&amp;ref_src=twsrc%5Etfw\">#InklusiveBildungJETZT</a><br>\nWir sind auf dem Place de Nations angekommen! Die <a href=\"https://twitter.com/hashtag/Staatenpruefung?src=hash&amp;ref_src=twsrc%5Etfw\">#Staatenpruefung</a> für Deutschland beginnt in Kürze.<a href=\"https://twitter.com/hashtag/WirFahrenNachGenf?src=hash&amp;ref_src=twsrc%5Etfw\">#WirFahrenNachGenf</a> <a href=\"https://t.co/XoBPh69iUO\">pic.twitter.com/XoBPh69iUO</a></p>"`

Das Ende eines Links wird also immer mit dem HTML-Element `</a>` gekennzeichnet sein. Wir wollen nun alle Absätze entfernen, die das HTML-Element `</a>` enthalten[^16]. Hierfür speichern wir vorübergehend alle Absätze im HTML-Format als Objekt `all_p` ab. Wir überführen alle Absätze (`all_p`) mit dem Befehl `as.character()` ins reine Textformat und überprüfen anschließend mit dem Befehl `str_detect("</a>")`, ob Verlinkungen in den Absätzen vorhanden sind. Das Ergebnis ist ein `TRUE`-`FALSE`-Aussage für jeden Absatz (`TRUE`: Link vorhanden; `FALSE`: Link nicht vorhanden). Diese `TRUE`-`FALSE`-Aussage speichern wir als Objekt `link` ab und lassen uns abschließend anzeigen, welche Absätze keine Links enthalten (`all_p[!link] |> html_text()`):

[^16]: Bei diesem Arbeitsschritt habe ich die R-Community um Unterstützung gebeten: <https://stackoverflow.com/questions/77152943/>

```{r}
all_p <-
  html |>
  html_elements("article") |>
  html_elements("p")
  
all_p |>
  as.character() |>
  str_detect("</a>") # R-Zusatzpaket stringr (tidyverse)

link <-
  all_p |>
  as.character() |>
  str_detect("</a>") # R-Zusatzpaket stringr (tidyverse)

all_p[!link] |>
  html_text()
```

Das Ergebnis ist schon ziemlich befriedigend: Absätze mit Links wurden entfernt. Aber wir haben auch einen inhaltlich relevanten Absatz entfernt. Ein Absatz enthält nämlich einen Link und ist zugleich inhaltlich relevant (@fig-absatz).

![Inhaltlich relevanter Absatz mit Link ("News4teachers berichtete")](images/absatz.png){#fig-absatz fig-alt="Inhaltlich relevanter Absatz mit Link (News4teachers berichtete)"}

Wir müssen unsere Systematik daher erweitern und entsprechende Absätze mit "*News4teachers berichtete*"-Links beibehalten, da diese Absätze inhaltlich relevant erscheinen. Dies erreichen wir, in dem wir alle Absätze (`all_p`) dahingehend überprüfen, ob die Zeichenabfolge `"News4teachers berichtete"` in den Absätzen vorhanden ist (`str_detect("News4teachers berichtete")`). Das Ergebnis dieser Überprüfung ist wieder eine `TRUE`-`FALSE`-Aussage für jeden Absatz (`TRUE`: "*News4teachers berichtete*"-Link vorhanden; `FALSE`: "*News4teachers berichtete*"-Link nicht vorhanden). Diese `TRUE`-`FALSE`-Aussage speichern wir als Objekt `n4t_link` ab und lassen uns abschließend Absätze anzeigen, welche "*News4teachers berichtete*"-Links enthalten, aber keine anderen Links enthalten (`all_p[n4t_link | !link] |> html_text()`):

```{r}
all_p |>
  as.character() |>
  str_detect("News4teachers berichtete") # R-Zusatzpaket stringr (tidyverse)

n4t_link <-
  all_p |>
  as.character() |>
  str_detect("News4teachers berichtete") # R-Zusatzpaket stringr (tidyverse)

all_p[!link | n4t_link] |>
  html_text()
```

Das sieht schon ziemlich gut aus. Der inhaltlich relevante Absatz mit “*News4teachers berichtete*”-Link (@fig-absatz) wurde nun beibehalten (`"[4] "Das Deutsche Institut für Menschenrechte ..."`). Aber wir haben zwei Absätze ohne sinnvollen Inhalt (`[13] ""` und `[14] " "`), da diese Absätze keine Zeichen oder ausschließlich Leerzeichen enthalten. Wir wollen daher Absätze, die keine Zeichen oder ausschließlich Leerzeichen enthalten, entfernen. Wir Überprüfen daher, ob der Textinhalt der Absätze (`all_p[!link | n4t_link] |> html_text()`) keine Zeichen oder ausschließlich Leerzeichen enthält (`str_detect("^\\s*$")`). Die regex-Formel `"^\\s*$"` hat folgende Bedeutung:

-   `^` Beginne die Suche am Anfang des Absatzes
-   `\\s*` Suche Leerzeichen bzw. keine Zeichen ("*s*" steht für "*space*")
-   `$` Führe diese Suche bis zum Ende des Absatzes durch

Das Ergebnis der Überprüfung ist erneut eine `TRUE`-`FALSE`-Aussage für jeden Absatz (`TRUE`: keine Zeichen oder ausschließlich Leerzeichen). Diese `TRUE`-`FALSE`-Aussage speichern wir als Objekt `spaces` ab und lassen uns abschließend Absätze anzeigen, welche nicht ausschließlich aus Leerzeichen oder keinen Zeichen bestehen (`.[!spaces]`). Der Punkt (`.`) symbolisiert die Absätze, welche zuvor mittels `all_p[!link | n4t_link] |> html_text()` erzeugt worden sind. Bei der Übergabe dieser Absätze nutzen wir, wie zuvor beim Auslesen des Links (@sec-link), die magrittr-Pipe (`%>%`), da nur diese Pipe, und nicht die base-Pipe (`|>`), eine Übergabe an eine Bedingung in eckigen Klammern ermöglicht (`.[!spaces]`):

```{r}
all_p[!link | n4t_link] |>
  html_text() |>
  str_detect("^\\s*$") # R-Zusatzpaket stringr (tidyverse)

spaces <-
  all_p[!link | n4t_link] |>
  html_text() |>
  str_detect("^\\s*$") # R-Zusatzpaket stringr (tidyverse)

all_p[!link | n4t_link] |>
  html_text() %>% # Pipe-Operator, R-Zusatzpaket magrittr (tidyverse)
  .[!spaces]
```

Fast geschafft! Wir müssen nur noch den ersten Absatz entfernen (`.[-1]`), da der erste Absatz die Zusammenfassung mit Ortsangabe darstellt (`[1] "GENF. „Schämt Euch!“ – so heißt es auf einem Transparent..."`) und daher nicht zum Haupttext gezählt werden kann. Dies ist daher die schlussendliche Befehlskette:

```{r}
all_p[!link | n4t_link] |>
  html_text() %>% # Pipe-Operator, R-Zusatzpaket magrittr (tidyverse)
  .[!spaces] %>% # Pipe-Operator, R-Zusatzpaket magrittr (tidyverse)
  .[-1]
```

-->

### Datensatz

Die bisher erfolgreich ausgelesenen Informationen (@sec-schritte: ID, Link, Erscheinungsdatum, Anzahl der Kommentare, Ort der Berichterstattung, Titel, Zusammenfassung des Beitrages und Haupttext) können wir nun in einem Datensatz zusammenfassen (vgl. @tbl-idee). Hierfür wiederholen wir pro forma und zwecks Übersichtlichkeit alle bisherigen Arbeitsschritte und speichern jede einzelne Information als Objekt (`ID`, `link`, `datum`, `n_kommentare`, `ort`, `titel`, `zusammenfassung` und `text`):

```{r}
# Alle Informationen von der Webseite extrahieren

html <-
  read_html("https://www.news4teachers.de/2023/08/schaemt-euch-deutschland-steht-vor-den-vereinten-nationen-am-pranger-weil-es-die-inklusion-an-schulen-verweigert/")

# ID

ID <-
  html |>
  html_elements("article") |>
  html_attr("id")

# Link

link <-
  html |>
  html_elements("article") |>
  html_attr("id") |>
  str_remove("post-") |> # R-Zusatzpaket stringr (tidyverse)
  as.numeric() %>% # Pipe-Operator, R-Zusatzpaket magrittr (tidyverse)
  paste0("https://www.news4teachers.de/?p=", .)

# Erscheinungsdatum

datum <-
  html |>
  html_elements("article") |>
  html_elements("time") |>
  html_attr("datetime") |>
  str_sub(end = 10) # R-Zusatzpaket stringr (tidyverse)

# Anzahl der kommentare

n_kommentare <-
  html |>
  html_elements("div") |>
  html_elements(".wpd-thread-info") |>
  html_attr("data-comments-count") |>
  as.numeric()

# Ort der Berichterstattung

ort <-
  html |>
  html_elements("article") |>
  html_element("p") |> # html_element anstatt html_elements
  html_text() |>
  str_extract("[^\\.]+") # R-Zusatzpaket stringr (tidyverse)

# Titel

titel <-
  html |>
  html_elements("h1") |>
  html_text()

# Zusammenfassung

zusammenfassung <-
  html |>
  html_elements("article") |>
  html_element("p") |> # html_element anstatt html_elements
  html_text() |>
  str_extract("\\.[\\s](.*)") |> # R-Zusatzpaket stringr (tidyverse)
  str_remove(". ") # R-Zusatzpaket stringr (tidyverse)

# Haupttext

all_p <-
  html |>
  html_elements("article") |>
  html_elements("p") |>
  html_text()

spaces <-
  all_p |>
  str_detect("^\\s*$") # R-Zusatzpaket stringr (tidyverse)

text <- all_p[!spaces] %>% # Pipe-Operator, R-Zusatzpaket magrittr (tidyverse)
  .[-1] |>
  list()
```

Der Datensatz `n4t_data` wird mit dem Befehl `tibble()` erstellt (der Datensatz ist auch in @tbl-daten1 dargestellt):

```{r}
n4t_data <- tibble(ID, link, datum, n_kommentare, ort, titel, zusammenfassung, text) # R-Zusatzpaket tibble (tidyverse)
n4t_data
```

```{r echo = F}
#| label: tbl-daten1
#| tbl-cap: "Datensatz für einen Beitrag"
n4t_data |>
  mutate(link = paste0(str_sub(link, start = 1, end = 8), "..."),
         titel = paste0(str_sub(titel, start = 1, end = 20), "..."),
         zusammenfassung = paste0(str_sub(zusammenfassung, start = 1, end = 20), "...")) |>
  flextable() |>
  bold(part = "header") |>
  footnote(i = 1, j = 8, value = as_paragraph(c("Absätze des Haupttextes sind als Liste hinterlegt")),
           ref_symbols = c("#"), part = "body", inline = TRUE, sep = "") |>
  autofit()
```

Dieser Datensatz (@tbl-daten1) enthält nun einen Beitrag von der Seite News4teachers. Wir wollen aber mehrere Beiträge in einem Datensatz bündeln. Für den nächsten Beitrag mit dem Titel "*Sparpläne: Inklusion hängt in NRW am seidenen Faden – Lebenshilfe appelliert*"^[<https://www.news4teachers.de/2023/09/sparplaene-inklusion-haengt-in-nrw-am-seidenen-faden-lebenshilfe-appelliert/>] wiederholen wir daher die Arbeitsschritte der Extraktion :

```{r}
# Alle Informationen von der Webseite extrahieren

html <-
  read_html("https://www.news4teachers.de/2023/09/sparplaene-inklusion-haengt-in-nrw-am-seidenen-faden-lebenshilfe-appelliert/")

# ID

ID <-
  html |>
  html_elements("article") |>
  html_attr("id")

# Link

link <-
  html |>
  html_elements("article") |>
  html_attr("id") |>
  str_remove("post-") |> # R-Zusatzpaket stringr (tidyverse)
  as.numeric() %>% # Pipe-Operator, R-Zusatzpaket magrittr (tidyverse)
  paste0("https://www.news4teachers.de/?p=", .)

# Erscheinungsdatum

datum <-
  html |>
  html_elements("article") |>
  html_elements("time") |>
  html_attr("datetime") |>
  str_sub(end = 10) # R-Zusatzpaket stringr (tidyverse)

# Anzahl der kommentare

n_kommentare <-
  html |>
  html_elements("div") |>
  html_elements(".wpd-thread-info") |>
  html_attr("data-comments-count") |>
  as.numeric()

# Ort der Berichterstattung

ort <-
  html |>
  html_elements("article") |>
  html_element("p") |> # html_element anstatt html_elements
  html_text() |>
  str_extract("[^\\.]+") # R-Zusatzpaket stringr (tidyverse)

# Titel

titel <-
  html |>
  html_elements("h1") |>
  html_text()

# Zusammenfassung

zusammenfassung <-
  html |>
  html_elements("article") |>
  html_element("p") |> # html_element anstatt html_elements
  html_text() |>
  str_extract("\\.[\\s](.*)") |> # R-Zusatzpaket stringr (tidyverse)
  str_remove(". ") # R-Zusatzpaket stringr (tidyverse)

# Haupttext

all_p <-
  html |>
  html_elements("article") |>
  html_elements("p") |>
  html_text()

spaces <-
  all_p |>
  str_detect("^\\s*$") # R-Zusatzpaket stringr (tidyverse)

text <- all_p[!spaces] %>% # Pipe-Operator, R-Zusatzpaket magrittr (tidyverse)
  .[-1] |>
  list()
```

Die Informationen aus dem neuen Beitrag werden mit dem Befehl `add_row()` dem Datensatz `n4t_data` hinzugefügt:

```{r}
n4t_data <-
  n4t_data |>
  add_row(ID, link, datum, n_kommentare, ort, titel, zusammenfassung, text)
```

```{r echo = F}
#| label: tbl-daten2
#| tbl-cap: "Datensatz für zwei Beiträge"
n4t_data |>
  mutate(link = paste0(str_sub(link, start = 1, end = 8), "..."),
         titel = paste0(str_sub(titel, start = 1, end = 20), "..."),
         zusammenfassung = paste0(str_sub(zusammenfassung, start = 1, end = 20), "...")) |>
  flextable() |>
  bold(part = "header") |>
  footnote(i = 1:2, j = 8, value = as_paragraph(c("Absätze des Haupttextes sind als Liste hinterlegt")),
           ref_symbols = c("#"), part = "body", inline = TRUE, sep = "") |>
  autofit()
```

Der neue Datensatz (@tbl-daten2) enthält nun Informationen für zwei News4teachers-Beiträge. Wir könnten die Arbeitsschritte der Extraktion nun immer wieder wiederholen und somit stetig neue Beiträge zum Datensatz hinzufügen. Bei einer großen Anzahl von Beiträgen wäre dies allerdings ein sehr langwieriger Prozess. Daher müssen wir für die Extraktion, also für das Web-Scraping der Beiträge, eine automatisierte Routine entwickeln.

## Automatisiertes Web-Scraping

TBA

# Weiteres

Urheber Gesetz

Politeness

Evtl. interessant für das Auslesen der Kommentare:

<https://www.news4teachers.de/2023/08/schaemt-euch-deutschland-steht-vor-den-vereinten-nationen-am-pranger-weil-es-die-inklusion-an-schulen-verweigert/feed/>

# Literatur
