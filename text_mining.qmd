---
title: "Was liest und schreibt man √ºber Inklusion?"
subtitle: "Web-Scraping und Text-Mining mit R am Beispiel einer Online-Nachrichten- und Diskussionsseite f√ºr Lehrkr√§fte"
author: "Pawel R. Kulawiak"
date: last-modified
abstract: "Preprint in Progress"
abstract-title: "Status"
lang: de
bibliography: references.bib
theme: sandstone
toc: true
number-sections: true
toc-depth: 90
toc-expand: true
title-block-banner: "#2A5B60"
code-overflow: wrap
code-block-bg: "#F8F8F8"
code-block-border-left: "#769E3C"
mermaid-format: png
language: 
  title-block-author-single: "Autor"
  title-block-published: "Datum der letzten Ver√§nderung"
format:
  html:
    embed-resources: false
    output-file: "index"
  #pdf:
    #include-in-header: 
      #text: |
        #\usepackage{lscape}
        #\newcommand{\blandscape}{\begin{landscape}}
        #\newcommand{\elandscape}{\end{landscape}}
    #header-includes: |
      #\titlehead{\includegraphics[width=6.5in]{images/owl.png}}
    #geometry:
      #- top=30mm
      #- left=30mm
      #- right=30mm
      #- bottom=30mm
      #- heightrounded
    #fig-pos: H
---

```{r}
#| echo: false
#| include: false
options(width = 75)
library(tidyverse)
library(rvest)
library(flextable)
```

![](images/owl.png){width="60%"}

{{< downloadthis text_mining.pdf dname=text_mining label="PDF-Download" icon=file-earmark-pdf type=success >}} 

\newpage

# Vorwort

TBA -

# Einleitung

Mein wertgesch√§tzter Kollege Timo L√ºke[^1] hat einst im Rahmen einer Medieninhaltsanalyse deutschsprachiger Printmedien [@l√ºke2014] folgende Forschungsfragen aufgeworfen:

[^1]: <https://timolueke.de/>

-   Welches Verst√§ndnis von Inklusion wird in den deutschen meinungsf√ºhrenden Medien kommuniziert?
-   Welche Argumente f√ºr und gegen die Umsetzung von Inklusion werden genannt?
-   Welche Fallbeispiele werden als Belege angef√ºhrt?

> *"Im Rahmen einer systematischen Inhaltsanalyse (R√∂ssler, 2010) deutscher Printmedien untersuchen wir die √∂ffentliche Berichterstattung zum Thema ‚ÄûInklusion". Dabei wollen wir verbreitete Definitionen, Argumente und Fallbeispiele systematisch erfassen. So sollen langfristig die Analyse des medialen Diskurses und in der Folge eine Versachlichung der kontroversen Debatte √ºber Inklusion erm√∂glicht werden."* [@l√ºke2014]

Erste Ergebnisse der Medieninhaltsanalyse sind in Form einer Posterpr√§sentation verf√ºgbar [@l√ºke2014] und ich erlaube mir die Darstellung des interessanten Posters (@fig-poster).

::: column-body-outset-left
![Posterpr√§sentation von @l√ºke2014: Was liest man √ºber Inklusion?](images/poster.png){#fig-poster fig-alt="Posterpr√§sentation von @l√ºke2014: Was liest man √ºber Inklusion?"}
:::

# Ziele

## Allgemeine Zielsetzung

Ich m√∂chte die Medieninhaltsanalyse von @l√ºke2014 replizieren sowie erweitern und mich dabei auf die Textinhalte einer Online-Nachrichten- und Diskussionsseite f√ºr Lehrkr√§fte fokussieren, n√§mlich News4teachers [@N4T_2022].

## Zielsetzung mit R: Web-Scraping und Text-Mining

Ich m√∂chte exemplarisch aufzeigen, wie die einzelnen Projektphasen der Medieninhaltsanalyse mit der Programmiersprache R umgesetzt werden k√∂nnen. Hierf√ºr werden wir uns auf zwei wichtige Arbeitsschritte fokussieren:

-   **Web-Scraping**, also eine automatisierte Methode zum Extrahieren der Textinformationen von der Webseite News4teachers. Eine Einf√ºhrung in das Thema Web-Scraping mit R bieten @Wickham2023-hx [Kapitel 25]^[<https://r4ds.hadley.nz/webscraping>].

-   **Text-Mining**: Die mittels Web-Scraping gesammelten Textdaten sollen mit Methoden des Text-Minings analysiert werden. Methoden des Text-Minings fokussieren sich auf die Extraktion von n√ºtzlichen Informationen aus unstrukturierten Textdaten. Unstrukturierte Textdaten sind Texte, die nicht in einer festen Datenbankstruktur vorliegen, also z.B. Textinhalte von Webseiten. Mit Methoden des Text-Minings kann auch der sentimentale Ton eines Textinhalts bzw. die im Text vermittelte subjektive Meinung analysiert werden. Das Hauptziel der sogenannten Sentimentanalyse besteht also darin, die in einem Textdokument ge√§u√üerten Emotionen und Ansichten bez√ºglich eines bestimmten Themas zu identifizieren, in unserem Fall also z.B. ge√§u√üerte Meinungen zum Thema Inklusion. Eine Einf√ºhrung in das Thema Text-Mining mit R bieten @Silge2017-sp.

# News4teachers: Online-Nachrichten- und Diskussionsseite f√ºr Lehrkr√§fte

Bevor wir mit dem Web-Scraping und Text-Mining beginnen, betrachten wir zun√§chst das Arbeitsmaterial, also die Webinhalte der Webseite News4teachers, und die entsprechende Selbstbeschreibung der Webseite [@N4T_2022]:

> "**Wer steckt hinter News4teachers?**
>
> News4teachers wird von einer Redaktion aus Lehrern und Journalisten betrieben. Die Seite ist ein gemeinsames Projekt von [4teachers](http://www.4teachers.de/), der Service-Plattform von Lehrern f√ºr Lehrer, sowie [der Agentur f√ºr Bildungsjournalismus](http://www.xn--agentur-fr-bildungsjournalismus-wid.de/).
>
> **Was ist News4teachers?**
>
> News4teachers ist eine Nachrichten- und Diskussionsseite, die sich mit seri√∂sen Berichten, Analysen und Kommentaren an p√§dagogische Profis und die an Bildungsthemen interessierte √ñffentlichkeit richtet. Die Redaktion sichtet t√§glich die Meldungen aus Politik, Forschung und Gesellschaft. Auf die Seite gelangt alles, was¬†f√ºr die¬†Bildung¬†wichtig¬†ist. News4teachers¬†bietet¬†also einen aktuellen √úberblick √ºber die relevanten Informationen f√ºr Lehrer, Erzieher, Sch√ºler und Eltern. Und zwar: unabh√§ngig und √ºberparteilich.
>
> **Was ist die Idee hinter News4teachers?**
>
> News4teachers f√ºhlt sich dem klassischen Journalismus verpflichtet. Das hei√üt konkret: Wir unterwerfen uns den publizistischen Grunds√§tzen des Deutschen Presserats, dem [Pressekodex](https://www.presserat.de/pressekodex/pressekodex/). Informationen, die auf die Seite gelangen,¬†wurden zuvor¬†von der Redaktion mit¬†der gebotenen Sorgfalt gepr√ºft. Quellen werden stets¬†genannt, Meinung und Bericht voneinander getrennt. News4teachers unterliegt zudem einer Chronistenpflicht: Alles, was f√ºr die Bildungsdebatte in Deutschland von Bedeutung ist, wird aktuell¬†berichtet. Regelm√§√üige Nutzer von News4teachers sind also immer im Bild." [@N4T_2022]

Die Redaktion besteht aus folgenden Personen [@N4T_impressum]: Anna H√ºckelheim, Sonja Mankowsky, Laura Millmann, Nina Odenius, Thomas Zab und Milla Priboschek (Podcast-Redaktion).

```{r}
#| echo: false
#| include: false
options(scipen=999)
BESUCHER <- 55000
LK <- 975000
```

## Inhalte von News4teachers und potenzielle Leserschaft aus Lehrkr√§ften

News4teachers verspricht eine unabh√§ngige und √ºberparteiliche Berichterstattung zu Bildungsthemen, wahrscheinlich auch zum Thema Inklusion. Die Inhalte sind f√ºr die Leserschaft kostenfrei (werbefinanziertes Angebot). Die Inhalte von News4teachers sind au√üerdem speziell auf Lehrkr√§fte ausgerichtet. Somit kann angenommen werden, dass ein gro√üer Teil der Leserschaft aus Lehrkr√§ften besteht. Die Internetseite News4teachers hatte folgende Besucherzahlen (Jahr 2023): Mai (54000 Personen), Juni (60000 Personen) und Juli und August jeweils 55000 Personen (Zahlen ermittelt mit: <https://neilpatel.com/website-traffic-checker/>). Nehmen wir an, dass die Leserschaft von News4teachers zu 75% aus Lehrkr√§ften aus Deutschland best√ºnde, dann h√§tten wir bei einer monatlichen Besucherzahl von `r BESUCHER` Personen eine monatliche Leserschaft von ca. `r BESUCHER * 0.75` Lehrkr√§ften (`r BESUCHER` \* 0,75 = `r BESUCHER * 0.75`). In Deutschland gibt es aber laut Mikrozensus 2022 rund `r LK` Lehrkr√§fte an allgemeinbildenden Schulen [@census]. Die potenzielle News4teachers-Leserschaft aus Lehrkr√§ften (`r BESUCHER * 0.75` Personen) entspr√§che dann einem Anteil von ca. `r (BESUCHER/LK*100) |> round(2)`% aller Lehrkr√§fte an allgemeinbildenden Schulen (`r BESUCHER` / `r LK` \* 100 = `r (BESUCHER/LK*100) |> round(2)`%). Im dargestellten Szenario w√ºrden die Inhalte von News4teachers also pro Monat ca. `r (BESUCHER/LK*100) |> round(2)`% der Lehrkr√§fte an allgemeinbildenden Schulen in Deutschland erreichen (5 von 100 Lehrkr√§ften lesen News4teachers). Dies sind aber nur vage Vermutungen zur Reichweite von News4teachers unter Lehrkr√§ften an allgemeinbildenden Schulen in Deutschland, unter der Annahme, dass 75% der Leserschaft von News4teachers aus Lehrkr√§ften best√ºnde.

AUFGREIFEN: \[<https://www.news4teachers.de/2021/12/liebe-leserin-lieber-leser-ein-wort-zum-jahreswechsel-in-eigener-sache/>\]

### Kommentare und Diskussionen

Die Webseite News4teachers bieten der Leserschaft die M√∂glichkeit die Inhalte zu kommentieren und zu diskutieren (@fig-beitrag und @fig-struktur3). Hierf√ºr formuliert die Redaktion spezifische Richtlinien [@N4T_2022]:

> "**Gibt's Regeln f√ºr die Leserzuschriften in den Foren?**
>
> Grunds√§tzlich gilt: Niemand hat einen Anspruch darauf, in den Foren zu den einzelnen Artikeln eine eigene Wortmeldung zu ver√∂ffentlichen. Die Redaktion legt Wert darauf, nur Leserzuschriften zu ver√∂ffentlichen, die erkennbar darauf abzielen, einen inhaltlichen Beitrag zur Diskussion des dar√ºberstehenden Artikels zu leisten. Das bedeutet konkret: Auch f√ºr Leserzuschriften gelten die publizistischen Grunds√§tze des Deutschen Presserats, gilt also [der Pressekodex](https://www.presserat.de/pressekodex/pressekodex/).
>
> Kurzgefasst:
>
> -   Wir ver√∂ffentlichen keine Leserbeitr√§ge, in denen ungepr√ºfte, unbelegte oder falsche Tatsachenbehauptungen verbreitet werden.
> -   Wir ver√∂ffentlichen keine Hetze gegen Menschen oder Menschengruppen.
> -   Wir ver√∂ffentlichen keine Werbung, ob nun f√ºr Produkte oder Parteien.
> -   Und wir ver√∂ffentlichen keine Links auf unseri√∂se Quellen.
>
> Wir behalten uns dar√ºber hinaus vor, Leserbriefe, die lediglich der Stimmungsmache dienen, zu l√∂schen. Oder Leserbriefe sinnwahrend zu k√ºrzen." [@N4T_2022]

\[Hier weitere Erl√§uterungen einf√ºgen\]

##### Facebook und Twitter

Die Beitr√§ge werden aber nicht nur unmittelbar auf der Seite von News4teachers kommentiert und diskutiert (@fig-struktur3). Die Diskussion der Beitr√§ge erfolgt auch auf einer externen Seite, n√§mlich bei Facebook. Innerhalb der Beitr√§ge wird auch auf die externe Diskussion bei Facebook verwiesen (@fig-facebook). Au√üerdem werden die Beitr√§ge von News4teachers ebenfalls bei Twitter geteilt und diskutiert (<https://twitter.com/News4teachers>). Die Kommentare und Diskussionen bei Facebook und Twitter sollen daher auch bei der vorliegenden Medieninhaltsanalyse Ber√ºcksichtigung finden. 

![Beitrag zum Thema Inklusion mit 152 Leserkommentaren auf der Internetseite News4teachers [@N4T_2023]](images/beitrag.png){#fig-beitrag fig-alt="Inhalt zum Thema Inklusion mit 152 Leserkommentaren auf der Internetseite News4teachers"}

![Verweis bei News4teachers auf die Diskussion bei Facebook](images/facebook.png){#fig-facebook fig-alt="Verweis bei News4teachers auf die Diskussion bei Facebook"}

# Explorative Forschungsfragen

Die Inhalte von der Webseite News4teachers und die Kommentare und Diskussionen der Leserschaft eignen sich eventuell zur Beantwortung folgender Forschungsfragen:

-   Auf welche Art und Weise wird das Thema Inklusion auf der Online-Nachrichten- und Diskussionsseite f√ºr Lehrkr√§fte dargestellt?
-   Auf welche Art und Weise werden die Inhalte zum Thema Inklusion von der Leserschaft kommentiert und diskutiert?

# Web-Scraping

Der erste Arbeitsschritt, hin zum Text-Mining, also hin zur Medieninhaltsanalyse, wird nun das Web-Scraping sein, also die automatisierte Extraktion der Webinhalte (z.B. Textinformationen) von der Webseite News4teachers. Traditionellerweise bzw. altmodischerweise w√ºrde man Webinhalte mit der Methode "*copy-and-paste*" in einen Datensatz √ºbertragen, also z.B. Text von einer Webseite kopieren und anschlie√üend die kopierte Textinformation in einen Datensatz einf√ºgen (z.B. bei Excel). Dieses Verfahren ist aber fehleranf√§llig, da z.B. die Gefahr besteht, dass aufgrund mangelnder Konzentration falsche oder unvollst√§ndige Textinhalte √ºbertragen werden. Web-Scraping ist daher als automatisierte Methode der Extraktion von Webinhalten weniger anf√§llig f√ºr Fehler und somit die Methode der Wahl. Eine Einf√ºhrung in das Thema Web-Scraping mit R bieten @Wickham2023-hx [Kapitel 25]^[<https://r4ds.hadley.nz/webscraping>].

## R-Zusatzpakete

### R-Zusatzpaket *rvest*

F√ºr das Web-Scraping nutzen wir nun das R-Zusatzpaket *rvest* [@rvest]. Der Name des R-Zusatzpaketes ist eine gelungene Anspielung auf das englische Wort *harvest* (ernten, sammeln), denn wir wollen ja Informationen aus dem Internet sammeln (mit R). Das kreative Wortspiel ist auch im Logo des R-Zusatzpaketes visualisiert (@fig-logo). Zun√§chst m√ºssen wird das R-Zusatzpaket installieren und laden.

```{r}
#| eval: false
install.packages("rvest")
library(rvest)
```

![Logo des R-Zusatzpaketes *rvest*](images/logo.png){#fig-logo fig-alt="Logo des R-Zusatzpaketes rvest"}

Das R-Zusatzpaket *rvest* verf√ºgt √ºber eine umfassende und hilfreiche Online-Dokumentation:

-   <https://rvest.tidyverse.org/>
-   <https://r4ds.hadley.nz/webscraping> [@Wickham2023-hx, Kapitel 25]

### R-Zusatzpaket *tidyverse*

Das R-Zusatzpaket *tidyverse* [@tidyverse] ist eine Zusammenstellung unterschiedlicher R-Zusatzpakete. Auch das R-Zusatzpaket *rvest* ist Bestandteil des R-Zusatzpakets *tidyverse*. Wir werden an diversen Stellen die herausragende Funktionalit√§t des R-Zusatzpaketes *tidyverse* nutzen. An den entsprechenden Stellen wird ein Verweis auf die R-Zusatzpakete erfolgen. Informationen zum R-Zusatzpaket *tidyverse* findet man hier:

<https://www.tidyverse.org/>

Wir installieren und laden das R-Zusatzpaket:

```{r}
#| eval: false
install.packages("tidyverse")
library(tidyverse)
```

## Struktur und Inhalte der Webseite

Ziel des Web-Scapings wird es sein, die relevanten Webinhalte von News4teachers automatisiert zu extrahieren. Hierf√ºr m√ºssen wir uns erstmal einen √úberblick √ºber die Struktur und Inhalte der Webseite verschaffen. Die Beitr√§ge auf den Internetseiten von News4teachers haben eine spezifische Struktur mit spezifischen Webinhalten. Wir betrachten den Beitrag mit dem Titel *"‚ÄûSch√§mt Euch!" -- Deutschland steht vor den Vereinten Nationen am Pranger, weil es die Inklusion an Schulen praktisch verweigert"* [@N4T_2023]. F√ºr unsere Forschungsfragen mehr oder weniger interessante Webinhalte sind in den Abbildungen kenntlich gemacht (@fig-struktur1, @fig-struktur2 und @fig-struktur3).

![(a) Beitrag auf der Internetseite News4teachers und Struktur der Webinhalte [@N4T_2023]](images/struktur1.png){#fig-struktur1 fig-alt="Beitrag auf der Internetseite News4teachers und Struktur der Webinhalte"}

![(b) Beitrag auf der Internetseite News4teachers und Struktur der Webinhalte [@N4T_2023]](images/struktur2.png){#fig-struktur2 fig-alt="Beitrag auf der Internetseite News4teachers und Struktur der Webinhalte"}

![(c) Beitrag auf der Internetseite News4teachers und Struktur der Webinhalte [@N4T_2023]](images/struktur3.png){#fig-struktur3 fig-alt="Beitrag auf der Internetseite News4teachers und Struktur der Webinhalte"}

\[Erl√§uterungen zu den Abbildungen und Inhalten hinzuf√ºgen\]

## Erster Web-Scraping-Versuch

Zuvor haben wir uns einen √úberblick √ºber die zu extrahierenden Webinhalte verschafft. F√ºr den ersten Web-Scraping-Versuch nutzen wir weiterhin den Beitrag mit dem Titel *"‚ÄûSch√§mt Euch!" -- Deutschland steht vor den Vereinten Nationen am Pranger, weil es die Inklusion an Schulen praktisch verweigert"* (@fig-struktur1). Dies ist der Link zum Beitrag:

<https://www.news4teachers.de/2023/08/schaemt-euch-deutschland-steht-vor-den-vereinten-nationen-am-pranger-weil-es-die-inklusion-an-schulen-verweigert/>

Wir nutzen den Befehl `read_html()` und den entsprechenden Link, um s√§mtliche Informationen von der Webseite zu extrahieren:

```{r}
html <-
  read_html("https://www.news4teachers.de/2023/08/schaemt-euch-deutschland-steht-vor-den-vereinten-nationen-am-pranger-weil-es-die-inklusion-an-schulen-verweigert/")

#html <- read_html("https://www.news4teachers.de/2012/02/im-kern-sind-wir-uns-einig-kein-streit-mehr-uber-die-struktur/")

#html <- read_html("https://www.news4teachers.de/2013/03/studie-hochbegabte-sind-besser-unter-sich/")

#html <- read_html("https://www.news4teachers.de/2019/03/mobbing-ritual-unter-grundschuelern-bringt-politik-in-bewegung/")
```

Alle Webinhalte sind nun im Objekt `html` hinterlegt. Wir sind allerdings nur an spezifischen Webinhalten interessiert und m√∂chten daher im n√§chsten Schritt einen spezifischen Textinhalt aus dem Objekt `html` auslesen. Beginnen wir mit einem Textinhalt, welcher sich relativ leicht extrahieren l√§sst. Wir wollen den Titel des Beitrages extrahieren: *"‚ÄûSch√§mt Euch!" -- Deutschland steht vor den Vereinten Nationen am Pranger, weil es die Inklusion an Schulen praktisch verweigert"*. Dabei ist es gar nicht so leicht, einen spezifischen Inhalt wie den Titel zu lokalisieren und auszulesen. Hierf√ºr ist HTML-[^4] und CSS-Selector-Grundlagenwissen[^5] hilfreich. Die eigentlichen Textinhalte sind n√§mlich im HTML-Dokument der Webseite hinterlegt (HTML-Quelltext). Ist eine Internetseite im Browser ge√∂ffnet, so gelangen wir mit einem Rechtsklick i.d.R. zur Option *"Seitenquelltext anzeigen"* (@fig-quelltext). Dies f√ºhrt uns zum HTML-Dokument der Webseite (@fig-html1).

[^4]: <https://developer.mozilla.org/en-US/docs/Web/HTML>

[^5]: <https://developer.mozilla.org/en-US/docs/Learn/CSS/Building_blocks/Selectors>; *"CSS includes a miniature language for selecting elements on a page called CSS selectors. CSS selectors define patterns for locating HTML elements, and are useful for scraping because they provide a concise way of describing which elements you want to extract."*, Quelle: <https://rvest.tidyverse.org/articles/rvest.html>

![Seitenquelltext (HTML) anzeigen](images/quelltext.png){#fig-quelltext fig-alt="Seitenquelltext (HTML) anzeigen"}

::: column-body-outset-left
![HTML-Dokument/Seitenquelltext (Ausschnitt)](images/html1.png){#fig-html1 fig-alt="HTML-Dokument (Ausschnitt)"}
:::

Das HTML-Dokument (@fig-html1) ist riesig (mehr als 10000 Zeilen) und wir m√ºssen etwas st√∂bern, um den passenden Webinhalt zu lokalisieren. Der HTML-Code aus @fig-html1 ist zwecks besserer Lesbarkeit auch nachfolgend dargestellt:

```{html}
<article id="post-132285" class="post-132285 post type-post status-publish format-standard has-post-thumbnail category-leben category-titelthema category-wissenschaft tag-forderschulen tag-inklusion tag-un-behindertenrechtskonvention" itemscope itemtype="https://schema.org/Article">
    <div class="td-post-header">
        <!-- category -->
        <ul class="td-category">
            <li class="entry-category"><a href="https://www.news4teachers.de/bildung/leben/">Leben</a></li>
            <li class="entry-category"><a href="https://www.news4teachers.de/bildung/titelthema/">Titelthema</a></li>
            <li class="entry-category"><a href="https://www.news4teachers.de/bildung/wissenschaft/">Wissen</a></li>
        </ul>
        <header class="td-post-title">
            <h1 class="entry-title">&#8222;Sch√§mt Euch!&#8220; &#8211; Deutschland steht vor den Vereinten Nationen am Pranger, weil es die Inklusion an Schulen praktisch verweigert</h1>
            <div class="td-module-meta-info">
                <!-- author -->
                <!-- date -->
                <span class="td-post-date">
                    <time class="entry-date updated td-module-date" datetime="2023-08-29T12:46:06+02:00">29. August 2023</time>
                </span>
                <!-- comments -->
                <div class="td-post-comments">
                    <a href="https://www.news4teachers.de/2023/08/schaemt-euch-deutschland-steht-vor-den-vereinten-nationen-am-pranger-weil-es-die-inklusion-an-schulen-verweigert/#comments">
                        <i class="td-icon-comments"></i>150
                    </a>
                </div>
                <!-- views -->
            </div>
        </header>
    </div>
</article>
```

Wir sehen z.B. in der Zeile 1297 (@fig-html1), dass der Titel des Beitrages ein `h1`-HTML-Element[^6] ist (header 1: √úberschrift erster Ebene):

[^6]: <https://developer.mozilla.org/en-US/docs/Web/HTML/Element/Heading_Elements>

```{html}
<h1 class="entry-title">&#8222;Sch√§mt Euch!&#8220; &#8211; Deutschland steht vor den Vereinten Nationen am Pranger, weil es die Inklusion an Schulen praktisch verweigert</h1>
```

Diese Information ben√∂tigen wir, um den Titel des Beitrags gezielt auszulesen. Hierf√ºr nutzen wir den Befehl `html_elements("h1")` und √ºbergeben das Objekt `html` an diesen Befehl.

```{r}
html |> html_elements("h1")
```

Die Information `<h1 class="entry-title">` ist √ºberfl√º√üig, da wir nur am HTML-Textinhalt interessiert sind. Daher extrahieren wir den reinen Textinhalt, also den Titel, mit dem Befehl `html_text()`. Die Befehlskette wird entsprechend erweitert:

```{r}
html |>
  html_elements("h1") |>
  html_text()
```

Herzlichen Gl√ºckwunsch! ü•≥ Somit haben wir erfolgreich alle Informationen von der Webseite extrahiert und eine relevante Textstelle (den Titel) ausgelesen.

### Datenstruktur

Im HTML-Seitenquelltext (@fig-html1) sehen wir, dass anscheinend jeder Beitrag √ºber eine ID verf√ºgt (`id="post-132285"`). Wenn wir in unserem zuk√ºnftigen Datensatz mehrere Beitr√§ge abspeichern wollen, dann wird eine ID-Variable zwecks Unterscheidung der Beitr√§ge eine hilfreiche Sache sein. @tbl-idee ist eine erste Idee bez√ºglich einer m√∂glichen/sinnvollen Datenstruktur. Bei dieser Datenstruktur ignorieren wir der Einfachheit halber vorerst ein paar relevante Webinhalte, z.B. Kommentare und Anzahl der Likes ("*Gef√§llt mir*").

\newpage

\blandscape

```{r}
#| echo: false
#| label: tbl-idee
#| tbl-cap: Erste Idee bez√ºglich einer m√∂glichen/sinnvollen Datenstruktur
ID <- c(132285, "...", "...")
link <- c("https://...", "...", "...")
datum <- c("29.\nAugust\n2023", "...", "...")
n_kommentare <- c("150", "...", "...")
ort <- c("GENF", "...", "...")
titel <- c("'Sch√§mt Euch!' --\nDeutschland steht\nvor den Vereinten\nNationen am Pranger...", "...", "...")
zusammenfassung <- c("‚ÄûSch√§mt Euch!‚Äú --\nso hei√üt es auf\neinem Transparent...", "...", "...")
haupttext <- c("Der offizielle Beitrag\nDeutschlands f√§llt\nd√ºnn aus...", "...", "...")
usw. <- c("...", "...", "...")

data.frame(ID, link, datum, n_kommentare, ort, titel, zusammenfassung, haupttext, usw.) |>
  flextable() |>
  #autofit() |>
  bold(part = "header") |>
  rotate(rotation = "tbrl", align = "bottom", part = "header") |>
  width(j = 1:5, width = 0.6) |>
  width(j = 2, width = 0.8) |>
  width(j = 6:8, width = 1.5) |>
  width(j = 9, width = 0.3) |>
  height(height = 5, part = "header")
```

\elandscape

[Gef√§llt mir???]

### Weitere Web-Scraping-Schritte {#sec-schritte}

Um die Datenstruktur aus @tbl-idee zu realisieren, m√ºssen wir nun die ID des Beitrags, den Link, das Erscheinungsdatum, die Anzahl der Kommentare, die Zusammenfassung und den eigentlichen Haupttext des Beitrages auslesen (den Titel haben wir ja bereits erfolgreich extrahiert). Beginnen wir mit der ID.

#### ID {#sec-link}

In @fig-html1 sehen wir, das die ID des Beitrages (`id="post-132285"`) ein Attribut[^7] eines HTML-Elements ist (HTML-Element: `article`[^8]):

[^7]: <https://developer.mozilla.org/en-US/docs/Web/HTML/Global_attributes/id>

[^8]: <https://developer.mozilla.org/en-US/docs/Web/HTML/Element/article>

```{html}
<article id="post-132285" class="post-132285 post type-post status-publish format-standard has-post-thumbnail category-leben category-titelthema category-wissenschaft tag-forderschulen tag-inklusion tag-un-behindertenrechtskonvention" itemscope itemtype="https://schema.org/Article">
```

Daher √ºbergeben wir das Objekt `html` zwecks Auslesung der ID zun√§chst an den Befehl `html_elements("article")` und dann an den Befehl `html_attr("id")`:

```{r}
html |>
  html_elements("article") |>
  html_attr("id")
```

Die ID des Beitrags erscheint mit dem Pr√§fix `"post-"`, eine nicht notwendigerweise n√ºtzliche Information. Das Pr√§fix entfernen wir daher mit dem Befehl `str_remove("post-")` und √ºberf√ºhren die ID mit dem Befehl `as.numeric()` in ein nummerisches Format. Somit erhalten wir die nummerische ID `132285`:

```{r}
html |>
  html_elements("article") |>
  html_attr("id") |>
  str_remove("post-") |> # R-Zusatzpaket stringr (tidyverse)
  as.numeric()
```

#### Link

Der Beitrag verf√ºgt √ºber einen langen Link:

<https://www.news4teachers.de/2023/08/schaemt-euch-deutschland-steht-vor-den-vereinten-nationen-am-pranger-weil-es-die-inklusion-an-schulen-verweigert/>

Im HTML-Quelltext ist allerdings auch ein kurzer Link, also ein `shortlink`, ausgewiesen:

```{html}
<link rel='shortlink' href='https://www.news4teachers.de/?p=132285' />
```

Die ID des Beitrags (`132285`) ist Bestandteil des kurzen Links. Wir k√∂nnen also den ersten Teil des kurzen Links (`"https://www.news4teachers.de/?p="`) mit der ID (`132285`) verbinden, um den gew√ºnschten Kurzlink zu generieren. Hierf√ºr nutzen wir nach der Auslesung der ID den Befehl `paste0("https://www.news4teachers.de/?p=", .)`. Mit dem magrittr-Pipe-Operator (`%>%`[^9]) wird die ID an das zweite Argument des Befehls `paste0("https://www.news4teachers.de/?p=", .)` √ºbergeben, also an die Stelle mit dem Punkt (`.`). Eine √úbergabe an das zweite Argument w√§re mit der sogenannten base-Pipe (`|>`) nicht m√∂glich, daher nutzen wir die magrittr-Pipe (`%>%`). Die Befehlskette zur Erstellung des Links gestaltet sich somit folgenderma√üen:

[^9]: <https://magrittr.tidyverse.org/>

```{r}
html |>
  html_elements("article") |>
  html_attr("id") |>
  str_remove("post-") |> # R-Zusatzpaket stringr (tidyverse)
  as.numeric() %>% # Pipe-Operator, R-Zusatzpaket magrittr (tidyverse)
  paste0("https://www.news4teachers.de/?p=", .) 
```

#### Erscheinungsdatum

Fahren wir fort mit dem Auslesen des Erscheinungsdatums des Beitrages. Im HTML-Quelltext (@fig-html1, Zeile 1301) erscheint folgende Information:

```{html}
<span class="td-post-date"><time class="entry-date updated td-module-date" datetime="2023-08-29T12:46:06+02:00" >29. August 2023</time></span> 
```

Wir sehen, dass das Datum ein HTML-Element ist, n√§mlich ein `time`-Element[^10]. Dieses `time`-Element ist innerhalb eines `span`-Elements[^11] geschachtelt. Wir k√∂nnen hier also von einer hierarchischen Schachtelung der HTML-Elemente sprechen (`span -> time`, @fig-schachtelung_1).

[^10]: <https://developer.mozilla.org/en-US/docs/Web/HTML/Element/time>

[^11]: <https://developer.mozilla.org/en-US/docs/Web/HTML/Element/span>

```{mermaid}
%%| label: fig-schachtelung_1
%%| fig-cap: Hierarchische Schachtelung der HTML-Elemente `span` und `time`
%%{init: {'theme':'forest'}}%%
flowchart TB
  id1(span) --> id2(time)
```

Entsprechend erfolgt die Extraktion des Datums mit der √úbergabe des Objektes `html`, zun√§chst an den Befehl `html_elements("span")`, und anschlie√üend an den Befehl `html_elements("time")`:

```{r}
html |>
  html_elements("span") |>
  html_elements("time")
```

Das Ergebnis ist aber nicht ganz befriedigend, da mehrere Datumsangaben extrahiert worden sind, unter anderem das gew√ºnschte Erscheinugsdatum des Beitrages (`2023-08-29`), aber auch andere, nicht relevate Datumsangaben (z.B. `2023-09-17`), welche ebenfalls auf der Webseite erscheinen (@fig-datum).

![Verweis auf einen anderen Beitrag mit nicht relevanter Datumsangabe](images/datum.png){#fig-datum fig-alt="Verweis auf einen anderen Beitrag mit nicht relevanter Datumsangabe"}

Wir m√ºssen daher beim Auslesen noch genauer die hierarchische Position des Erscheinungsdatums definieren. Ein Blick auf @fig-html1 offenbart, dass die beiden HTML-Elemente `span` und `time` innerhalb des bereits bekannten HTML-Elements `article` geschachtelt sind (`article -> span -> time`, @fig-schachtelung_2).

```{mermaid}
%%| label: fig-schachtelung_2
%%| fig-cap: Hierarchische Schachtelung der HTML-Elemente `article`, `span` und `time`
%%{init: {'theme':'forest'}}%%
flowchart TB
  id1(article) --> id2(span) --> id3(time)
```

Diese hierarchische Schachtelung (`article -> span -> time`) muss daher beim Auslesen des Erscheingsdatums beachtet werden:

```{r}
html |>
  html_elements("article") |>
  html_elements("span") |>
  html_elements("time")
```

Das Erscheinungsdatum ist in diesem Falle das einzige `time`-Element innerhalb des `article`-Elements. Daher f√ºhrt auch das Weglassen des `span`-Elements und somit die Anwendung einer reduzierten hierarchischen Schachtelung der HTML-Elemente (`article -> time`, @fig-schachtelung_3) zum gew√ºnschten Erfolg:

```{mermaid}
%%| label: fig-schachtelung_3
%%| fig-cap: Reduzierte hierarchische Schachtelung der HTML-Elemente `article` und `time`
%%{init: {'theme':'forest'}}%%
flowchart TB
  id1(article) --> id2(time)
```

```{r}
html |>
  html_elements("article") |>
  html_elements("time")
```

Auch bei der Datumsangabe wollen wir uns auf die wesentliche Information fokussieren und extrahieren daher die reine Datumsangabe, die dem Attribut `"datetime"` zugeordnet ist. Die Befehlskette wird daher um den Befehl `"html_attr("datetime")"` erg√§nzt:

```{r}
html |>
  html_elements("article") |>
  html_elements("time") |>
  html_attr("datetime")
```

Die Datumsangabe (`"2023-08-29T12:46:06+02:00"`) beinhaltet eine f√ºr uns nicht relevante Zeitangabe, also die genaue Uhrzeit der Beitragserscheinung (`T12:46:06+02:00`). Die ersten 10 Zeichen (inkl. Bindestriche: `JJJJ-MM-TT`/`2023-08-29`) beibehalten die relevante Datumsangabe. Die nicht relevante Zeitangabe entfernen wir, indem wir lediglich die ersten 10 Zeichen der Datumsangabe beibehalten. Hierf√ºr erg√§nzen wir die Befehlskette um den Befehl `str_sub(end = 10)`:

```{r}
html |>
  html_elements("article") |>
  html_elements("time") |>
  html_attr("datetime") |>
  str_sub(end = 10) # R-Zusatzpaket stringr (tidyverse)
```

#### Anzahl der Kommentare

Die Anzahl der Kommentare ist im HTML-Element `div`^[<https://developer.mozilla.org/en-US/docs/Web/HTML/Element/div>] hinterlegt. Und dieses HTML-Element `div` ist durch eine CSS-Klasse[^100] gekennzeichnet (`class="wpd-thread-info"`), wobei die eigentliche Anzahl der Kommentare ein HTML-Attribut ist (`data-comments-count="150"`). Im HTML-Seitenquelltext sieht dies folgenderma√üen aus:

[^100]: <https://developer.mozilla.org/en-US/docs/Learn/CSS/Building_blocks/Selectors>; *"CSS includes a miniature language for selecting elements on a page called CSS selectors. CSS selectors define patterns for locating HTML elements, and are useful for scraping because they provide a concise way of describing which elements you want to extract."*, Quelle: <https://rvest.tidyverse.org/articles/rvest.html>

```{html}
<div class="wpd-thread-info" data-comments-count="150"><span class='wpdtc' title='150'>150</span> Kommentare </div>
```

Das Objekt `html` wird daher an den Befehl `html_elements("div")` √ºbergeben. Die anschlie√üende Angabe der CSS-Klasse `"wpd-thread-info"` erfolgt mit einem vorangestellten Punkt (`".wpd-thread-info"`) innerhalb des Befehls `html_elements(".wpd-thread-info")`:

```{r}
html |>
  html_elements("div") |>
  html_elements(".wpd-thread-info")
```
Die eigentliche Extraktion der Anzahl der Kommentare erfolgt mit der Angabe des entsprechenden HTML-Attributes innerhalb des Befehls `html_attr("data-comments-count")`. Die Anzahl der Kommentare wird mit dem Befehl `as.numeric()` in ein nummerisches Format √ºberf√ºhrt. Die Befehlskette gestaltet sich daher folgenderma√üen:

```{r}
html |>
  html_elements("div") |>
  html_elements(".wpd-thread-info") |>
  html_attr("data-comments-count") |>
  as.numeric()
```

#### Ort der Berichterstattung

Die Ortsangabe ist Bestandteil der Zusammenfassung (siehe @fig-struktur1) und die Zusammenfassung ist ein Absatz, i.d.R. der erste Absatz des Beitrages. F√ºr die Extraktion der Ortsangabe ist daher das HTML-Element f√ºr Abs√§tze notwendig (`p`[^12]; `p` steht f√ºr "paragraph"). Dieses HTML-Element (`p`) ist wie gewohnt innerhalb des HTML-Elements `article` geschachtelt. Zur Extraktion des ersten Absatzes wird diesmal der Befehl `html_element("p")` anstatt `html_elements("p")` genutzt. Der Befehl `html_elements("p")` w√ºrde alle Abs√§tze des Beitrages extrahieren. Wir ben√∂tigen aber nur den ersten Absatz mit der Ortsangabe und daher nutzen wir diesmal den Befehl `html_element("p")` anstatt `html_elements("p")`. Die Befehlskette gestaltet sich daher wie folgt:

[^12]: <https://developer.mozilla.org/en-US/docs/Web/HTML/Element/p>

```{r}
html |>
  html_elements("article") |>
  html_element("p") |> # html_element anstatt html_elements
  html_text()
```

Somit sehen wir den Absatz mit der Ortsangabe. Wir ben√∂tigen allerdings nur die Ortsangabe, also das erste Wort des Absatzes. Hinter der gew√ºnschten Ortsangabe steht ein Punkt (`GENF.`). Mit dem Befehl `str_extract("[^.]+")`[^13] extrahieren wir alle Zeichen vor dem ersten Punkt, also die Ortsangabe `GENF`. Die Befehlskette gestaltet sich daher wie folgt:

[^13]: Bei so einer kryptischen Formel (`"[^.]+"`) handelt es sich um eine sogenannte "*regular expression*" (*regex*). Eine Einf√ºhrung in diese Thematik findet man hier: <https://r4ds.hadley.nz/regexps> [@Wickham2023-hx, Kapitel 16].

```{r}
html |>
  html_elements("article") |>
  html_element("p") |> # html_element anstatt html_elements
  html_text() |>
  str_extract("[^\\.]+") # R-Zusatzpaket stringr (tidyverse)
```

#### Zusammenfassung des Beitrages

Wie soeben bei der Extraktion der Ortsangabe erw√§hnt, ist die Zusammenfassung des Beitrages der erste Absatz des Textes (siehe @fig-struktur1). Der erste Absatz wurde soeben folgenderma√üen extrahiert:

```{r}
html |>
  html_elements("article") |>
  html_element("p") |> # html_element anstatt html_elements
  html_text()
```

Somit erhalten wir die Zusammenfassung mit der Ortsangabe inkl. Punkt (`GENF.`). Nun wollen wir die √ºberfl√ºssige Ortsangabe entfernen und nur die eigentliche Zusammenfassung beibehalten. Dies erreichen wir mit dem Befehl `str_extract("\\.[\\s](.*)")`. Die regex-Formel `"\\.[\\s](.*)"` hat folgende Bedeutung:

-   `\\.` Suche und extrahiere Zeichen nach dem ersten Punkt (einschlie√ülich des ersten Punktes)
-   `[\\s]` Die extrahierten Zeichen k√∂nnen Leerzeichen sein ("*s*" steht f√ºr "*space*")
-   `(.*)` Extrahiere au√üerdem alle weiteren Zeichen

Die Befehlskette gestaltet sich daher wie folgt:

```{r}
html |>
  html_elements("article") |>
  html_element("p") |> # html_element anstatt html_elements
  html_text() |>
  str_extract("\\.[\\s](.*)") # R-Zusatzpaket stringr (tidyverse)
```

Die Ortsangabe (`GENF`) wurde erfolgreich entfernt. Der Punkt hinter der Ortsangabe (`GENF.`) wurde allerdings nicht entfernt und bleibt bestehen. Die Zusammenfassung beginnt daher nun mit einem Punkt (`.`) gefolgt von einem Leerzeichen. Wir entfernen den Punkt und das Leerzeichen (`". "`) mit dem Befehl `str_remove(". ")`. Die Befehlskette zur Extraktion der Zusammenfassung gestaltet sich daher folgenderma√üen:

```{r}
html |>
  html_elements("article") |>
  html_element("p") |> # html_element anstatt html_elements
  html_text() |>
  str_extract("\\.[\\s](.*)") |> # R-Zusatzpaket stringr (tidyverse)
  str_remove(". ") # R-Zusatzpaket stringr (tidyverse)
```

#### Haupttext

Kommen wir nun zum Filetst√ºck, also zum eigentlichen Haupttext des Beitrages. Der Beitragstext besteht aus Abs√§tzen. Also k√∂nnen wir, wie bereits gewohnt, das HTML-Element `p` ber√ºcksichtigen. Und dieses HTML-Element `p` ist bekannterweise innerhalb des HTML-Elements `article` geschachtelt:

[X-PATH erkl√§ren und anwenden √úberall]

```{r}
# Seitenquelltext (HTML)
html |>
  html_elements(xpath = "//article/div/p[not(descendant::blockquote or iframe)]")

# Text
html |>
  html_elements(xpath = "//article/div/p[not(descendant::blockquote or iframe)]") |>
  html_text()
```

Mit der obigen Befehlskette werden 25 Abs√§tze extrahiert. Aber nur 11 Abs√§tze geh√∂ren zum eigentlichen Haupttext (4 bis 11, 14, 18 und 19). Es ist also auch eine Menge Ged√∂ns dabei, also prim√§r nicht relevante Informationen, verstreut innerhalb des Beitrages, z.B. Verweise auf externe Quellen:

`[2] "#InklusiveBildungJETZT\nWir sind auf dem Place de Nations angekommen! Die #Staatenpruefung f√ºr Deutschland beginnt in K√ºrze.#WirFahrenNachGenf pic.twitter.com/XoBPh69iUO"`

`[20] "Hier geht es zum vollst√§ndigen ‚ÄûGemeinsamen Bericht der Zivilgesellschaft zum 2. und 3. Bericht der Bundesregierung zur Umsetzung der UN-Behindertenrechtskonvention durch Deutschland‚Äú."`

`[21] "Der Beitrag wird auch auf der Facebook-Seite von News4teachers hei√ü diskutiert."`

Eigentlich m√ºsste man hier eine Systematik entwickeln, um die inhaltlich nicht relevanten Abs√§tze zu entfernen. Man k√∂nnte z.B. alle Abs√§tze entfernen, die die W√∂rter "*Twitter*" oder "*Facebook*" enthalten. Mit dieser Systematik w√ºrde man aber Gefahr laufen, dass man auch f√§lschlicherweise inhaltlich relevante Abs√§tze entfernt, z.B. einen Absatz, der darauf verweist, dass sich ein Bildungspolitiker bei "*Twitter*" kritisch zum Thema Inklusion ge√§u√üert hat. Wir m√ºssen also Abs√§tze mit weniger oder nicht relevanten Inhalten in Kauf nehmen.

Aber wir haben auch drei Abs√§tze ohne sinnvollen Inhalt (`[22] ""`, `[24] ""` und `[25] " "`), da diese Abs√§tze keine Zeichen oder ausschlie√ülich Leerzeichen enthalten. Wir wollen daher Abs√§tze, die keine Zeichen oder ausschlie√ülich Leerzeichen enthalten, entfernen. Hierf√ºr speichern wir vor√ºbergehend alle Abs√§tze im Textformat als Objekt `all_p` ab. Wir √úberpr√ºfen anschlie√üend, ob der Textinhalt der Abs√§tze  keine Zeichen oder ausschlie√ülich Leerzeichen enth√§lt (`all_p |> str_detect("^\\s*$")`). Die regex-Formel `"^\\s*$"` hat folgende Bedeutung:

-   `^` Beginne die Suche am Anfang des Absatzes
-   `\\s*` Suche Leerzeichen bzw. keine Zeichen ("*s*" steht f√ºr "*space*")
-   `$` F√ºhre diese Suche bis zum Ende des Absatzes durch

Das Ergebnis der √úberpr√ºfung ist eine `TRUE`-`FALSE`-Aussage f√ºr jeden Absatz (`TRUE`: Keine Zeichen oder ausschlie√ülich Leerzeichen; `FALSE`: Andere Zeichen). Diese `TRUE`-`FALSE`-Aussage speichern wir als Objekt `spaces` ab und lassen uns abschlie√üend Abs√§tze anzeigen, welche nicht ausschlie√ülich aus Leerzeichen oder keinen Zeichen bestehen (`all_p[!spaces]`):

```{r}
all_p <-
  html |>
  html_elements("article") |>
  html_elements("p") |>
  html_text()

all_p |>
  str_detect("^\\s*$") # R-Zusatzpaket stringr (tidyverse)

spaces <-
  all_p |>
  str_detect("^\\s*$") # R-Zusatzpaket stringr (tidyverse)

all_p[!spaces]
```

Fast geschafft! Wir m√ºssen nur noch den ersten Absatz entfernen (`all_p[!spaces] %>% .[-1]`), da der erste Absatz die Zusammenfassung mit Ortsangabe darstellt (`[1] "GENF. ‚ÄûSch√§mt Euch!‚Äú ‚Äì so hei√üt es auf einem Transparent..."`) und daher nicht zum Haupttext gez√§hlt werden kann. Der Punkt vor der eckigen Klammer (`.[-1]`) symbolisiert die Abs√§tze (`all_p[!spaces]`). Bei der √úbergabe dieser Abs√§tze nutzen wir, wie zuvor beim Auslesen des Links (@sec-link), die magrittr-Pipe (`%>%`), da nur diese Pipe, und nicht die base-Pipe (`|>`), eine √úbergabe an eine Bedingung in eckigen Klammern erm√∂glicht (`.[-1]`). Alle Abs√§tze werden anschlie√üend mit dem Befehl `list()` in eine Liste √ºberf√ºhrt. Dies ist daher die schlussendliche Befehlskette:

```{r}
all_p[!spaces] %>% # Pipe-Operator, R-Zusatzpaket magrittr (tidyverse)
  .[-1] |>
  list()
```

<!-- 

Wir m√ºssen diese nicht relevanten Abs√§tze also mittels einer geeigneten Systematik entfernen. Woran erkennen wir die nicht relevanten Abs√§tze? Es sind Verweise auf externe Quellen und diese Verweise enthalten Links auf externe Quellen. Im n√§chsten Schritt wollen wir daher Abs√§tze mit Links entfernen. Ob in einem Absatz ein Link vorhanden ist, erkennen wir am HTML-Element `a`[^15] (`a` steht f√ºr "*anchor*"), z.B. beim Verweis auf externe Inhalte bei Twitter (@fig-struktur1 und @fig-struktur2):

[^15]: <https://developer.mozilla.org/en-US/docs/Web/HTML/Element/a>

`"<p dir=\"ltr\" lang=\"de\"><a href=\"https://twitter.com/hashtag/InklusiveBildungJETZT?src=hash&amp;ref_src=twsrc%5Etfw\">#InklusiveBildungJETZT</a><br>\nWir sind auf dem Place de Nations angekommen! Die <a href=\"https://twitter.com/hashtag/Staatenpruefung?src=hash&amp;ref_src=twsrc%5Etfw\">#Staatenpruefung</a> f√ºr Deutschland beginnt in K√ºrze.<a href=\"https://twitter.com/hashtag/WirFahrenNachGenf?src=hash&amp;ref_src=twsrc%5Etfw\">#WirFahrenNachGenf</a> <a href=\"https://t.co/XoBPh69iUO\">pic.twitter.com/XoBPh69iUO</a></p>"`

Das Ende eines Links wird also immer mit dem HTML-Element `</a>` gekennzeichnet sein. Wir wollen nun alle Abs√§tze entfernen, die das HTML-Element `</a>` enthalten[^16]. Hierf√ºr speichern wir vor√ºbergehend alle Abs√§tze im HTML-Format als Objekt `all_p` ab. Wir √ºberf√ºhren alle Abs√§tze (`all_p`) mit dem Befehl `as.character()` ins reine Textformat und √ºberpr√ºfen anschlie√üend mit dem Befehl `str_detect("</a>")`, ob Verlinkungen in den Abs√§tzen vorhanden sind. Das Ergebnis ist ein `TRUE`-`FALSE`-Aussage f√ºr jeden Absatz (`TRUE`: Link vorhanden; `FALSE`: Link nicht vorhanden). Diese `TRUE`-`FALSE`-Aussage speichern wir als Objekt `link` ab und lassen uns abschlie√üend anzeigen, welche Abs√§tze keine Links enthalten (`all_p[!link] |> html_text()`):

[^16]: Bei diesem Arbeitsschritt habe ich die R-Community um Unterst√ºtzung gebeten: <https://stackoverflow.com/questions/77152943/>

```{r}
all_p <-
  html |>
  html_elements("article") |>
  html_elements("p")
  
all_p |>
  as.character() |>
  str_detect("</a>") # R-Zusatzpaket stringr (tidyverse)

link <-
  all_p |>
  as.character() |>
  str_detect("</a>") # R-Zusatzpaket stringr (tidyverse)

all_p[!link] |>
  html_text()
```

Das Ergebnis ist schon ziemlich befriedigend: Abs√§tze mit Links wurden entfernt. Aber wir haben auch einen inhaltlich relevanten Absatz entfernt. Ein Absatz enth√§lt n√§mlich einen Link und ist zugleich inhaltlich relevant (@fig-absatz).

![Inhaltlich relevanter Absatz mit Link ("News4teachers berichtete")](images/absatz.png){#fig-absatz fig-alt="Inhaltlich relevanter Absatz mit Link (News4teachers berichtete)"}

Wir m√ºssen unsere Systematik daher erweitern und entsprechende Abs√§tze mit "*News4teachers berichtete*"-Links beibehalten, da diese Abs√§tze inhaltlich relevant erscheinen. Dies erreichen wir, in dem wir alle Abs√§tze (`all_p`) dahingehend √ºberpr√ºfen, ob die Zeichenabfolge `"News4teachers berichtete"` in den Abs√§tzen vorhanden ist (`str_detect("News4teachers berichtete")`). Das Ergebnis dieser √úberpr√ºfung ist wieder eine `TRUE`-`FALSE`-Aussage f√ºr jeden Absatz (`TRUE`: "*News4teachers berichtete*"-Link vorhanden; `FALSE`: "*News4teachers berichtete*"-Link nicht vorhanden). Diese `TRUE`-`FALSE`-Aussage speichern wir als Objekt `n4t_link` ab und lassen uns abschlie√üend Abs√§tze anzeigen, welche "*News4teachers berichtete*"-Links enthalten, aber keine anderen Links enthalten (`all_p[n4t_link | !link] |> html_text()`):

```{r}
all_p |>
  as.character() |>
  str_detect("News4teachers berichtete") # R-Zusatzpaket stringr (tidyverse)

n4t_link <-
  all_p |>
  as.character() |>
  str_detect("News4teachers berichtete") # R-Zusatzpaket stringr (tidyverse)

all_p[!link | n4t_link] |>
  html_text()
```

Das sieht schon ziemlich gut aus. Der inhaltlich relevante Absatz mit ‚Äú*News4teachers berichtete*‚Äù-Link (@fig-absatz) wurde nun beibehalten (`"[4] "Das Deutsche Institut f√ºr Menschenrechte ..."`). Aber wir haben zwei Abs√§tze ohne sinnvollen Inhalt (`[13] ""` und `[14] " "`), da diese Abs√§tze keine Zeichen oder ausschlie√ülich Leerzeichen enthalten. Wir wollen daher Abs√§tze, die keine Zeichen oder ausschlie√ülich Leerzeichen enthalten, entfernen. Wir √úberpr√ºfen daher, ob der Textinhalt der Abs√§tze (`all_p[!link | n4t_link] |> html_text()`) keine Zeichen oder ausschlie√ülich Leerzeichen enth√§lt (`str_detect("^\\s*$")`). Die regex-Formel `"^\\s*$"` hat folgende Bedeutung:

-   `^` Beginne die Suche am Anfang des Absatzes
-   `\\s*` Suche Leerzeichen bzw. keine Zeichen ("*s*" steht f√ºr "*space*")
-   `$` F√ºhre diese Suche bis zum Ende des Absatzes durch

Das Ergebnis der √úberpr√ºfung ist erneut eine `TRUE`-`FALSE`-Aussage f√ºr jeden Absatz (`TRUE`: keine Zeichen oder ausschlie√ülich Leerzeichen). Diese `TRUE`-`FALSE`-Aussage speichern wir als Objekt `spaces` ab und lassen uns abschlie√üend Abs√§tze anzeigen, welche nicht ausschlie√ülich aus Leerzeichen oder keinen Zeichen bestehen (`.[!spaces]`). Der Punkt (`.`) symbolisiert die Abs√§tze, welche zuvor mittels `all_p[!link | n4t_link] |> html_text()` erzeugt worden sind. Bei der √úbergabe dieser Abs√§tze nutzen wir, wie zuvor beim Auslesen des Links (@sec-link), die magrittr-Pipe (`%>%`), da nur diese Pipe, und nicht die base-Pipe (`|>`), eine √úbergabe an eine Bedingung in eckigen Klammern erm√∂glicht (`.[!spaces]`):

```{r}
all_p[!link | n4t_link] |>
  html_text() |>
  str_detect("^\\s*$") # R-Zusatzpaket stringr (tidyverse)

spaces <-
  all_p[!link | n4t_link] |>
  html_text() |>
  str_detect("^\\s*$") # R-Zusatzpaket stringr (tidyverse)

all_p[!link | n4t_link] |>
  html_text() %>% # Pipe-Operator, R-Zusatzpaket magrittr (tidyverse)
  .[!spaces]
```

Fast geschafft! Wir m√ºssen nur noch den ersten Absatz entfernen (`.[-1]`), da der erste Absatz die Zusammenfassung mit Ortsangabe darstellt (`[1] "GENF. ‚ÄûSch√§mt Euch!‚Äú ‚Äì so hei√üt es auf einem Transparent..."`) und daher nicht zum Haupttext gez√§hlt werden kann. Dies ist daher die schlussendliche Befehlskette:

```{r}
all_p[!link | n4t_link] |>
  html_text() %>% # Pipe-Operator, R-Zusatzpaket magrittr (tidyverse)
  .[!spaces] %>% # Pipe-Operator, R-Zusatzpaket magrittr (tidyverse)
  .[-1]
```

-->

### Datensatz

Die bisher erfolgreich ausgelesenen Informationen (@sec-schritte: ID, Link, Erscheinungsdatum, Anzahl der Kommentare, Ort der Berichterstattung, Titel, Zusammenfassung des Beitrages und Haupttext) k√∂nnen wir nun in einem Datensatz zusammenfassen (vgl. @tbl-idee). Hierf√ºr wiederholen wir pro forma und zwecks √úbersichtlichkeit alle bisherigen Arbeitsschritte und speichern jede einzelne Information als Objekt (`ID`, `link`, `datum`, `n_kommentare`, `ort`, `titel`, `zusammenfassung` und `text`):

```{r}
# Alle Informationen von der Webseite extrahieren

html <-
  read_html("https://www.news4teachers.de/2023/08/schaemt-euch-deutschland-steht-vor-den-vereinten-nationen-am-pranger-weil-es-die-inklusion-an-schulen-verweigert/")

# ID

ID <-
  html |>
  html_elements("article") |>
  html_attr("id")

# Link

link <-
  html |>
  html_elements("article") |>
  html_attr("id") |>
  str_remove("post-") |> # R-Zusatzpaket stringr (tidyverse)
  as.numeric() %>% # Pipe-Operator, R-Zusatzpaket magrittr (tidyverse)
  paste0("https://www.news4teachers.de/?p=", .)

# Erscheinungsdatum

datum <-
  html |>
  html_elements("article") |>
  html_elements("time") |>
  html_attr("datetime") |>
  str_sub(end = 10) # R-Zusatzpaket stringr (tidyverse)

# Anzahl der kommentare

n_kommentare <-
  html |>
  html_elements("div") |>
  html_elements(".wpd-thread-info") |>
  html_attr("data-comments-count") |>
  as.numeric()

# Ort der Berichterstattung

ort <-
  html |>
  html_elements("article") |>
  html_element("p") |> # html_element anstatt html_elements
  html_text() |>
  str_extract("[^\\.]+") # R-Zusatzpaket stringr (tidyverse)

# Titel

titel <-
  html |>
  html_elements("h1") |>
  html_text()

# Zusammenfassung

zusammenfassung <-
  html |>
  html_elements("article") |>
  html_element("p") |> # html_element anstatt html_elements
  html_text() |>
  str_extract("\\.[\\s](.*)") |> # R-Zusatzpaket stringr (tidyverse)
  str_remove(". ") # R-Zusatzpaket stringr (tidyverse)

# Haupttext

all_p <-
  html |>
  html_elements("article") |>
  html_elements("p") |>
  html_text()

spaces <-
  all_p |>
  str_detect("^\\s*$") # R-Zusatzpaket stringr (tidyverse)

text <- all_p[!spaces] %>% # Pipe-Operator, R-Zusatzpaket magrittr (tidyverse)
  .[-1] |>
  list()
```

Der Datensatz `n4t_data` wird mit dem Befehl `tibble()` erstellt (der Datensatz ist auch in @tbl-daten1 dargestellt):

```{r}
n4t_data <- tibble(ID, link, datum, n_kommentare, ort, titel, zusammenfassung, text) # R-Zusatzpaket tibble (tidyverse)
n4t_data
```

```{r echo = F}
#| label: tbl-daten1
#| tbl-cap: "Datensatz f√ºr einen Beitrag"
n4t_data |>
  mutate(link = paste0(str_sub(link, start = 1, end = 8), "..."),
         titel = paste0(str_sub(titel, start = 1, end = 20), "..."),
         zusammenfassung = paste0(str_sub(zusammenfassung, start = 1, end = 20), "...")) |>
  flextable() |>
  bold(part = "header") |>
  footnote(i = 1, j = 8, value = as_paragraph(c("Abs√§tze des Haupttextes sind als Liste hinterlegt")),
           ref_symbols = c("#"), part = "body", inline = TRUE, sep = "") |>
  autofit()
```

Dieser Datensatz (@tbl-daten1) enth√§lt nun einen Beitrag von der Seite News4teachers. Wir wollen aber mehrere Beitr√§ge in einem Datensatz b√ºndeln. F√ºr den n√§chsten Beitrag mit dem Titel "*Sparpl√§ne: Inklusion h√§ngt in NRW am seidenen Faden ‚Äì Lebenshilfe appelliert*"^[<https://www.news4teachers.de/2023/09/sparplaene-inklusion-haengt-in-nrw-am-seidenen-faden-lebenshilfe-appelliert/>] wiederholen wir daher die Arbeitsschritte der Extraktion :

```{r}
# Alle Informationen von der Webseite extrahieren

html <-
  read_html("https://www.news4teachers.de/2023/09/sparplaene-inklusion-haengt-in-nrw-am-seidenen-faden-lebenshilfe-appelliert/")

# ID

ID <-
  html |>
  html_elements("article") |>
  html_attr("id")

# Link

link <-
  html |>
  html_elements("article") |>
  html_attr("id") |>
  str_remove("post-") |> # R-Zusatzpaket stringr (tidyverse)
  as.numeric() %>% # Pipe-Operator, R-Zusatzpaket magrittr (tidyverse)
  paste0("https://www.news4teachers.de/?p=", .)

# Erscheinungsdatum

datum <-
  html |>
  html_elements("article") |>
  html_elements("time") |>
  html_attr("datetime") |>
  str_sub(end = 10) # R-Zusatzpaket stringr (tidyverse)

# Anzahl der kommentare

n_kommentare <-
  html |>
  html_elements("div") |>
  html_elements(".wpd-thread-info") |>
  html_attr("data-comments-count") |>
  as.numeric()

# Ort der Berichterstattung

ort <-
  html |>
  html_elements("article") |>
  html_element("p") |> # html_element anstatt html_elements
  html_text() |>
  str_extract("[^\\.]+") # R-Zusatzpaket stringr (tidyverse)

# Titel

titel <-
  html |>
  html_elements("h1") |>
  html_text()

# Zusammenfassung

zusammenfassung <-
  html |>
  html_elements("article") |>
  html_element("p") |> # html_element anstatt html_elements
  html_text() |>
  str_extract("\\.[\\s](.*)") |> # R-Zusatzpaket stringr (tidyverse)
  str_remove(". ") # R-Zusatzpaket stringr (tidyverse)

# Haupttext

all_p <-
  html |>
  html_elements("article") |>
  html_elements("p") |>
  html_text()

spaces <-
  all_p |>
  str_detect("^\\s*$") # R-Zusatzpaket stringr (tidyverse)

text <- all_p[!spaces] %>% # Pipe-Operator, R-Zusatzpaket magrittr (tidyverse)
  .[-1] |>
  list()
```

Die Informationen aus dem neuen Beitrag werden mit dem Befehl `add_row()` dem Datensatz `n4t_data` hinzugef√ºgt:

```{r}
n4t_data <-
  n4t_data |>
  add_row(ID, link, datum, n_kommentare, ort, titel, zusammenfassung, text)
```

```{r echo = F}
#| label: tbl-daten2
#| tbl-cap: "Datensatz f√ºr zwei Beitr√§ge"
n4t_data |>
  mutate(link = paste0(str_sub(link, start = 1, end = 8), "..."),
         titel = paste0(str_sub(titel, start = 1, end = 20), "..."),
         zusammenfassung = paste0(str_sub(zusammenfassung, start = 1, end = 20), "...")) |>
  flextable() |>
  bold(part = "header") |>
  footnote(i = 1:2, j = 8, value = as_paragraph(c("Abs√§tze des Haupttextes sind als Liste hinterlegt")),
           ref_symbols = c("#"), part = "body", inline = TRUE, sep = "") |>
  autofit()
```

Der neue Datensatz (@tbl-daten2) enth√§lt nun Informationen f√ºr zwei News4teachers-Beitr√§ge. Wir k√∂nnten die Arbeitsschritte der Extraktion nun immer wieder wiederholen und somit stetig neue Beitr√§ge zum Datensatz hinzuf√ºgen. Bei einer gro√üen Anzahl von Beitr√§gen w√§re dies allerdings ein sehr langwieriger Prozess. Daher m√ºssen wir f√ºr die Extraktion, also f√ºr das Web-Scraping der Beitr√§ge, eine automatisierte Routine entwickeln.

## Automatisiertes Web-Scraping

TBA

# Weiteres

Urheber Gesetz

Politeness

Evtl. interessant f√ºr das Auslesen der Kommentare:

<https://www.news4teachers.de/2023/08/schaemt-euch-deutschland-steht-vor-den-vereinten-nationen-am-pranger-weil-es-die-inklusion-an-schulen-verweigert/feed/>

# Literatur
