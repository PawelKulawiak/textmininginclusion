---
title: "Was liest und schreibt man über Inklusion?"
subtitle: "Web-Scraping und Text-Mining mit R am Beispiel einer Online-Nachrichten- und Diskussionsseite für Lehrkräfte"
author: "Pawel R. Kulawiak"
date: last-modified
abstract: "Preprint in Progress"
abstract-title: "Status"
lang: de
bibliography: references.bib
theme: sandstone
toc: true
number-sections: true
toc-depth: 90
toc-expand: true
title-block-banner: "#2A5B60"
code-overflow: wrap
code-block-bg: "#F8F8F8"
code-block-border-left: "#769E3C"
mermaid-format: png
language: 
  title-block-author-single: "Autor"
  title-block-published: "Datum der letzten Veränderung"
format:
  html:
    embed-resources: false
    output-file: "index"
#  pdf:
#    toccolor: "blue"
#    lof: true
#    lot: true
#    link-citations: true
#    include-in-header: 
#      text: |
#        \usepackage{lscape}
#        \newcommand{\blandscape}{\begin{landscape}}
#        \newcommand{\elandscape}{\end{landscape}}
#    header-includes: |
#      \titlehead{\includegraphics[width=6.5in]{images/owl.png}}
#    geometry:
#      - top=30mm
#      - left=30mm
#      - right=30mm
#      - bottom=30mm
#      - heightrounded
#    fig-pos: H
---

```{r}
#| echo: false
#| include: false
options(width = 75)
library(tidyverse)
library(rvest)
library(flextable)
```

\newpage

![](images/owl.png){width="60%" fig-align="left"}

[![](images/pdf.png){width="25%" fig-align="left"}](https://pawelkulawiak.github.io/textmininginclusion/)

[![](images/online.png){width="25%" fig-align="left"}](https://pawelkulawiak.github.io/textmininginclusion/)

\newpage

# Vorwort und Hinweise

TBA -

In einigen Befehlsketten dienen einige R-Befehle lediglich der optischen Optimierung des R-Outputs. Diese nicht relevante R-Befehle sind mit `# don't run` gekennzeichnet und müssen daher nicht ausgeführt werden. Im angeführten Beispiel wäre lediglich die Befehlskette `html |> html_elements("h1") |> html_text()` relevant:

```{r eval = F}
html |>
  html_elements("h1") |>
  html_text() %>% 
  paste0("[", 1:length(.), "] ", '"', . ,'"') |> # don't run
  str_wrap(75) |>                                # don't run
  cat()                                          # don't run
```

# Einleitung

Mein wertgeschätzter Kollege Timo Lüke[^1] hat einst im Rahmen einer Medieninhaltsanalyse deutschsprachiger Printmedien [@lüke2014] folgende Forschungsfragen aufgeworfen:

[^1]: <https://timolueke.de/>

-   Welches Verständnis von Inklusion wird in den deutschen meinungsführenden Medien kommuniziert?
-   Welche Argumente für und gegen die Umsetzung von Inklusion werden genannt?
-   Welche Fallbeispiele werden als Belege angeführt?

> *"Im Rahmen einer systematischen Inhaltsanalyse (Rössler, 2010) deutscher Printmedien untersuchen wir die öffentliche Berichterstattung zum Thema „Inklusion". Dabei wollen wir verbreitete Definitionen, Argumente und Fallbeispiele systematisch erfassen. So sollen langfristig die Analyse des medialen Diskurses und in der Folge eine Versachlichung der kontroversen Debatte über Inklusion ermöglicht werden."* [@lüke2014]

Erste Ergebnisse der Medieninhaltsanalyse sind in Form einer Posterpräsentation verfügbar [@lüke2014] und ich erlaube mir die Darstellung des interessanten Posters (@fig-poster).

::: column-body-outset-left
![Posterpräsentation von @lüke2014: Was liest man über Inklusion?](images/poster.png){#fig-poster fig-alt="Posterpräsentation von @lüke2014: Was liest man über Inklusion?"}
:::

# Ziele

## Allgemeine Zielsetzung

Ich möchte die Medieninhaltsanalyse von @lüke2014 replizieren sowie erweitern und mich dabei auf die Textinhalte einer Online-Nachrichten- und Diskussionsseite für Lehrkräfte fokussieren, nämlich News4teachers [@N4T_2022].

## Zielsetzung mit R: Web-Scraping und Text-Mining

Ich möchte exemplarisch aufzeigen, wie die einzelnen Projektphasen der Medieninhaltsanalyse mit der Programmiersprache R umgesetzt werden können. Hierfür werden wir uns auf zwei wichtige Arbeitsschritte fokussieren:

-   **Web-Scraping**, also eine automatisierte Methode zum Extrahieren der Textinformationen von der Webseite News4teachers. Eine Einführung in das Thema Web-Scraping mit R bieten @Wickham2023-hx [Kapitel 25][^2].

-   **Text-Mining**: Die mittels Web-Scraping gesammelten Textdaten sollen mit Methoden des Text-Minings analysiert werden. Methoden des Text-Minings fokussieren sich auf die Extraktion von nützlichen Informationen aus unstrukturierten Textdaten. Unstrukturierte Textdaten sind Texte, die nicht in einer festen Datenbankstruktur vorliegen, also z.B. Textinhalte von Webseiten. Mit Methoden des Text-Minings kann auch der sentimentale Ton eines Textinhalts bzw. die im Text vermittelte subjektive Meinung analysiert werden. Das Hauptziel der sogenannten Sentimentanalyse besteht also darin, die in einem Textdokument geäußerten Emotionen und Ansichten bezüglich eines bestimmten Themas zu identifizieren, in unserem Fall also z.B. geäußerte Meinungen zum Thema Inklusion. Eine Einführung in das Thema Text-Mining mit R bieten @Silge2017-sp.

[^2]: <https://r4ds.hadley.nz/webscraping>

# News4teachers: Online-Nachrichten- und Diskussionsseite für Lehrkräfte

Bevor wir mit dem Web-Scraping und Text-Mining beginnen, betrachten wir zunächst das Arbeitsmaterial, also die Webinhalte der Webseite News4teachers, und die entsprechende Selbstbeschreibung der Webseite [@N4T_2022]:

> *"**Wer steckt hinter News4teachers?***
>
> *News4teachers wird von einer Redaktion aus Lehrern und Journalisten betrieben. Die Seite ist ein gemeinsames Projekt von [4teachers](http://www.4teachers.de/), der Service-Plattform von Lehrern für Lehrer, sowie [der Agentur für Bildungsjournalismus](http://www.xn--agentur-fr-bildungsjournalismus-wid.de/).*
>
> ***Was ist News4teachers?***
>
> *News4teachers ist eine Nachrichten- und Diskussionsseite, die sich mit seriösen Berichten, Analysen und Kommentaren an pädagogische Profis und die an Bildungsthemen interessierte Öffentlichkeit richtet. Die Redaktion sichtet täglich die Meldungen aus Politik, Forschung und Gesellschaft. Auf die Seite gelangt alles, was für die Bildung wichtig ist. News4teachers bietet also einen aktuellen Überblick über die relevanten Informationen für Lehrer, Erzieher, Schüler und Eltern. Und zwar: unabhängig und überparteilich.*
>
> ***Was ist die Idee hinter News4teachers?***
>
> *News4teachers fühlt sich dem klassischen Journalismus verpflichtet. Das heißt konkret: Wir unterwerfen uns den publizistischen Grundsätzen des Deutschen Presserats, dem [Pressekodex](https://www.presserat.de/pressekodex/pressekodex/). Informationen, die auf die Seite gelangen, wurden zuvor von der Redaktion mit der gebotenen Sorgfalt geprüft. Quellen werden stets genannt, Meinung und Bericht voneinander getrennt. News4teachers unterliegt zudem einer Chronistenpflicht: Alles, was für die Bildungsdebatte in Deutschland von Bedeutung ist, wird aktuell berichtet. Regelmäßige Nutzer von News4teachers sind also immer im Bild."* [@N4T_2022]

Die Redaktion besteht aus folgenden Personen [@N4T_impressum]: Anna Hückelheim, Sonja Mankowsky, Laura Millmann, Nina Odenius, Thomas Zab und Milla Priboschek (Podcast-Redaktion).

```{r}
#| echo: false
#| include: false
options(scipen=999)
BESUCHER <- 55000
LK <- 975000
```

## Inhalte von News4teachers und potenzielle Leserschaft aus Lehrkräften

News4teachers verspricht eine unabhängige und überparteiliche Berichterstattung zu Bildungsthemen, wahrscheinlich auch zum Thema Inklusion. Die Inhalte sind für die Leserschaft kostenfrei (werbefinanziertes Angebot). Die Inhalte von News4teachers sind außerdem speziell auf Lehrkräfte ausgerichtet. Somit kann angenommen werden, dass ein großer Teil der Leserschaft aus Lehrkräften besteht. Die Internetseite News4teachers hatte folgende Besucherzahlen (Jahr 2023): Mai (54000 Personen), Juni (60000 Personen) und Juli und August jeweils 55000 Personen (Zahlen ermittelt mit: <https://neilpatel.com/website-traffic-checker/>). Nehmen wir an, dass die Leserschaft von News4teachers zu 75% aus Lehrkräften aus Deutschland bestünde, dann hätten wir bei einer monatlichen Besucherzahl von `r BESUCHER` Personen eine monatliche Leserschaft von ca. `r BESUCHER * 0.75` Lehrkräften (`r BESUCHER` \* 0,75 = `r BESUCHER * 0.75`). In Deutschland gibt es aber laut Mikrozensus 2022 rund `r LK` Lehrkräfte an allgemeinbildenden Schulen [@census]. Die potenzielle News4teachers-Leserschaft aus Lehrkräften (`r BESUCHER * 0.75` Personen) entspräche dann einem Anteil von ca. `r (BESUCHER/LK*100) |> round(2)`% aller Lehrkräfte an allgemeinbildenden Schulen (`r BESUCHER` / `r LK` \* 100 = `r (BESUCHER/LK*100) |> round(2)`%). Im dargestellten Szenario würden die Inhalte von News4teachers also pro Monat ca. `r (BESUCHER/LK*100) |> round(2)`% der Lehrkräfte an allgemeinbildenden Schulen in Deutschland erreichen (5 von 100 Lehrkräften lesen News4teachers). Dies sind aber nur vage Vermutungen zur Reichweite von News4teachers unter Lehrkräften an allgemeinbildenden Schulen in Deutschland, unter der Annahme, dass 75% der Leserschaft von News4teachers aus Lehrkräften bestünde.

AUFGREIFEN: \[<https://www.news4teachers.de/2021/12/liebe-leserin-lieber-leser-ein-wort-zum-jahreswechsel-in-eigener-sache/>\]

### Kommentare und Diskussionen

Die Webseite News4teachers bieten der Leserschaft die Möglichkeit die Inhalte zu kommentieren und zu diskutieren (@fig-beitrag und @fig-struktur3). Hierfür formuliert die Redaktion spezifische Richtlinien [@N4T_2022]:

> *"**Gibt's Regeln für die Leserzuschriften in den Foren?***
>
> *Grundsätzlich gilt: Niemand hat einen Anspruch darauf, in den Foren zu den einzelnen Artikeln eine eigene Wortmeldung zu veröffentlichen. Die Redaktion legt Wert darauf, nur Leserzuschriften zu veröffentlichen, die erkennbar darauf abzielen, einen inhaltlichen Beitrag zur Diskussion des darüberstehenden Artikels zu leisten. Das bedeutet konkret: Auch für Leserzuschriften gelten die publizistischen Grundsätze des Deutschen Presserats, gilt also [der Pressekodex](https://www.presserat.de/pressekodex/pressekodex/).*
>
> *Kurzgefasst:*
>
> -   *Wir veröffentlichen keine Leserbeiträge, in denen ungeprüfte, unbelegte oder falsche Tatsachenbehauptungen verbreitet werden.*
> -   *Wir veröffentlichen keine Hetze gegen Menschen oder Menschengruppen.*
> -   *Wir veröffentlichen keine Werbung, ob nun für Produkte oder Parteien.*
> -   *Und wir veröffentlichen keine Links auf unseriöse Quellen.*
>
> *Wir behalten uns darüber hinaus vor, Leserbriefe, die lediglich der Stimmungsmache dienen, zu löschen. Oder Leserbriefe sinnwahrend zu kürzen."* [@N4T_2022]

\[Hier weitere Erläuterungen einfügen\]

##### Facebook und Twitter

Die Beiträge werden aber nicht nur unmittelbar auf der Seite von News4teachers kommentiert und diskutiert (@fig-struktur3). Die Diskussion der Beiträge erfolgt auch auf einer externen Seite, nämlich bei Facebook. Innerhalb der Beiträge wird auch auf die externe Diskussion bei Facebook verwiesen (@fig-facebook). Außerdem werden die Beiträge von News4teachers ebenfalls bei Twitter geteilt und diskutiert (<https://twitter.com/News4teachers>). Die Kommentare und Diskussionen bei Facebook und Twitter sollen daher auch bei der vorliegenden Medieninhaltsanalyse Berücksichtigung finden.

![Beitrag zum Thema Inklusion mit 152 Leserkommentaren auf der Internetseite News4teachers [@N4T_2023]](images/beitrag.png){#fig-beitrag fig-alt="Inhalt zum Thema Inklusion mit 152 Leserkommentaren auf der Internetseite News4teachers"}

![Verweis bei News4teachers auf die Diskussion bei Facebook (Diskussion zum Beitrag *"„Schämt Euch!" -- Deutschland steht vor den Vereinten Nationen am Pranger, weil es die Inklusion an Schulen praktisch verweigert"*)](images/facebook.png){#fig-facebook fig-alt="Verweis bei News4teachers auf die Diskussion bei Facebook"}

# Explorative Forschungsfragen

Die Inhalte von der Webseite News4teachers und die Kommentare und Diskussionen der Leserschaft eignen sich eventuell zur Beantwortung folgender Forschungsfragen:

-   Auf welche Art und Weise wird das Thema Inklusion auf der Online-Nachrichten- und Diskussionsseite für Lehrkräfte dargestellt?
-   Auf welche Art und Weise werden die Inhalte zum Thema Inklusion von der Leserschaft kommentiert und diskutiert?

# Web-Scraping

Der erste Arbeitsschritt, hin zum Text-Mining, also hin zur Medieninhaltsanalyse, wird nun das Web-Scraping sein, also die automatisierte Extraktion der Webinhalte (z.B. Textinformationen) von der Webseite News4teachers. Traditionellerweise bzw. altmodischerweise würde man Webinhalte mit der Methode "*copy-and-paste*" in einen Datensatz übertragen, also z.B. Text von einer Webseite kopieren und anschließend die kopierte Textinformation in einen Datensatz einfügen (z.B. bei Excel). Dieses Verfahren ist aber fehleranfällig, da z.B. die Gefahr besteht, dass aufgrund mangelnder Konzentration falsche oder unvollständige Textinhalte übertragen werden. Web-Scraping ist daher als automatisierte Methode der Extraktion von Webinhalten weniger anfällig für Fehler und somit die Methode der Wahl. Eine Einführung in das Thema Web-Scraping mit R bieten @Wickham2023-hx [Kapitel 25][^3].

[^3]: <https://r4ds.hadley.nz/webscraping>

## R-Zusatzpakete

### R-Zusatzpakete *rvest* und *tidyverse*

Für das Web-Scraping nutzen wir nun das R-Zusatzpaket *rvest* [@rvest]. Der Name des R-Zusatzpaketes ist eine gelungene Anspielung auf das englische Wort *harvest* (ernten, sammeln), denn wir wollen ja Informationen aus dem Internet sammeln (mit R). Das kreative Wortspiel ist auch im Logo des R-Zusatzpaketes visualisiert (@fig-logo). Das R-Zusatzpaket *rvest* ist in der R-Paketsammlung *tidyverse* [@tidyverse] enthalten. *tidyverse* ist eine Zusammenstellung unterschiedlicher R-Zusatzpakete. Wir werden an diversen Stellen die herausragende Funktionalität der Paketsammlung *tidyverse* nutzen. An den entsprechenden Stellen wird ein Verweis auf die *tidyverse*-Zusatzpakete erfolgen. Mit der Installation von *tidyverse* wird auch das Web-Scraping-Zusatzpaket *rvest* installiert (`install.packages("tidyverse")`). Die Paketsammlung *tidyverse* und das darin enthaltene Web-Scraping-Zusatzpaket *rvest* müssen anschließend mit dem Befehl `library(tidyverse, rvest)` geladen werden:

```{r}
#| eval: false
install.packages("tidyverse")
library(tidyverse, rvest)
```

Das R-Zusatzpaket *rvest* verfügt über eine umfassende und hilfreiche Online-Dokumentation:

-   <https://rvest.tidyverse.org/>
-   <https://r4ds.hadley.nz/webscraping> [@Wickham2023-hx, Kapitel 25]

Informationen zum Paketsammlung *tidyverse* findet man hier:

<https://www.tidyverse.org/>

![Logo des R-Zusatzpaketes *rvest*](images/logo.png){#fig-logo fig-alt="Logo des R-Zusatzpaketes rvest"}

## Struktur und Inhalte der Webseite

Ziel des Web-Scapings wird es sein, die relevanten Webinhalte von News4teachers automatisiert zu extrahieren. Hierfür müssen wir uns erstmal einen Überblick über die Struktur und Inhalte der Webseite verschaffen. Die Beiträge auf den Internetseiten von News4teachers haben eine spezifische Struktur mit spezifischen Webinhalten. Wir betrachten den Beitrag mit dem Titel *"„Schämt Euch!" -- Deutschland steht vor den Vereinten Nationen am Pranger, weil es die Inklusion an Schulen praktisch verweigert"* [@N4T_2023]. Für unsere Forschungsfragen mehr oder weniger interessante Webinhalte sind in den Abbildungen kenntlich gemacht (@fig-struktur1, @fig-struktur2 und @fig-struktur3).

![(a) Beitrag auf der Internetseite News4teachers und Struktur der Webinhalte [@N4T_2023]](images/struktur1.png){#fig-struktur1 fig-alt="Beitrag auf der Internetseite News4teachers und Struktur der Webinhalte"}

![(b) Beitrag auf der Internetseite News4teachers und Struktur der Webinhalte [@N4T_2023]](images/struktur2.png){#fig-struktur2 fig-alt="Beitrag auf der Internetseite News4teachers und Struktur der Webinhalte"}

![(c) Beitrag auf der Internetseite News4teachers und Struktur der Webinhalte [@N4T_2023]](images/struktur3.png){#fig-struktur3 fig-alt="Beitrag auf der Internetseite News4teachers und Struktur der Webinhalte"}

\[Erläuterungen zu den Abbildungen und Inhalten hinzufügen\]

## Erster Web-Scraping-Versuch

Zuvor haben wir uns einen Überblick über die zu extrahierenden Webinhalte verschafft. Für den ersten Web-Scraping-Versuch nutzen wir weiterhin den Beitrag mit dem Titel *"„Schämt Euch!" -- Deutschland steht vor den Vereinten Nationen am Pranger, weil es die Inklusion an Schulen praktisch verweigert"* (@fig-struktur1). Dies ist der Link zum Beitrag:

<https://www.news4teachers.de/2023/08/schaemt-euch-deutschland-steht-vor-den-vereinten-nationen-am-pranger-weil-es-die-inklusion-an-schulen-verweigert/>

Wir nutzen den Befehl `read_html()` und den entsprechenden Link, um sämtliche Informationen von der Webseite zu extrahieren:

```{r, include = F}
html <-
  read_html("https://www.news4teachers.de/2023/08/schaemt-euch-deutschland-steht-vor-den-vereinten-nationen-am-pranger-weil-es-die-inklusion-an-schulen-verweigert/")
```

```{r eval = F}
html <-
  read_html("https://www.news4teachers.de/2023/08/
            schaemt-euch-deutschland-steht-vor-den-vereinten-nationen-
            am-pranger-weil-es-die-inklusion-an-schulen-verweigert/")

# Achtung: Zeilenumbruch ist innerhalb des Links nicht erlaubt!
#          An dieser Stelle wurden Zeilenumbrüche nur aufgrund des
#          Platzmangels eingefügt (zwecks Verbesserung der Darstellung).
#          Bitte entfernen Sie alle Zeilenumbrüche im Link, sodass der
#          Link bei Ihnen in einer Zeile erscheint.
```

Alle Webinhalte sind nun im Objekt `html` hinterlegt. Wir sind allerdings nur an spezifischen Webinhalten interessiert und möchten daher im nächsten Schritt einen spezifischen Textinhalt aus dem Objekt `html` auslesen. Wir wollen den Titel des Beitrages extrahieren: *"„Schämt Euch!" -- Deutschland steht vor den Vereinten Nationen am Pranger, weil es die Inklusion an Schulen praktisch verweigert"*. Dabei ist es gar nicht so leicht, einen spezifischen Inhalt wie den Titel zu lokalisieren und auszulesen. Hierfür ist HTML-[^4] und CSS-Selector-Grundlagenwissen[^5] hilfreich. Die eigentlichen Textinhalte sind nämlich im HTML-Dokument der Webseite hinterlegt (HTML-Quelltext). Ist eine Internetseite im Browser geöffnet, so gelangen wir mit einem Rechtsklick i.d.R. zur Option *"Seitenquelltext anzeigen"* (@fig-quelltext). Dies führt uns zum HTML-Dokument bzw. zum HTML-Quelltext der Webseite (@fig-html1).

[^4]: <https://developer.mozilla.org/en-US/docs/Web/HTML>

[^5]: <https://developer.mozilla.org/en-US/docs/Learn/CSS/Building_blocks/Selectors>; *"CSS includes a miniature language for selecting elements on a page called CSS selectors. CSS selectors define patterns for locating HTML elements, and are useful for scraping because they provide a concise way of describing which elements you want to extract."*, Quelle: <https://rvest.tidyverse.org/articles/rvest.html>

![Seitenquelltext (HTML) anzeigen](images/quelltext.png){#fig-quelltext fig-alt="Seitenquelltext (HTML) anzeigen"}

::: column-body-outset-left
![HTML-Quelltext (Ausschnitt)](images/html1.png){#fig-html1 fig-alt="HTML-Seitenquelltext (Ausschnitt)"}
:::

Das HTML-Dokument (@fig-html1) ist riesig (mehr als 10000 Zeilen) und wir müssen etwas stöbern, um den passenden Webinhalt zu lokalisieren. Der HTML-Code aus dem HTML-Dokument (@fig-html1) ist zwecks besserer Lesbarkeit auch nachfolgend dargestellt (auszugsweise):

```{html}
<article id="post-132285" class="post-132285 post type-post status-publish
  format-standard has-post-thumbnail category-leben category-titelthema
  category-wissenschaft tag-forderschulen tag-inklusion
  tag-un-behindertenrechtskonvention" itemscope
  itemtype="https://schema.org/Article">
  <div class="td-post-header">
    <!-- category -->
    <ul class="td-category">
      <li class="entry-category">
        <a href="https://www.news4teachers.de/bildung/leben/">Leben</a>
      </li>
      <li class="entry-category">
        <a href="https://www.news4teachers.de/bildung/titelthema/">
        Titelthema</a>
      </li>
      <li class="entry-category">
        <a href="https://www.news4teachers.de/bildung/wissenschaft/">
        Wissen</a>
      </li>
    </ul>
    <header class="td-post-title">
      <h1 class="entry-title">&#8222;Schämt Euch!&#8220; &#8211;
        Deutschland steht vor den Vereinten Nationen am Pranger, weil es
        die Inklusion an Schulen praktisch verweigert
      </h1>
      <div class="td-module-meta-info">
        <!-- author -->
        <!-- date -->
        <span class="td-post-date">
        <time class="entry-date updated td-module-date"
          datetime="2023-08-29T12:46:06+02:00">29. August 2023</time>
        </span>
        <!-- comments -->
        <div class="td-post-comments">
          <a href="https://www.news4teachers.de/2023/08/schaemt-euch-
deutschland-steht-vor-den-vereinten-nationen-am-pranger-weil
-es-die-inklusion-an-schulen-verweigert/#comments">
          <i class="td-icon-comments"></i>150
          </a>
        </div>
        <!-- views -->
      </div>
    </header>
  </div>
</article>
```

Wir sehen z.B. in der Zeile 1297 (@fig-html1), dass der Titel des Beitrages ein `h1`-HTML-Element[^6] ist (header 1: Überschrift erster Ebene):

[^6]: <https://developer.mozilla.org/en-US/docs/Web/HTML/Element/Heading_Elements>

```{html}
<h1 class="entry-title">&#8222;Schämt Euch!&#8220; &#8211; Deutschland
  steht vor den Vereinten Nationen am Pranger, weil es die Inklusion an 
  Schulen praktisch verweigert</h1>
```

Diese Information benötigen wir, um den Titel des Beitrags gezielt auszulesen. Hierfür nutzen wir den Befehl `html_elements("h1")` und übergeben das Objekt `html` an diesen Befehl.

```{r}
html |>
  html_elements("h1")
```

Die Information `<h1 class="entry-title">` ist überflüßig, da wir nur am HTML-Textinhalt interessiert sind. Daher extrahieren wir den reinen Textinhalt, also den Titel, mit dem Befehl `html_text()`. Die Befehlskette wird entsprechend erweitert:

```{r}
html |>
  html_elements("h1") |>
  html_text() %>% 
  paste0("[", 1:length(.), "] ", '"', . ,'"') |> # don't run
  str_wrap(75) |>                                # don't run
  cat()                                          # don't run
```

Herzlichen Glückwunsch! 🥳 Somit haben wir erfolgreich alle Informationen von der Webseite extrahiert und eine relevante Textstelle (den Titel) ausgelesen.

### Datenstruktur

Im HTML-Code (@fig-html1) sehen wir, dass anscheinend jeder Beitrag über eine ID verfügt (`id="post-132285"`). Wenn wir in unserem zukünftigen Datensatz mehrere Beiträge abspeichern wollen, dann wird eine ID-Variable zwecks Unterscheidung der Beiträge eine hilfreiche Sache sein. @tbl-idee ist eine erste Idee bezüglich einer möglichen/sinnvollen Datenstruktur. Bei dieser Datenstruktur ignorieren wir der Einfachheit halber vorerst ein paar relevante Webinhalte, z.B. Kommentare und Anzahl der Likes ("*Gefällt mir*").

\newpage

\blandscape

```{r}
#| echo: false
#| label: tbl-idee
#| tbl-cap: Erste Idee bezüglich einer möglichen/sinnvollen Datenstruktur
ID <- c(132285, "...", "...")
link <- c("https://...", "...", "...")
datum <- c("29.\nAugust\n2023", "...", "...")
n_kommentare <- c("150", "...", "...")
ort <- c("GENF", "...", "...")
titel <- c("'Schämt Euch!' --\nDeutschland steht\nvor den Vereinten\nNationen am Pranger...", "...", "...")
zusammenfassung <- c("„Schämt Euch!“ --\nso heißt es auf\neinem Transparent...", "...", "...")
haupttext <- c("Der offizielle Beitrag\nDeutschlands fällt\ndünn aus...", "...", "...")
usw. <- c("...", "...", "...")

data.frame(ID, link, datum, n_kommentare, ort, titel, zusammenfassung, haupttext, usw.) |>
  flextable() |>
  #autofit() |>
  bold(part = "header") |>
  rotate(rotation = "tbrl", align = "bottom", part = "header") |>
  width(j = 1:5, width = 0.6) |>
  width(j = 2, width = 0.8) |>
  width(j = 6:8, width = 1.5) |>
  width(j = 9, width = 0.3) |>
  height(height = 5, part = "header")
```

\elandscape

\[Gefällt mir???\]

### Weitere Web-Scraping-Schritte {#sec-schritte}

Um die Datenstruktur aus @tbl-idee zu realisieren, müssen wir nun die ID des Beitrags, den Link, das Erscheinungsdatum, die Anzahl der Kommentare, die Zusammenfassung und den eigentlichen Haupttext des Beitrages auslesen (den Titel haben wir ja bereits erfolgreich extrahiert). Beginnen wir mit der ID.

#### ID {#sec-link}

In @fig-html1 sehen wir, das die ID des Beitrages (`id="post-132285"`) ein Attribut[^7] eines HTML-Elements ist (HTML-Element: `article`[^8]):

[^7]: <https://developer.mozilla.org/en-US/docs/Web/HTML/Global_attributes/id>

[^8]: <https://developer.mozilla.org/en-US/docs/Web/HTML/Element/article>

```{html}
<article id="post-132285" class="post-132285 post type-post status-publish
format-standard has-post-thumbnail category-leben category-titelthema
category-wissenschaft tag-forderschulen tag-inklusion
tag-un-behindertenrechtskonvention" itemscope
itemtype="https://schema.org/Article">
```

Daher übergeben wir das Objekt `html` zwecks Auslesung der ID zunächst an den Befehl `html_elements("article")` und dann an den Befehl `html_attr("id")`:

```{r}
html |>
  html_elements("article") |>
  html_attr("id")
```

Die ID des Beitrags erscheint mit dem Präfix `"post-"`, eine nicht notwendigerweise nützliche Information. Das Präfix entfernen wir daher mit dem Befehl `str_remove("post-")` und überführen die ID mit dem Befehl `as.numeric()` in ein nummerisches Format. Somit erhalten wir die nummerische ID `132285`:

```{r}
html |>
  html_elements("article") |>
  html_attr("id") |>
  str_remove("post-") |> # R-Zusatzpaket stringr (tidyverse)
  as.numeric()
```

#### Link

Der Beitrag verfügt über einen langen Link:

<https://www.news4teachers.de/2023/08/schaemt-euch-deutschland-steht-vor-den-vereinten-nationen-am-pranger-weil-es-die-inklusion-an-schulen-verweigert/>

Im HTML-Code ist allerdings auch ein kurzer Link, also ein `shortlink`, ausgewiesen:

```{html}
<link rel='shortlink' href='https://www.news4teachers.de/?p=132285' />
```

Die ID des Beitrags (`132285`) ist Bestandteil des kurzen Links. Wir können also den ersten Teil des kurzen Links (`"https://www.news4teachers.de/?p="`) mit der ID (`132285`) verbinden, um den gewünschten Kurzlink zu generieren. Hierfür nutzen wir nach der Auslesung der ID den Befehl `paste0("https://www.news4teachers.de/?p=", .)`. Mit dem magrittr-Pipe-Operator (`%>%`[^9]) wird die ID an das zweite Argument des Befehls `paste0("https://www.news4teachers.de/?p=", .)` übergeben, also an die Stelle mit dem Punkt (`.`). Eine Übergabe an das zweite Argument wäre mit der sogenannten base-Pipe (`|>`) nicht möglich, daher nutzen wir die magrittr-Pipe (`%>%`) anstatt der base-Pipe (`|>`). Die Befehlskette zur Erstellung des Links gestaltet sich somit folgendermaßen:

[^9]: <https://magrittr.tidyverse.org/>

```{r}
html |>
  html_elements("article") |>
  html_attr("id") |>
  str_remove("post-") |> # R-Zusatzpaket stringr (tidyverse)
  as.numeric() %>% # Pipe-Operator, R-Zusatzpaket magrittr (tidyverse)
  paste0("https://www.news4teachers.de/?p=", .) 
```

#### Erscheinungsdatum

Fahren wir fort mit dem Auslesen des Erscheinungsdatums des Beitrages. Im HTML-Code (@fig-html1, Zeile 1301) erscheint folgende Information:

```{html}
<span class="td-post-date"><time class="entry-date updated td-module-date"
datetime="2023-08-29T12:46:06+02:00" >29. August 2023</time></span> 
```

Wir sehen, dass das Datum ein HTML-Element ist, nämlich ein `time`-Element[^10]. Dieses `time`-Element ist innerhalb eines `span`-Elements[^11] geschachtelt. Wir können hier also von einer hierarchischen Schachtelung der HTML-Elemente sprechen (`span -> time`, @fig-schachtelung_1).

[^10]: <https://developer.mozilla.org/en-US/docs/Web/HTML/Element/time>

[^11]: <https://developer.mozilla.org/en-US/docs/Web/HTML/Element/span>

```{mermaid}
%%| label: fig-schachtelung_1
%%| fig-cap: Hierarchische Schachtelung der HTML-Elemente `span` und `time`
%%{init: {'theme':'forest'}}%%
flowchart TB
  id1(span) --> id2(time)
```

Entsprechend erfolgt die Extraktion des Datums mit der Übergabe des Objektes `html`, zunächst an den Befehl `html_elements("span")`, und anschließend an den Befehl `html_elements("time")`:

```{r}
html |>
  html_elements("span") |>
  html_elements("time")
```

Das Ergebnis ist aber nicht ganz befriedigend, da mehrere Datumsangaben extrahiert worden sind, unter anderem das gewünschte Erscheinugsdatum des Beitrages (`2023-08-29`), aber auch andere, nicht relevate Datumsangaben (z.B. `2023-09-17`), welche ebenfalls auf der Webseite erscheinen (@fig-datum).

![Verweis auf einen anderen Beitrag mit nicht relevanter Datumsangabe](images/datum.png){#fig-datum fig-alt="Verweis auf einen anderen Beitrag mit nicht relevanter Datumsangabe"}

Wir müssen daher beim Auslesen noch genauer die hierarchische Position des Erscheinungsdatums definieren. Ein Blick auf @fig-html1 offenbart, dass die beiden HTML-Elemente `span` und `time` innerhalb des bereits bekannten HTML-Elements `article` geschachtelt sind (`article -> span -> time`, @fig-schachtelung_2).

```{mermaid}
%%| label: fig-schachtelung_2
%%| fig-cap: Hierarchische Schachtelung der HTML-Elemente `article`, `span` und `time`
%%{init: {'theme':'forest'}}%%
flowchart TB
  id1(article) --> id2(span) --> id3(time)
```

Diese hierarchische Schachtelung (`article -> span -> time`) muss daher beim Auslesen des Erscheingsdatums beachtet werden:

```{r}
html |>
  html_elements("article") |>
  html_elements("span") |>
  html_elements("time")
```

Das Erscheinungsdatum ist in diesem Falle das einzige `time`-Element innerhalb des `article`-Elements. Daher führt auch das Weglassen des `span`-Elements und somit die Anwendung einer reduzierten hierarchischen Schachtelung der HTML-Elemente (`article -> time`, @fig-schachtelung_3) zum gewünschten Erfolg:

```{mermaid}
%%| label: fig-schachtelung_3
%%| fig-cap: Reduzierte hierarchische Schachtelung der HTML-Elemente `article` und `time`
%%{init: {'theme':'forest'}}%%
flowchart TB
  id1(article) --> id2(time)
```

```{r}
html |>
  html_elements("article") |>
  html_elements("time")
```

Auch bei der Datumsangabe wollen wir uns auf die wesentliche Information fokussieren und extrahieren daher die reine Datumsangabe, die dem Attribut `"datetime"` zugeordnet ist. Die Befehlskette wird daher um den Befehl `"html_attr("datetime")"` ergänzt:

```{r}
html |>
  html_elements("article") |>
  html_elements("time") |>
  html_attr("datetime")
```

Die Datumsangabe (`"2023-08-29T12:46:06+02:00"`) beinhaltet eine für uns nicht relevante Zeitangabe, also die genaue Uhrzeit der Beitragserscheinung (`T12:46:06+02:00`). Die ersten 10 Zeichen (inkl. Bindestriche: `JJJJ-MM-TT`/`2023-08-29`) beinhalten die relevante Datumsangabe. Die nicht relevante Zeitangabe entfernen wir, indem wir lediglich die ersten 10 Zeichen der Datumsangabe beibehalten. Hierfür ergänzen wir die Befehlskette um den Befehl `str_sub(end = 10)`:

```{r}
html |>
  html_elements("article") |>
  html_elements("time") |>
  html_attr("datetime") |>
  str_sub(end = 10) # R-Zusatzpaket stringr (tidyverse)
```

#### Anzahl der Kommentare

Die Anzahl der Kommentare ist im HTML-Element `div`[^12] hinterlegt. Und dieses HTML-Element `div` ist durch eine CSS-Klasse[^13] gekennzeichnet (`class="wpd-thread-info"`), wobei die eigentliche Anzahl der Kommentare ein HTML-Attribut ist (`data-comments-count="150"`). Im HTML-Seitenquelltext sieht dies folgendermaßen aus:

[^12]: <https://developer.mozilla.org/en-US/docs/Web/HTML/Element/div>

[^13]: <https://developer.mozilla.org/en-US/docs/Learn/CSS/Building_blocks/Selectors>; *"CSS includes a miniature language for selecting elements on a page called CSS selectors. CSS selectors define patterns for locating HTML elements, and are useful for scraping because they provide a concise way of describing which elements you want to extract."*, Quelle: <https://rvest.tidyverse.org/articles/rvest.html>

```{html}
<div class="wpd-thread-info" data-comments-count="150">
  <span class='wpdtc' title='150'>150</span> Kommentare </div>
```

Das Objekt `html` wird daher an den Befehl `html_elements("div")` übergeben. Die anschließende Angabe der CSS-Klasse `"wpd-thread-info"` erfolgt mit einem vorangestellten Punkt[^14] (`".wpd-thread-info"`) innerhalb des Befehls `html_elements(".wpd-thread-info")`:

[^14]: Die Angabe einer CSS-Klasse muss immer mit einem vorangestellten Punkt erfolgen

```{r}
html |>
  html_elements("div") |>
  html_elements(".wpd-thread-info")
```

Die eigentliche Extraktion der Anzahl der Kommentare erfolgt nun mit der Angabe des entsprechenden HTML-Attributes (`"data-comments-count"`) innerhalb des Befehls `html_attr("data-comments-count")`. Anschließend wird die Anzahl der Kommentare mit dem Befehl `as.numeric()` in ein nummerisches Format überführt. Die Befehlskette gestaltet sich daher folgendermaßen:

```{r}
html |>
  html_elements("div") |>
  html_elements(".wpd-thread-info") |>
  html_attr("data-comments-count") |>
  as.numeric()
```

#### Ort der Berichterstattung

Die Ortsangabe ist Bestandteil der Zusammenfassung (siehe @fig-struktur1) und die Zusammenfassung ist ein Absatz, i.d.R. der erste Absatz des Beitrages. Für die Extraktion der Ortsangabe ist daher das HTML-Element für Absätze notwendig (`p`[^15]; `p` steht für "paragraph"). Dieses HTML-Element (`p`) ist wie gewohnt innerhalb des HTML-Elements `article` geschachtelt. Zur Extraktion des ersten Absatzes wird diesmal der Befehl `html_element("p")` anstatt `html_elements("p")` genutzt. Der Befehl `html_elements("p")` würde alle Absätze des Beitrages extrahieren. Wir benötigen aber nur den ersten Absatz mit der Ortsangabe und daher nutzen wir diesmal den Befehl `html_element("p")` anstatt `html_elements("p")`. Die Befehlskette gestaltet sich daher wie folgt:

[^15]: <https://developer.mozilla.org/en-US/docs/Web/HTML/Element/p>

```{r}
html |>
  html_elements("article") |>
  html_element("p") |> # html_element() anstatt html_elements()
  html_text() %>% 
  paste0("[", 1:length(.), "] ", '"', . ,'"') |> # don't run
  str_wrap(75) |>                                # don't run
  cat()                                          # don't run
```

Somit sehen wir den Absatz mit der Ortsangabe. Wir benötigen allerdings nur die Ortsangabe, also das erste Wort des Absatzes. Hinter der gewünschten Ortsangabe steht ein Punkt (`GENF.`). Mit dem Befehl `str_extract("[^.]+")`[^16] extrahieren wir alle Zeichen vor dem ersten Punkt, also die Ortsangabe `GENF`. Die Befehlskette gestaltet sich daher wie folgt:

[^16]: Bei so einer kryptischen Formel (`"[^.]+"`) handelt es sich um eine sogenannte "*regular expression*" (*regex*). Eine Einführung in diese Thematik findet man hier: <https://r4ds.hadley.nz/regexps> [@Wickham2023-hx, Kapitel 16].

```{r}
html |>
  html_elements("article") |>
  html_element("p") |> # html_element anstatt html_elements
  html_text() |>
  str_extract("[^\\.]+") # R-Zusatzpaket stringr (tidyverse)
```

#### Zusammenfassung des Beitrages

Wie soeben bei der Extraktion der Ortsangabe erwähnt, ist die Zusammenfassung des Beitrages der erste Absatz des Textes (siehe @fig-struktur1). Der erste Absatz wurde soeben folgendermaßen extrahiert:

```{r}
html |>
  html_elements("article") |>
  html_element("p") |> # html_element() anstatt html_elements()
  html_text() %>% 
  paste0("[", 1:length(.), "] ", '"', . ,'"') |> # don't run
  str_wrap(75) |>                                # don't run
  cat()                                          # don't run
```

Somit erhalten wir die Zusammenfassung mit der Ortsangabe inkl. Punkt (`GENF.`). Nun wollen wir die überflüssige Ortsangabe entfernen und nur die eigentliche Zusammenfassung beibehalten. Dies erreichen wir mit dem Befehl `str_extract("\\.[\\s](.*)")`. Die regex-Formel `"\\.[\\s](.*)"` hat folgende Bedeutung:

-   `\\.` Suche und extrahiere Zeichen nach dem ersten Punkt (einschließlich des ersten Punktes)
-   `[\\s]` Die extrahierten Zeichen können Leerzeichen sein ("*s*" steht für "*space*")
-   `(.*)` Extrahiere außerdem alle weiteren Zeichen

Die Befehlskette gestaltet sich daher wie folgt:

```{r}
html |>
  html_elements("article") |>
  html_element("p") |> # html_element anstatt html_elements
  html_text() |>
  str_extract("\\.[\\s](.*)") %>% # R-Zusatzpaket stringr (tidyverse) 
  paste0("[", 1:length(.), "] ", '"', . ,'"') |> # don't run
  str_wrap(75) |>                                # don't run
  cat()                                          # don't run
```

Die Ortsangabe (`GENF`) wurde erfolgreich entfernt. Der Punkt hinter der Ortsangabe (`GENF.`) wurde allerdings nicht entfernt und bleibt bestehen. Die Zusammenfassung beginnt daher nun mit einem Punkt (`.`) gefolgt von einem Leerzeichen. Wir entfernen den Punkt und das Leerzeichen (`". "`) mit dem Befehl `str_remove(". ")`. Die Befehlskette zur Extraktion der Zusammenfassung gestaltet sich daher folgendermaßen:

```{r}
html |>
  html_elements("article") |>
  html_element("p") |> # html_element anstatt html_elements
  html_text() |>
  str_extract("\\.[\\s](.*)") |> # R-Zusatzpaket stringr (tidyverse)
  str_remove(". ") %>% # R-Zusatzpaket stringr (tidyverse) 
  paste0("[", 1:length(.), "] ", '"', . ,'"') |> # don't run
  str_wrap(75) |>                                # don't run
  cat()                                          # don't run
```

#### Haupttext

Kommen wir nun zum Filetstück, also zum eigentlichen Haupttext des Beitrages. Der Beitragstext besteht aus Absätzen. Also können wir, wie bereits gewohnt, das HTML-Element `p` berücksichtigen. Und dieses HTML-Element `p` ist bekannterweise innerhalb des HTML-Elements `article` geschachtelt. Bisher haben wir die eigentliche hierarchische Schachtelung des Haupttextes allerdings nie expliziert. Die Absätze des Haupttextes (`p`) sind nämlich Bestandteil des HTML-Elements `div` und dieses HTML-Element ist wiederum Bestandteil des HTML-Elements `article` (`article -> div -> p`; @fig-schachtelung_4).

```{mermaid}
%%| label: fig-schachtelung_4
%%| fig-align: left
%%| fig-cap: Hierarchische Schachtelung der HTML-Elemente `article`, `div` und `p`
%%{init: {'theme':'forest'}}%%
flowchart TB
  id1(article) --> id2(div) --> id3(p)
```

Zur Abbildung dieser hierarchischen Schachtelung nutzen wir diesmal aus Gründen eine sogenannte *xpath*-Angabe[^17] (`"//article/div/p"`). Die *xpath*-Angabe (`"//article/div/p"`) erfolgt innerhalb des Befehls `html_elements(xpath = "//article/div/p")`:

[^17]: <https://developer.mozilla.org/en-US/docs/Web/XPath>

```{r}
html |>
  html_elements(xpath = "//article/div/p") |>
  html_text() |>
  str_wrap(75) %>%                                     # don't run
  paste0("\n", "[", 1:length(.), "] ", '"', . ,'"') |> # don't run
  cat()                                                # don't run
```

Mit der obigen Befehlskette werden alle relevanten Absätze extrahiert, aber auch 3 Absätze ohne jeglichen Inhalt (`[15]`, `[16]` und `[17]`). Diese Absätze enthalten keine Zeichen oder ausschließlich Leerzeichen. Wir wollen daher Absätze, die keine Zeichen oder ausschließlich Leerzeichen enthalten, entfernen. Hierfür speichern wir vorübergehend alle Absätze im Textformat als Objekt `all_p` ab. Wir Überprüfen anschließend, ob der Textinhalt der Absätze keine Zeichen oder ausschließlich Leerzeichen enthält (`all_p |> str_detect("^\\s*$")`). Die regex-Formel `"^\\s*$"` hat folgende Bedeutung:

-   `^` Beginne die Suche am Anfang des Absatzes
-   `\\s*` Suche Leerzeichen bzw. keine Zeichen ("*s*" steht für "*space*")
-   `$` Führe diese Suche bis zum Ende des Absatzes durch

Das Ergebnis der Überprüfung ist für jeden Absatz eine `TRUE`-`FALSE`-Aussage (`TRUE`: Keine Zeichen oder ausschließlich Leerzeichen; `FALSE`: Andere Zeichen). Diese `TRUE`-`FALSE`-Aussage speichern wir als Objekt `spaces` ab und lassen uns abschließend Absätze anzeigen, welche nicht ausschließlich aus Leerzeichen oder keinen Zeichen bestehen (`all_p[!spaces]`):

```{r}
all_p <-
  html |>
  html_elements(xpath = "//article/div/p") |>
  html_text()

all_p |>
  str_detect("^\\s*$") # R-Zusatzpaket stringr (tidyverse)

spaces <-
  all_p |>
  str_detect("^\\s*$") # R-Zusatzpaket stringr (tidyverse)

all_p[!spaces] |>
  str_wrap(75) %>%                                     # don't run
  paste0("\n", "[", 1:length(.), "] ", '"', . ,'"') |> # don't run
  cat()                                                # don't run
```

Fast geschafft! Wir müssen nur noch den ersten Absatz entfernen (`all_p[!spaces] %>% .[-1]`), da der erste Absatz die Zusammenfassung mit Ortsangabe darstellt (`[1] "GENF. „Schämt Euch!“ – so heißt es auf einem Transparent..."`) und daher nicht zum Haupttext gezählt werden kann. Der Punkt vor der eckigen Klammer (`.[-1]`) symbolisiert die Absätze (`all_p[!spaces]`). Bei der Übergabe dieser Absätze nutzen wir, wie zuvor beim Auslesen des Links (@sec-link), die magrittr-Pipe (`%>%`), da nur diese Pipe, und nicht die base-Pipe (`|>`), eine Übergabe an eine Bedingung in eckigen Klammern ermöglicht (`.[-1]`). Alle Absätze werden anschließend mit dem Befehl `list()` in eine Liste überführt. Dies ist daher die schlussendliche Befehlskette, welche zur Darstellung des Haupttextes führt:

```{r eval = F}
all_p[!spaces] %>% # Pipe-Operator, R-Zusatzpaket magrittr (tidyverse)
  .[-1] |>
  list()

# R-Output wird aus Platzgründen nicht erneut dargestellt
```

### Datensatz

Die bisher erfolgreich ausgelesenen Informationen (@sec-schritte: ID, Link, Erscheinungsdatum, Anzahl der Kommentare, Ort der Berichterstattung, Titel, Zusammenfassung des Beitrages und Haupttext) können wir nun in einem Datensatz zusammenfassen (vgl. @tbl-idee). Hierfür wiederholen wir pro forma und zwecks Übersichtlichkeit alle bisherigen Arbeitsschritte und speichern jede einzelne Information als Objekt (`ID`, `link`, `datum`, `n_kommentare`, `ort`, `titel`, `zusammenfassung` und `text`):

```{r eval = F}
# Alle Informationen von der Webseite extrahieren

html <-
  read_html("https://www.news4teachers.de/2023/08/
            schaemt-euch-deutschland-steht-vor-den-vereinten-nationen-
            am-pranger-weil-es-die-inklusion-an-schulen-verweigert/")

# Achtung: Zeilenumbruch ist innerhalb des Links nicht erlaubt!
#          An dieser Stelle wurden Zeilenumbrüche nur aufgrund des
#          Platzmangels eingefügt (zwecks Verbesserung der Darstellung).
#          Bitte entfernen Sie alle Zeilenumbrüche im Link, sodass der
#          Link bei Ihnen in einer Zeile erscheint.
```

```{r}
# ID

ID <-
  html |>
  html_elements("article") |>
  html_attr("id") |>
  str_remove("post-") |> # R-Zusatzpaket stringr (tidyverse)
  as.numeric()
```

```{r}
# Link

link <-
  html |>
  html_elements("article") |>
  html_attr("id") |>
  str_remove("post-") |> # R-Zusatzpaket stringr (tidyverse)
  as.numeric() %>% # Pipe-Operator, R-Zusatzpaket magrittr (tidyverse)
  paste0("https://www.news4teachers.de/?p=", .)
```

```{r}
# Erscheinungsdatum

datum <-
  html |>
  html_elements("article") |>
  html_elements("time") |>
  html_attr("datetime") |>
  str_sub(end = 10) # R-Zusatzpaket stringr (tidyverse)
```

```{r}
# Anzahl der Kommentare

n_kommentare <-
  html |>
  html_elements("div") |>
  html_elements(".wpd-thread-info") |>
  html_attr("data-comments-count") |>
  as.numeric()
```

```{r}
# Ort der Berichterstattung

ort <-
  html |>
  html_elements("article") |>
  html_element("p") |> # html_element anstatt html_elements
  html_text() |>
  str_extract("[^\\.]+") # R-Zusatzpaket stringr (tidyverse)
```

```{r}
# Titel

titel <-
  html |>
  html_elements("h1") |>
  html_text()
```

```{r}
# Zusammenfassung

zusammenfassung <-
  html |>
  html_elements("article") |>
  html_element("p") |> # html_element anstatt html_elements
  html_text() |>
  str_extract("\\.[\\s](.*)") |> # R-Zusatzpaket stringr (tidyverse)
  str_remove(". ")
```

```{r}
# Haupttext

all_p <-
  html |>
  html_elements(xpath = "//article/div/p") |>
  html_text()

spaces <-
  all_p |>
  str_detect("^\\s*$") # R-Zusatzpaket stringr (tidyverse)

text <-
  all_p[!spaces] %>% 
  .[-1] |>
  list()
```

Der Datensatz `n4t_data` wird mit dem Befehl `tibble()` erstellt (der Datensatz ist auch in @tbl-daten1 dargestellt):

```{r}
n4t_data <-
  tibble(ID, link, datum, n_kommentare, # R-Zusatzpaket tibble (tidyverse)
         ort, titel, zusammenfassung, text)

n4t_data
```

\blandscape

```{r echo = F}
#| label: tbl-daten1
#| tbl-cap: "Datensatz mit einem Beitrag"
n4t_data |>
  mutate(n_kommentare = as.character(n_kommentare)) |>
  mutate(ID = as.character(ID)) |>
  mutate(link = paste0(str_sub(link, start = 1, end = 8), "..."),
         titel = paste0(str_sub(titel, start = 1, end = 20), "..."),
         zusammenfassung =
           paste0(str_sub(zusammenfassung, start = 1, end = 20), "...")) |>
  flextable() |>
  bold(part = "header") |>
  rotate(rotation = "tbrl", align = "bottom", part = "header") |>
  height(height = 6, part = "header") |>
  width(j = 1:8, width = 1) |>
  footnote(i = 1, j = 8, value =
             as_paragraph(c("Absätze des Haupttextes sind als Liste hinterlegt")),
           ref_symbols = c("#"), part = "body", inline = TRUE, sep = "")
```

\elandscape

Dieser Datensatz (@tbl-daten1) enthält nun einen Beitrag von der Seite News4teachers. Wir wollen aber mehrere Beiträge in einem Datensatz bündeln. Für den nächsten Beitrag mit dem Titel "*Sparpläne: Inklusion hängt in NRW am seidenen Faden - Lebenshilfe appelliert*"[^18] wiederholen wir daher die Arbeitsschritte der Extraktion:

[^18]: <https://www.news4teachers.de/2023/09/sparplaene-inklusion-haengt-in-nrw-am-seidenen-faden-lebenshilfe-appelliert/>

```{r echo=FALSE}
html <-
  read_html("https://www.news4teachers.de/2023/09/sparplaene-inklusion-haengt-in-nrw-am-seidenen-faden-lebenshilfe-appelliert/")
```

```{r eval = F}
# Alle Informationen von der Webseite extrahieren

html <-
  read_html("https://www.news4teachers.de/2023/09/
            sparplaene-inklusion-haengt-in-nrw-am-seidenen-faden-
            lebenshilfe-appelliert/")

# Achtung: Zeilenumbruch ist innerhalb des Links nicht erlaubt!
#          An dieser Stelle wurden Zeilenumbrüche nur aufgrund des
#          Platzmangels eingefügt (zwecks Verbesserung der Darstellung).
#          Bitte entfernen Sie alle Zeilenumbrüche im Link, sodass der
#          Link bei Ihnen in einer Zeile erscheint.
```

```{r}
# ID

ID <-
  html |>
  html_elements("article") |>
  html_attr("id") |>
  str_remove("post-") |> # R-Zusatzpaket stringr (tidyverse)
  as.numeric()
```

```{r}
# Link

link <-
  html |>
  html_elements("article") |>
  html_attr("id") |>
  str_remove("post-") |> # R-Zusatzpaket stringr (tidyverse)
  as.numeric() %>% # Pipe-Operator, R-Zusatzpaket magrittr (tidyverse)
  paste0("https://www.news4teachers.de/?p=", .)
```

```{r}
# Erscheinungsdatum

datum <-
  html |>
  html_elements("article") |>
  html_elements("time") |>
  html_attr("datetime") |>
  str_sub(end = 10) # R-Zusatzpaket stringr (tidyverse)
```

```{r}
# Anzahl der Kommentare

n_kommentare <-
  html |>
  html_elements("div") |>
  html_elements(".wpd-thread-info") |>
  html_attr("data-comments-count") |>
  as.numeric()
```

```{r}
# Ort der Berichterstattung

ort <-
  html |>
  html_elements("article") |>
  html_element("p") |> # html_element anstatt html_elements
  html_text() |>
  str_extract("[^\\.]+") # R-Zusatzpaket stringr (tidyverse)
```

```{r}
# Titel

titel <-
  html |>
  html_elements("h1") |>
  html_text()
```

```{r}
# Zusammenfassung

zusammenfassung <-
  html |>
  html_elements("article") |>
  html_element("p") |> # html_element anstatt html_elements
  html_text() |>
  str_extract("\\.[\\s](.*)") |> # R-Zusatzpaket stringr (tidyverse)
  str_remove(". ")
```

```{r}
# Haupttext

all_p <-
  html |>
  html_elements(xpath = "//article/div/p") |>
  html_text()

spaces <-
  all_p |>
  str_detect("^\\s*$") # R-Zusatzpaket stringr (tidyverse)

text <-
  all_p[!spaces] %>% 
  .[-1] |>
  list()
```

Die Informationen aus dem neuen Beitrag werden dem bereits bestehenden Datensatz `n4t_data` mit dem Befehl `add_row()` hinzugefügt:

```{r}
n4t_data <-
  n4t_data |>
  add_row(ID, link, datum, n_kommentare, ort, titel, zusammenfassung, text)
```

\blandscape

```{r echo = F}
#| label: tbl-daten2
#| tbl-cap: "Datensatz mit zwei Beiträgen"
n4t_data |>
  mutate(n_kommentare = as.character(n_kommentare)) |>
  mutate(ID = as.character(ID)) |>
  mutate(link = paste0(str_sub(link, start = 1, end = 8), "..."),
         titel = paste0(str_sub(titel, start = 1, end = 20), "..."),
         zusammenfassung =
           paste0(str_sub(zusammenfassung, start = 1, end = 20), "...")) |>
  flextable() |>
  bold(part = "header") |>
  rotate(rotation = "tbrl", align = "bottom", part = "header") |>
  height(height = 6, part = "header") |>
  width(j = 1:8, width = 1) |>
  footnote(i = 1:2, j = 8, value =
             as_paragraph(c("Absätze des Haupttextes sind als Liste hinterlegt")),
           ref_symbols = c("#"), part = "body", inline = TRUE, sep = "")
```

\elandscape

Der neue Datensatz (@tbl-daten2) enthält nun Informationen für zwei News4teachers-Beiträge. Wir könnten die Arbeitsschritte der Extraktion nun immer wieder wiederholen und somit stetig neue Beiträge zum Datensatz hinzufügen. Bei einer großen Anzahl von Beiträgen wäre dies allerdings ein sehr langwieriger Prozess. Daher müssen wir für die Extraktion, also für das Web-Scraping der Beiträge, eine automatisierte Routine entwickeln.

## Automatisiertes Web-Scraping

### Funktion

Beim Auslesen eines Beitrags von der Webseite News4teachers haben wir bisher stets folgende Arbeitsschritte getätigt (@sec-schritte):

1.  Alle Informationen von der Webseite (Beitragsseite) extrahieren
2.  ID auslesen
3.  Link auslesen
4.  Erscheinungsdatum auslesen
5.  Anzahl der Kommentare auslesen
6.  Ort der Berichterstattung auslesen
7.  Titel auslesen
8.  Zusammenfassung auslesen
9.  Haupttext auslesen
10. Alle Informationen (Arbeitsschritte 1 bis 9) in einem Datensatz zusammenfassen

Diese Arbeitsschritte können wir automatisieren, indem wir die jeweiligen Arbeitsschritte in einer Funktion bündeln. Hierfür schreiben wir eine eigene Funktion [@Wickham2023-hx, Kapitel 26]^[<https://r4ds.hadley.nz/function>]:

```{r}
web_scrape <-
  function(url = NULL) {
    
    # 1. Alle Informationen von der Webseite (Beitragsseite) extrahieren
    
    html <-
      read_html(url)
    
    # 2. ID auslesen
    
    ID <-
      html |>
      html_elements("article") |>
      html_attr("id") |>
      str_remove("post-") |> 
      as.numeric()
    
    # 3. Link auslesen
    
    link <-
      html |>
      html_elements("article") |>
      html_attr("id") |>
      str_remove("post-") |>
      as.numeric() %>%
      paste0("https://www.news4teachers.de/?p=", .)
    
    # 4. Erscheinungsdatum auslesen
    
    datum <-
      html |>
      html_elements("article") |>
      html_elements("time") |>
      html_attr("datetime") |>
      str_sub(end = 10)
    
    # 5. Anzahl der Kommentare auslesen
    
    n_kommentare <-
      html |>
      html_elements("div") |>
      html_elements(".wpd-thread-info") |>
      html_attr("data-comments-count") |>
      as.numeric()
    
    # 6. Ort der Berichterstattung auslesen
    
    ort <-
      html |>
      html_elements("article") |>
      html_element("p") |>
      html_text() |>
      str_extract("[^\\.]+")
    
    # 7. Titel auslesen
    
    titel <-
      html |>
      html_elements("h1") |>
      html_text()
    
    # 8. Zusammenfassung auslesen
    
    zusammenfassung <-
      html |>
      html_elements("article") |>
      html_element("p") |>
      html_text() |>
      str_extract("\\.[\\s](.*)") |>
      str_remove(". ")
    
    # 9. Haupttext auslesen

    all_p <-
      html |>
      html_elements(xpath = "//article/div/p") |>
      html_text()
    
    spaces <-
      all_p |>
      str_detect("^\\s*$")
    
    text <-
      all_p[!spaces] %>% 
      .[-1] |>
      list()
    
    # 10. Alle Informationen (Arbeitsschritte 1 bis 9)
    #     in einem Datensatz zusammenfassen
    
    tibble(ID, link, datum, n_kommentare, ort,
           titel, zusammenfassung, text)
  }
```

Wir können nun die neu erstellte Funktion `web_scrape()` nutzen und müssen lediglich den Link eines Beitrags an die Funktion übergeben. Dann werden die Arbeitsschritte 1 bis 10 ausgeführt und wir erhalten einen Datensatz für den neuen Beitrag (@tbl-daten3):

```{r eval = F}
web_scrape("https://www.news4teachers.de/2023/10/
           montessori-verband-zur-inklusionsdebatte-in-deutschland-ein-
           umdenken-ist-noetig/")

# Achtung: Zeilenumbruch ist innerhalb des Links nicht erlaubt!
#          An dieser Stelle wurden Zeilenumbrüche nur aufgrund des
#          Platzmangels eingefügt (zwecks Verbesserung der Darstellung).
#          Bitte entfernen Sie alle Zeilenumbrüche im Link, sodass der
#          Link bei Ihnen in einer Zeile erscheint.
```

```{r echo = F}
web_scrape("https://www.news4teachers.de/2023/10/montessori-verband-zur-inklusionsdebatte-in-deutschland-ein-umdenken-ist-noetig/")
```

\blandscape

```{r echo = F}
#| label: tbl-daten3
#| tbl-cap: "Datensatz erstellt mit der Funktion `web_scrape()`"
web_scrape("https://www.news4teachers.de/2023/10/montessori-verband-zur-inklusionsdebatte-in-deutschland-ein-umdenken-ist-noetig/") |>
  mutate(n_kommentare = as.character(n_kommentare)) |>
  mutate(ID = as.character(ID)) |>
  mutate(link = paste0(str_sub(link, start = 1, end = 8), "..."),
         titel = paste0(str_sub(titel, start = 1, end = 20), "..."),
         zusammenfassung =
           paste0(str_sub(zusammenfassung, start = 1, end = 20), "...")) |>
  flextable() |>
  bold(part = "header") |>
  rotate(rotation = "tbrl", align = "bottom", part = "header") |>
  height(height = 6, part = "header") |>
  width(j = 1:8, width = 1) |>
  footnote(i = 1, j = 8, value =
             as_paragraph(c("Absätze des Haupttextes sind als Liste hinterlegt")),
           ref_symbols = c("#"), part = "body", inline = TRUE, sep = "")
```

\elandscape

Ein mit der Funktion `web_scrape()` erzeugter Datensatz (@tbl-daten2) kann auch dem bereits erstellten Datensatz `n4t_data` (@tbl-daten3) hinzugefügt werden (@tbl-daten4 = @tbl-daten2 + @tbl-daten3):

```{r eval = F}
n4t_data <-
  n4t_data |>
  bind_rows(web_scrape("https://www.news4teachers.de/2023/10/
                       montessori-verband-zur-inklusionsdebatte-in-
                       deutschland-ein-umdenken-ist-noetig/"))

# Achtung: Zeilenumbruch ist innerhalb des Links nicht erlaubt!
#          An dieser Stelle wurden Zeilenumbrüche nur aufgrund des
#          Platzmangels eingefügt (zwecks Verbesserung der Darstellung).
#          Bitte entfernen Sie alle Zeilenumbrüche im Link, sodass der
#          Link bei Ihnen in einer Zeile erscheint.
```

```{r echo=FALSE}
n4t_data <-
  n4t_data |>
  bind_rows(web_scrape("https://www.news4teachers.de/2023/10/montessori-verband-zur-inklusionsdebatte-in-deutschland-ein-umdenken-ist-noetig/"))

n4t_data
```

\blandscape

```{r echo = F}
#| label: tbl-daten4
#| tbl-cap: "Datensatz mit drei Beiträgen"
n4t_data |>
  mutate(n_kommentare = as.character(n_kommentare)) |>
  mutate(ID = as.character(ID)) |>
  mutate(link = paste0(str_sub(link, start = 1, end = 8), "..."),
         titel = paste0(str_sub(titel, start = 1, end = 20), "..."),
         zusammenfassung =
           paste0(str_sub(zusammenfassung, start = 1, end = 20), "...")) |>
  flextable() |>
  bold(part = "header") |>
  rotate(rotation = "tbrl", align = "bottom", part = "header") |>
  height(height = 6, part = "header") |>
  width(j = 1:8, width = 1.1) |>
  footnote(i = 1:3, j = 8, value =
             as_paragraph(c("Absätze des Haupttextes sind als Liste hinterlegt")),
           ref_symbols = c("#"), part = "body", inline = TRUE, sep = "")
```

\elandscape



# Weiteres

Selector Gadget

Urheber Gesetz

Politeness

Evtl. interessant für das Auslesen der Kommentare:

<https://www.news4teachers.de/2023/08/schaemt-euch-deutschland-steht-vor-den-vereinten-nationen-am-pranger-weil-es-die-inklusion-an-schulen-verweigert/feed/>

# Literatur
