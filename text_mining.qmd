---
title: "Was liest und schreibt man √ºber Inklusion?"
subtitle: "Web-Scraping und Text-Mining mit R am Beispiel einer Online-Nachrichten- und Diskussionsseite f√ºr Lehrkr√§fte"
author: "Pawel R. Kulawiak"
date: last-modified
abstract: "Preprint in Progress"
abstract-title: "Status"
lang: de
bibliography: references.bib
theme: sandstone
toc: true
number-sections: true
toc-depth: 90
toc-expand: true
title-block-banner: "#2A5B60"
code-overflow: wrap
code-block-bg: "#F8F8F8"
code-block-border-left: "#769E3C"
mermaid-format: png
language: 
  title-block-author-single: "Autor"
  title-block-published: "Datum der letzten Ver√§nderung"
format:
  html:
    embed-resources: false
    output-file: "index"
#  pdf:
#    toccolor: "blue"
#    lof: true
#    lot: true
#    link-citations: true
#    include-in-header: 
#      text: |
#        \usepackage{lscape}
#        \newcommand{\blandscape}{\begin{landscape}}
#        \newcommand{\elandscape}{\end{landscape}}
#    header-includes: |
#      \titlehead{\includegraphics[width=6.5in]{images/owl.png}}
#    geometry:
#      - top=30mm
#      - left=30mm
#      - right=30mm
#      - bottom=30mm
#      - heightrounded
#    fig-pos: H
---

```{r}
#| echo: false
#| include: false
options(width = 75)
library(tidyverse)
library(rvest)
library(flextable)
```

\newpage

![](images/owl.png){width="60%" fig-align="left"}

[![](images/pdf.png){width="25%" fig-align="left"}](https://pawelkulawiak.github.io/textmininginclusion/)

[![](images/online.png){width="25%" fig-align="left"}](https://pawelkulawiak.github.io/textmininginclusion/)

\newpage

# Vorwort und Hinweise

TBA -

In einigen Befehlsketten dienen einige R-Befehle lediglich der optischen Optimierung des R-Outputs. Diese nicht relevante R-Befehle sind mit `# don't run` gekennzeichnet und m√ºssen daher nicht ausgef√ºhrt werden. Im angef√ºhrten Beispiel w√§re lediglich die Befehlskette `html |> html_elements("h1") |> html_text()` relevant:

```{r eval = F}
html |>
  html_elements("h1") |>
  html_text() %>% 
  paste0("[", 1:length(.), "] ", '"', . ,'"') |> # don't run
  str_wrap(75) |>                                # don't run
  cat()                                          # don't run
```

# Einleitung

Mein wertgesch√§tzter Kollege Timo L√ºke[^1] hat einst im Rahmen einer Medieninhaltsanalyse deutschsprachiger Printmedien [@l√ºke2014] folgende Forschungsfragen aufgeworfen:

[^1]: <https://timolueke.de/>

-   Welches Verst√§ndnis von Inklusion wird in den deutschen meinungsf√ºhrenden Medien kommuniziert?
-   Welche Argumente f√ºr und gegen die Umsetzung von Inklusion werden genannt?
-   Welche Fallbeispiele werden als Belege angef√ºhrt?

> *"Im Rahmen einer systematischen Inhaltsanalyse (R√∂ssler, 2010) deutscher Printmedien untersuchen wir die √∂ffentliche Berichterstattung zum Thema ‚ÄûInklusion". Dabei wollen wir verbreitete Definitionen, Argumente und Fallbeispiele systematisch erfassen. So sollen langfristig die Analyse des medialen Diskurses und in der Folge eine Versachlichung der kontroversen Debatte √ºber Inklusion erm√∂glicht werden."* [@l√ºke2014]

Erste Ergebnisse der Medieninhaltsanalyse sind in Form einer Posterpr√§sentation verf√ºgbar [@l√ºke2014] und ich erlaube mir die Darstellung des interessanten Posters (@fig-poster).

::: column-body-outset-left
![Posterpr√§sentation von @l√ºke2014: Was liest man √ºber Inklusion?](images/poster.png){#fig-poster fig-alt="Posterpr√§sentation von @l√ºke2014: Was liest man √ºber Inklusion?"}
:::

# Ziele

## Allgemeine Zielsetzung

Ich m√∂chte die Medieninhaltsanalyse von @l√ºke2014 replizieren sowie erweitern und mich dabei auf die Textinhalte einer Online-Nachrichten- und Diskussionsseite f√ºr Lehrkr√§fte fokussieren, n√§mlich News4teachers [@N4T_2022].

## Zielsetzung mit R: Web-Scraping und Text-Mining

Ich m√∂chte exemplarisch aufzeigen, wie die einzelnen Projektphasen der Medieninhaltsanalyse mit der Programmiersprache R umgesetzt werden k√∂nnen. Hierf√ºr werden wir uns auf zwei wichtige Arbeitsschritte fokussieren:

-   **Web-Scraping**, also eine automatisierte Methode zum Extrahieren der Textinformationen von der Webseite News4teachers. Eine Einf√ºhrung in das Thema Web-Scraping mit R bieten @Wickham2023-hx [Kapitel 25][^2].

-   **Text-Mining**: Die mittels Web-Scraping gesammelten Textdaten sollen mit Methoden des Text-Minings analysiert werden. Methoden des Text-Minings fokussieren sich auf die Extraktion von n√ºtzlichen Informationen aus unstrukturierten Textdaten. Unstrukturierte Textdaten sind Texte, die nicht in einer festen Datenbankstruktur vorliegen, also z.B. Textinhalte von Webseiten. Mit Methoden des Text-Minings kann auch der sentimentale Ton eines Textinhalts bzw. die im Text vermittelte subjektive Meinung analysiert werden. Das Hauptziel der sogenannten Sentimentanalyse besteht also darin, die in einem Textdokument ge√§u√üerten Emotionen und Ansichten bez√ºglich eines bestimmten Themas zu identifizieren, in unserem Fall also z.B. ge√§u√üerte Meinungen zum Thema Inklusion. Eine Einf√ºhrung in das Thema Text-Mining mit R bieten @Silge2017-sp.

[^2]: <https://r4ds.hadley.nz/webscraping>

# News4teachers: Online-Nachrichten- und Diskussionsseite f√ºr Lehrkr√§fte

Bevor wir mit dem Web-Scraping und Text-Mining beginnen, betrachten wir zun√§chst das Arbeitsmaterial, also die Webinhalte der Webseite News4teachers, und die entsprechende Selbstbeschreibung der Webseite [@N4T_2022]:

> *"**Wer steckt hinter News4teachers?***
>
> *News4teachers wird von einer Redaktion aus Lehrern und Journalisten betrieben. Die Seite ist ein gemeinsames Projekt von [4teachers](http://www.4teachers.de/), der Service-Plattform von Lehrern f√ºr Lehrer, sowie [der Agentur f√ºr Bildungsjournalismus](http://www.xn--agentur-fr-bildungsjournalismus-wid.de/).*
>
> ***Was ist News4teachers?***
>
> *News4teachers ist eine Nachrichten- und Diskussionsseite, die sich mit seri√∂sen Berichten, Analysen und Kommentaren an p√§dagogische Profis und die an Bildungsthemen interessierte √ñffentlichkeit richtet. Die Redaktion sichtet t√§glich die Meldungen aus Politik, Forschung und Gesellschaft. Auf die Seite gelangt alles, was¬†f√ºr die¬†Bildung¬†wichtig¬†ist. News4teachers¬†bietet¬†also einen aktuellen √úberblick √ºber die relevanten Informationen f√ºr Lehrer, Erzieher, Sch√ºler und Eltern. Und zwar: unabh√§ngig und √ºberparteilich.*
>
> ***Was ist die Idee hinter News4teachers?***
>
> *News4teachers f√ºhlt sich dem klassischen Journalismus verpflichtet. Das hei√üt konkret: Wir unterwerfen uns den publizistischen Grunds√§tzen des Deutschen Presserats, dem [Pressekodex](https://www.presserat.de/pressekodex/pressekodex/). Informationen, die auf die Seite gelangen,¬†wurden zuvor¬†von der Redaktion mit¬†der gebotenen Sorgfalt gepr√ºft. Quellen werden stets¬†genannt, Meinung und Bericht voneinander getrennt. News4teachers unterliegt zudem einer Chronistenpflicht: Alles, was f√ºr die Bildungsdebatte in Deutschland von Bedeutung ist, wird aktuell¬†berichtet. Regelm√§√üige Nutzer von News4teachers sind also immer im Bild."* [@N4T_2022]

Die Redaktion besteht aus folgenden Personen [@N4T_impressum]: Anna H√ºckelheim, Sonja Mankowsky, Laura Millmann, Nina Odenius, Thomas Zab und Milla Priboschek (Podcast-Redaktion).

```{r}
#| echo: false
#| include: false
options(scipen=999)
BESUCHER <- 55000
LK <- 975000
```

## Inhalte von News4teachers und potenzielle Leserschaft aus Lehrkr√§ften

News4teachers verspricht eine unabh√§ngige und √ºberparteiliche Berichterstattung zu Bildungsthemen, wahrscheinlich auch zum Thema Inklusion. Die Inhalte sind f√ºr die Leserschaft kostenfrei (werbefinanziertes Angebot). Die Inhalte von News4teachers sind au√üerdem speziell auf Lehrkr√§fte ausgerichtet. Somit kann angenommen werden, dass ein gro√üer Teil der Leserschaft aus Lehrkr√§ften besteht. Die Internetseite News4teachers hatte folgende Besucherzahlen (Jahr 2023): Mai (54000 Personen), Juni (60000 Personen) und Juli und August jeweils 55000 Personen (Zahlen ermittelt mit: <https://neilpatel.com/website-traffic-checker/>). Nehmen wir an, dass die Leserschaft von News4teachers zu 75% aus Lehrkr√§ften aus Deutschland best√ºnde, dann h√§tten wir bei einer monatlichen Besucherzahl von `r BESUCHER` Personen eine monatliche Leserschaft von ca. `r BESUCHER * 0.75` Lehrkr√§ften (`r BESUCHER` \* 0,75 = `r BESUCHER * 0.75`). In Deutschland gibt es aber laut Mikrozensus 2022 rund `r LK` Lehrkr√§fte an allgemeinbildenden Schulen [@census]. Die potenzielle News4teachers-Leserschaft aus Lehrkr√§ften (`r BESUCHER * 0.75` Personen) entspr√§che dann einem Anteil von ca. `r (BESUCHER/LK*100) |> round(2)`% aller Lehrkr√§fte an allgemeinbildenden Schulen (`r BESUCHER` / `r LK` \* 100 = `r (BESUCHER/LK*100) |> round(2)`%). Im dargestellten Szenario w√ºrden die Inhalte von News4teachers also pro Monat ca. `r (BESUCHER/LK*100) |> round(2)`% der Lehrkr√§fte an allgemeinbildenden Schulen in Deutschland erreichen (5 von 100 Lehrkr√§ften lesen News4teachers). Dies sind aber nur vage Vermutungen zur Reichweite von News4teachers unter Lehrkr√§ften an allgemeinbildenden Schulen in Deutschland, unter der Annahme, dass 75% der Leserschaft von News4teachers aus Lehrkr√§ften best√ºnde.

AUFGREIFEN: \[<https://www.news4teachers.de/2021/12/liebe-leserin-lieber-leser-ein-wort-zum-jahreswechsel-in-eigener-sache/>\]

### Kommentare und Diskussionen

Die Webseite News4teachers bieten der Leserschaft die M√∂glichkeit die Inhalte zu kommentieren und zu diskutieren (@fig-beitrag und @fig-struktur3). Hierf√ºr formuliert die Redaktion spezifische Richtlinien [@N4T_2022]:

> *"**Gibt's Regeln f√ºr die Leserzuschriften in den Foren?***
>
> *Grunds√§tzlich gilt: Niemand hat einen Anspruch darauf, in den Foren zu den einzelnen Artikeln eine eigene Wortmeldung zu ver√∂ffentlichen. Die Redaktion legt Wert darauf, nur Leserzuschriften zu ver√∂ffentlichen, die erkennbar darauf abzielen, einen inhaltlichen Beitrag zur Diskussion des dar√ºberstehenden Artikels zu leisten. Das bedeutet konkret: Auch f√ºr Leserzuschriften gelten die publizistischen Grunds√§tze des Deutschen Presserats, gilt also [der Pressekodex](https://www.presserat.de/pressekodex/pressekodex/).*
>
> *Kurzgefasst:*
>
> -   *Wir ver√∂ffentlichen keine Leserbeitr√§ge, in denen ungepr√ºfte, unbelegte oder falsche Tatsachenbehauptungen verbreitet werden.*
> -   *Wir ver√∂ffentlichen keine Hetze gegen Menschen oder Menschengruppen.*
> -   *Wir ver√∂ffentlichen keine Werbung, ob nun f√ºr Produkte oder Parteien.*
> -   *Und wir ver√∂ffentlichen keine Links auf unseri√∂se Quellen.*
>
> *Wir behalten uns dar√ºber hinaus vor, Leserbriefe, die lediglich der Stimmungsmache dienen, zu l√∂schen. Oder Leserbriefe sinnwahrend zu k√ºrzen."* [@N4T_2022]

\[Hier weitere Erl√§uterungen einf√ºgen\]

##### Facebook und Twitter

Die Beitr√§ge werden aber nicht nur unmittelbar auf der Seite von News4teachers kommentiert und diskutiert (@fig-struktur3). Die Diskussion der Beitr√§ge erfolgt auch auf einer externen Seite, n√§mlich bei Facebook. Innerhalb der Beitr√§ge wird auch auf die externe Diskussion bei Facebook verwiesen (@fig-facebook). Au√üerdem werden die Beitr√§ge von News4teachers ebenfalls bei Twitter geteilt und diskutiert (<https://twitter.com/News4teachers>). Die Kommentare und Diskussionen bei Facebook und Twitter sollen daher auch bei der vorliegenden Medieninhaltsanalyse Ber√ºcksichtigung finden.

![Beitrag zum Thema Inklusion mit 152 Leserkommentaren auf der Internetseite News4teachers [@N4T_2023]](images/beitrag.png){#fig-beitrag fig-alt="Inhalt zum Thema Inklusion mit 152 Leserkommentaren auf der Internetseite News4teachers"}

![Verweis bei News4teachers auf die Diskussion bei Facebook (Diskussion zum Beitrag *"‚ÄûSch√§mt Euch!" -- Deutschland steht vor den Vereinten Nationen am Pranger, weil es die Inklusion an Schulen praktisch verweigert"*)](images/facebook.png){#fig-facebook fig-alt="Verweis bei News4teachers auf die Diskussion bei Facebook"}

# Explorative Forschungsfragen

Die Inhalte von der Webseite News4teachers und die Kommentare und Diskussionen der Leserschaft eignen sich eventuell zur Beantwortung folgender Forschungsfragen:

-   Auf welche Art und Weise wird das Thema Inklusion auf der Online-Nachrichten- und Diskussionsseite f√ºr Lehrkr√§fte dargestellt?
-   Auf welche Art und Weise werden die Inhalte zum Thema Inklusion von der Leserschaft kommentiert und diskutiert?

# Web-Scraping

Der erste Arbeitsschritt, hin zum Text-Mining, also hin zur Medieninhaltsanalyse, wird nun das Web-Scraping sein, also die automatisierte Extraktion der Webinhalte (z.B. Textinformationen) von der Webseite News4teachers. Traditionellerweise bzw. altmodischerweise w√ºrde man Webinhalte mit der Methode "*copy-and-paste*" in einen Datensatz √ºbertragen, also z.B. Text von einer Webseite kopieren und anschlie√üend die kopierte Textinformation in einen Datensatz einf√ºgen (z.B. bei Excel). Dieses Verfahren ist aber fehleranf√§llig, da z.B. die Gefahr besteht, dass aufgrund mangelnder Konzentration falsche oder unvollst√§ndige Textinhalte √ºbertragen werden. Web-Scraping ist daher als automatisierte Methode der Extraktion von Webinhalten weniger anf√§llig f√ºr Fehler und somit die Methode der Wahl. Eine Einf√ºhrung in das Thema Web-Scraping mit R bieten @Wickham2023-hx [Kapitel 25][^3].

[^3]: <https://r4ds.hadley.nz/webscraping>

## R-Zusatzpakete

### R-Zusatzpakete *rvest* und *tidyverse*

F√ºr das Web-Scraping nutzen wir nun das R-Zusatzpaket *rvest* [@rvest]. Der Name des R-Zusatzpaketes ist eine gelungene Anspielung auf das englische Wort *harvest* (ernten, sammeln), denn wir wollen ja Informationen aus dem Internet sammeln (mit R). Das kreative Wortspiel ist auch im Logo des R-Zusatzpaketes visualisiert (@fig-logo). Das R-Zusatzpaket *rvest* ist in der R-Paketsammlung *tidyverse* [@tidyverse] enthalten. *tidyverse* ist eine Zusammenstellung unterschiedlicher R-Zusatzpakete. Wir werden an diversen Stellen die herausragende Funktionalit√§t der Paketsammlung *tidyverse* nutzen. An den entsprechenden Stellen wird ein Verweis auf die *tidyverse*-Zusatzpakete erfolgen. Mit der Installation von *tidyverse* wird auch das Web-Scraping-Zusatzpaket *rvest* installiert (`install.packages("tidyverse")`). Die Paketsammlung *tidyverse* und das darin enthaltene Web-Scraping-Zusatzpaket *rvest* m√ºssen anschlie√üend mit dem Befehl `library(tidyverse, rvest)` geladen werden:

```{r}
#| eval: false
install.packages("tidyverse")
library(tidyverse, rvest)
```

Das R-Zusatzpaket *rvest* verf√ºgt √ºber eine umfassende und hilfreiche Online-Dokumentation:

-   <https://rvest.tidyverse.org/>
-   <https://r4ds.hadley.nz/webscraping> [@Wickham2023-hx, Kapitel 25]

Informationen zum Paketsammlung *tidyverse* findet man hier:

<https://www.tidyverse.org/>

![Logo des R-Zusatzpaketes *rvest*](images/logo.png){#fig-logo fig-alt="Logo des R-Zusatzpaketes rvest"}

## Struktur und Inhalte der Webseite

Ziel des Web-Scapings wird es sein, die relevanten Webinhalte von News4teachers automatisiert zu extrahieren. Hierf√ºr m√ºssen wir uns erstmal einen √úberblick √ºber die Struktur und Inhalte der Webseite verschaffen. Die Beitr√§ge auf den Internetseiten von News4teachers haben eine spezifische Struktur mit spezifischen Webinhalten. Wir betrachten den Beitrag mit dem Titel *"‚ÄûSch√§mt Euch!" -- Deutschland steht vor den Vereinten Nationen am Pranger, weil es die Inklusion an Schulen praktisch verweigert"* [@N4T_2023]. F√ºr unsere Forschungsfragen mehr oder weniger interessante Webinhalte sind in den Abbildungen kenntlich gemacht (@fig-struktur1, @fig-struktur2 und @fig-struktur3).

![(a) Beitrag auf der Internetseite News4teachers und Struktur der Webinhalte [@N4T_2023]](images/struktur1.png){#fig-struktur1 fig-alt="Beitrag auf der Internetseite News4teachers und Struktur der Webinhalte"}

![(b) Beitrag auf der Internetseite News4teachers und Struktur der Webinhalte [@N4T_2023]](images/struktur2.png){#fig-struktur2 fig-alt="Beitrag auf der Internetseite News4teachers und Struktur der Webinhalte"}

![(c) Beitrag auf der Internetseite News4teachers und Struktur der Webinhalte [@N4T_2023]](images/struktur3.png){#fig-struktur3 fig-alt="Beitrag auf der Internetseite News4teachers und Struktur der Webinhalte"}

\[Erl√§uterungen zu den Abbildungen und Inhalten hinzuf√ºgen\]

## Erster Web-Scraping-Versuch

Zuvor haben wir uns einen √úberblick √ºber die zu extrahierenden Webinhalte verschafft. F√ºr den ersten Web-Scraping-Versuch nutzen wir weiterhin den Beitrag mit dem Titel *"‚ÄûSch√§mt Euch!" -- Deutschland steht vor den Vereinten Nationen am Pranger, weil es die Inklusion an Schulen praktisch verweigert"* (@fig-struktur1). Dies ist der Link zum Beitrag:

<https://www.news4teachers.de/2023/08/schaemt-euch-deutschland-steht-vor-den-vereinten-nationen-am-pranger-weil-es-die-inklusion-an-schulen-verweigert/>

Wir nutzen den Befehl `read_html()` und den entsprechenden Link, um s√§mtliche Informationen von der Webseite zu extrahieren:

```{r, include = F}
html <-
  read_html("https://www.news4teachers.de/2023/08/schaemt-euch-deutschland-steht-vor-den-vereinten-nationen-am-pranger-weil-es-die-inklusion-an-schulen-verweigert/")
```

```{r eval = F}
html <-
  read_html("https://www.news4teachers.de/2023/08/
            schaemt-euch-deutschland-steht-vor-den-vereinten-nationen-
            am-pranger-weil-es-die-inklusion-an-schulen-verweigert/")

# Achtung: Zeilenumbruch ist innerhalb des Links nicht erlaubt!
#          An dieser Stelle wurden Zeilenumbr√ºche nur aufgrund des
#          Platzmangels eingef√ºgt (zwecks Verbesserung der Darstellung).
#          Bitte entfernen Sie alle Zeilenumbr√ºche im Link, sodass der
#          Link bei Ihnen in einer Zeile erscheint.
```

Alle Webinhalte sind nun im Objekt `html` hinterlegt. Wir sind allerdings nur an spezifischen Webinhalten interessiert und m√∂chten daher im n√§chsten Schritt einen spezifischen Textinhalt aus dem Objekt `html` auslesen. Wir wollen den Titel des Beitrages extrahieren: *"‚ÄûSch√§mt Euch!" -- Deutschland steht vor den Vereinten Nationen am Pranger, weil es die Inklusion an Schulen praktisch verweigert"*. Dabei ist es gar nicht so leicht, einen spezifischen Inhalt wie den Titel zu lokalisieren und auszulesen. Hierf√ºr ist HTML-[^4] und CSS-Selector-Grundlagenwissen[^5] hilfreich. Die eigentlichen Textinhalte sind n√§mlich im HTML-Dokument der Webseite hinterlegt (HTML-Quelltext). Ist eine Internetseite im Browser ge√∂ffnet, so gelangen wir mit einem Rechtsklick i.d.R. zur Option *"Seitenquelltext anzeigen"* (@fig-quelltext). Dies f√ºhrt uns zum HTML-Dokument bzw. zum HTML-Quelltext der Webseite (@fig-html1).

[^4]: <https://developer.mozilla.org/en-US/docs/Web/HTML>

[^5]: <https://developer.mozilla.org/en-US/docs/Learn/CSS/Building_blocks/Selectors>; *"CSS includes a miniature language for selecting elements on a page called CSS selectors. CSS selectors define patterns for locating HTML elements, and are useful for scraping because they provide a concise way of describing which elements you want to extract."*, Quelle: <https://rvest.tidyverse.org/articles/rvest.html>

![Seitenquelltext (HTML) anzeigen](images/quelltext.png){#fig-quelltext fig-alt="Seitenquelltext (HTML) anzeigen"}

::: column-body-outset-left
![HTML-Quelltext (Ausschnitt)](images/html1.png){#fig-html1 fig-alt="HTML-Seitenquelltext (Ausschnitt)"}
:::

Das HTML-Dokument (@fig-html1) ist riesig (mehr als 10000 Zeilen) und wir m√ºssen etwas st√∂bern, um den passenden Webinhalt zu lokalisieren. Der HTML-Code aus dem HTML-Dokument (@fig-html1) ist zwecks besserer Lesbarkeit auch nachfolgend dargestellt (auszugsweise):

```{html}
<article id="post-132285" class="post-132285 post type-post status-publish
  format-standard has-post-thumbnail category-leben category-titelthema
  category-wissenschaft tag-forderschulen tag-inklusion
  tag-un-behindertenrechtskonvention" itemscope
  itemtype="https://schema.org/Article">
  <div class="td-post-header">
    <!-- category -->
    <ul class="td-category">
      <li class="entry-category">
        <a href="https://www.news4teachers.de/bildung/leben/">Leben</a>
      </li>
      <li class="entry-category">
        <a href="https://www.news4teachers.de/bildung/titelthema/">
        Titelthema</a>
      </li>
      <li class="entry-category">
        <a href="https://www.news4teachers.de/bildung/wissenschaft/">
        Wissen</a>
      </li>
    </ul>
    <header class="td-post-title">
      <h1 class="entry-title">&#8222;Sch√§mt Euch!&#8220; &#8211;
        Deutschland steht vor den Vereinten Nationen am Pranger, weil es
        die Inklusion an Schulen praktisch verweigert
      </h1>
      <div class="td-module-meta-info">
        <!-- author -->
        <!-- date -->
        <span class="td-post-date">
        <time class="entry-date updated td-module-date"
          datetime="2023-08-29T12:46:06+02:00">29. August 2023</time>
        </span>
        <!-- comments -->
        <div class="td-post-comments">
          <a href="https://www.news4teachers.de/2023/08/schaemt-euch-
deutschland-steht-vor-den-vereinten-nationen-am-pranger-weil
-es-die-inklusion-an-schulen-verweigert/#comments">
          <i class="td-icon-comments"></i>150
          </a>
        </div>
        <!-- views -->
      </div>
    </header>
  </div>
</article>
```

Wir sehen z.B. in der Zeile 1297 (@fig-html1), dass der Titel des Beitrages ein `h1`-HTML-Element[^6] ist (header 1: √úberschrift erster Ebene):

[^6]: <https://developer.mozilla.org/en-US/docs/Web/HTML/Element/Heading_Elements>

```{html}
<h1 class="entry-title">&#8222;Sch√§mt Euch!&#8220; &#8211; Deutschland
  steht vor den Vereinten Nationen am Pranger, weil es die Inklusion an 
  Schulen praktisch verweigert</h1>
```

Diese Information ben√∂tigen wir, um den Titel des Beitrags gezielt auszulesen. Hierf√ºr nutzen wir den Befehl `html_elements("h1")` und √ºbergeben das Objekt `html` an diesen Befehl.

```{r}
html |>
  html_elements("h1")
```

Die Information `<h1 class="entry-title">` ist √ºberfl√º√üig, da wir nur am HTML-Textinhalt interessiert sind. Daher extrahieren wir den reinen Textinhalt, also den Titel, mit dem Befehl `html_text()`. Die Befehlskette wird entsprechend erweitert:

```{r}
html |>
  html_elements("h1") |>
  html_text() %>% 
  paste0("[", 1:length(.), "] ", '"', . ,'"') |> # don't run
  str_wrap(75) |>                                # don't run
  cat()                                          # don't run
```

Herzlichen Gl√ºckwunsch! ü•≥ Somit haben wir erfolgreich alle Informationen von der Webseite extrahiert und eine relevante Textstelle (den Titel) ausgelesen.

### Datenstruktur

Im HTML-Code (@fig-html1) sehen wir, dass anscheinend jeder Beitrag √ºber eine ID verf√ºgt (`id="post-132285"`). Wenn wir in unserem zuk√ºnftigen Datensatz mehrere Beitr√§ge abspeichern wollen, dann wird eine ID-Variable zwecks Unterscheidung der Beitr√§ge eine hilfreiche Sache sein. @tbl-idee ist eine erste Idee bez√ºglich einer m√∂glichen/sinnvollen Datenstruktur. Bei dieser Datenstruktur ignorieren wir der Einfachheit halber vorerst ein paar relevante Webinhalte, z.B. Kommentare und Anzahl der Likes ("*Gef√§llt mir*").

\newpage

\blandscape

```{r}
#| echo: false
#| label: tbl-idee
#| tbl-cap: Erste Idee bez√ºglich einer m√∂glichen/sinnvollen Datenstruktur
ID <- c(132285, "...", "...")
link <- c("https://...", "...", "...")
datum <- c("29.\nAugust\n2023", "...", "...")
n_kommentare <- c("150", "...", "...")
ort <- c("GENF", "...", "...")
titel <- c("'Sch√§mt Euch!' --\nDeutschland steht\nvor den Vereinten\nNationen am Pranger...", "...", "...")
zusammenfassung <- c("‚ÄûSch√§mt Euch!‚Äú --\nso hei√üt es auf\neinem Transparent...", "...", "...")
haupttext <- c("Der offizielle Beitrag\nDeutschlands f√§llt\nd√ºnn aus...", "...", "...")
usw. <- c("...", "...", "...")

data.frame(ID, link, datum, n_kommentare, ort, titel, zusammenfassung, haupttext, usw.) |>
  flextable() |>
  #autofit() |>
  bold(part = "header") |>
  rotate(rotation = "tbrl", align = "bottom", part = "header") |>
  width(j = 1:5, width = 0.6) |>
  width(j = 2, width = 0.8) |>
  width(j = 6:8, width = 1.5) |>
  width(j = 9, width = 0.3) |>
  height(height = 5, part = "header")
```

\elandscape

\[Gef√§llt mir???\]

### Weitere Web-Scraping-Schritte {#sec-schritte}

Um die Datenstruktur aus @tbl-idee zu realisieren, m√ºssen wir nun die ID des Beitrags, den Link, das Erscheinungsdatum, die Anzahl der Kommentare, die Zusammenfassung und den eigentlichen Haupttext des Beitrages auslesen (den Titel haben wir ja bereits erfolgreich extrahiert). Beginnen wir mit der ID.

#### ID {#sec-link}

In @fig-html1 sehen wir, das die ID des Beitrages (`id="post-132285"`) ein Attribut[^7] eines HTML-Elements ist (HTML-Element: `article`[^8]):

[^7]: <https://developer.mozilla.org/en-US/docs/Web/HTML/Global_attributes/id>

[^8]: <https://developer.mozilla.org/en-US/docs/Web/HTML/Element/article>

```{html}
<article id="post-132285" class="post-132285 post type-post status-publish
format-standard has-post-thumbnail category-leben category-titelthema
category-wissenschaft tag-forderschulen tag-inklusion
tag-un-behindertenrechtskonvention" itemscope
itemtype="https://schema.org/Article">
```

Daher √ºbergeben wir das Objekt `html` zwecks Auslesung der ID zun√§chst an den Befehl `html_elements("article")` und dann an den Befehl `html_attr("id")`:

```{r}
html |>
  html_elements("article") |>
  html_attr("id")
```

Die ID des Beitrags erscheint mit dem Pr√§fix `"post-"`, eine nicht notwendigerweise n√ºtzliche Information. Das Pr√§fix entfernen wir daher mit dem Befehl `str_remove("post-")` und √ºberf√ºhren die ID mit dem Befehl `as.numeric()` in ein nummerisches Format. Somit erhalten wir die nummerische ID `132285`:

```{r}
html |>
  html_elements("article") |>
  html_attr("id") |>
  str_remove("post-") |> # R-Zusatzpaket stringr (tidyverse)
  as.numeric()
```

#### Link

Der Beitrag verf√ºgt √ºber einen langen Link:

<https://www.news4teachers.de/2023/08/schaemt-euch-deutschland-steht-vor-den-vereinten-nationen-am-pranger-weil-es-die-inklusion-an-schulen-verweigert/>

Im HTML-Code ist allerdings auch ein kurzer Link, also ein `shortlink`, ausgewiesen:

```{html}
<link rel='shortlink' href='https://www.news4teachers.de/?p=132285' />
```

Die ID des Beitrags (`132285`) ist Bestandteil des kurzen Links. Wir k√∂nnen also den ersten Teil des kurzen Links (`"https://www.news4teachers.de/?p="`) mit der ID (`132285`) verbinden, um den gew√ºnschten Kurzlink zu generieren. Hierf√ºr nutzen wir nach der Auslesung der ID den Befehl `paste0("https://www.news4teachers.de/?p=", .)`. Mit dem magrittr-Pipe-Operator (`%>%`[^9]) wird die ID an das zweite Argument des Befehls `paste0("https://www.news4teachers.de/?p=", .)` √ºbergeben, also an die Stelle mit dem Punkt (`.`). Eine √úbergabe an das zweite Argument w√§re mit der sogenannten base-Pipe (`|>`) nicht m√∂glich, daher nutzen wir die magrittr-Pipe (`%>%`) anstatt der base-Pipe (`|>`). Die Befehlskette zur Erstellung des Links gestaltet sich somit folgenderma√üen:

[^9]: <https://magrittr.tidyverse.org/>

```{r}
html |>
  html_elements("article") |>
  html_attr("id") |>
  str_remove("post-") |> # R-Zusatzpaket stringr (tidyverse)
  as.numeric() %>% # Pipe-Operator, R-Zusatzpaket magrittr (tidyverse)
  paste0("https://www.news4teachers.de/?p=", .) 
```

#### Erscheinungsdatum

Fahren wir fort mit dem Auslesen des Erscheinungsdatums des Beitrages. Im HTML-Code (@fig-html1, Zeile 1301) erscheint folgende Information:

```{html}
<span class="td-post-date"><time class="entry-date updated td-module-date"
datetime="2023-08-29T12:46:06+02:00" >29. August 2023</time></span> 
```

Wir sehen, dass das Datum ein HTML-Element ist, n√§mlich ein `time`-Element[^10]. Dieses `time`-Element ist innerhalb eines `span`-Elements[^11] geschachtelt. Wir k√∂nnen hier also von einer hierarchischen Schachtelung der HTML-Elemente sprechen (`span -> time`, @fig-schachtelung_1).

[^10]: <https://developer.mozilla.org/en-US/docs/Web/HTML/Element/time>

[^11]: <https://developer.mozilla.org/en-US/docs/Web/HTML/Element/span>

```{mermaid}
%%| label: fig-schachtelung_1
%%| fig-cap: Hierarchische Schachtelung der HTML-Elemente `span` und `time`
%%{init: {'theme':'forest'}}%%
flowchart TB
  id1(span) --> id2(time)
```

Entsprechend erfolgt die Extraktion des Datums mit der √úbergabe des Objektes `html`, zun√§chst an den Befehl `html_elements("span")`, und anschlie√üend an den Befehl `html_elements("time")`:

```{r}
html |>
  html_elements("span") |>
  html_elements("time")
```

Das Ergebnis ist aber nicht ganz befriedigend, da mehrere Datumsangaben extrahiert worden sind, unter anderem das gew√ºnschte Erscheinugsdatum des Beitrages (`2023-08-29`), aber auch andere, nicht relevate Datumsangaben (z.B. `2023-09-17`), welche ebenfalls auf der Webseite erscheinen (@fig-datum).

![Verweis auf einen anderen Beitrag mit nicht relevanter Datumsangabe](images/datum.png){#fig-datum fig-alt="Verweis auf einen anderen Beitrag mit nicht relevanter Datumsangabe"}

Wir m√ºssen daher beim Auslesen noch genauer die hierarchische Position des Erscheinungsdatums definieren. Ein Blick auf @fig-html1 offenbart, dass die beiden HTML-Elemente `span` und `time` innerhalb des bereits bekannten HTML-Elements `article` geschachtelt sind (`article -> span -> time`, @fig-schachtelung_2).

```{mermaid}
%%| label: fig-schachtelung_2
%%| fig-cap: Hierarchische Schachtelung der HTML-Elemente `article`, `span` und `time`
%%{init: {'theme':'forest'}}%%
flowchart TB
  id1(article) --> id2(span) --> id3(time)
```

Diese hierarchische Schachtelung (`article -> span -> time`) muss daher beim Auslesen des Erscheingsdatums beachtet werden:

```{r}
html |>
  html_elements("article") |>
  html_elements("span") |>
  html_elements("time")
```

Das Erscheinungsdatum ist in diesem Falle das einzige `time`-Element innerhalb des `article`-Elements. Daher f√ºhrt auch das Weglassen des `span`-Elements und somit die Anwendung einer reduzierten hierarchischen Schachtelung der HTML-Elemente (`article -> time`, @fig-schachtelung_3) zum gew√ºnschten Erfolg:

```{mermaid}
%%| label: fig-schachtelung_3
%%| fig-cap: Reduzierte hierarchische Schachtelung der HTML-Elemente `article` und `time`
%%{init: {'theme':'forest'}}%%
flowchart TB
  id1(article) --> id2(time)
```

```{r}
html |>
  html_elements("article") |>
  html_elements("time")
```

Auch bei der Datumsangabe wollen wir uns auf die wesentliche Information fokussieren und extrahieren daher die reine Datumsangabe, die dem Attribut `"datetime"` zugeordnet ist. Die Befehlskette wird daher um den Befehl `"html_attr("datetime")"` erg√§nzt:

```{r}
html |>
  html_elements("article") |>
  html_elements("time") |>
  html_attr("datetime")
```

Die Datumsangabe (`"2023-08-29T12:46:06+02:00"`) beinhaltet eine f√ºr uns nicht relevante Zeitangabe, also die genaue Uhrzeit der Beitragserscheinung (`T12:46:06+02:00`). Die ersten 10 Zeichen (inkl. Bindestriche: `JJJJ-MM-TT`/`2023-08-29`) beinhalten die relevante Datumsangabe. Die nicht relevante Zeitangabe entfernen wir, indem wir lediglich die ersten 10 Zeichen der Datumsangabe beibehalten. Hierf√ºr erg√§nzen wir die Befehlskette um den Befehl `str_sub(end = 10)`:

```{r}
html |>
  html_elements("article") |>
  html_elements("time") |>
  html_attr("datetime") |>
  str_sub(end = 10) # R-Zusatzpaket stringr (tidyverse)
```

#### Anzahl der Kommentare

Die Anzahl der Kommentare ist im HTML-Element `div`[^12] hinterlegt. Und dieses HTML-Element `div` ist durch eine CSS-Klasse[^13] gekennzeichnet (`class="wpd-thread-info"`), wobei die eigentliche Anzahl der Kommentare ein HTML-Attribut ist (`data-comments-count="150"`). Im HTML-Seitenquelltext sieht dies folgenderma√üen aus:

[^12]: <https://developer.mozilla.org/en-US/docs/Web/HTML/Element/div>

[^13]: <https://developer.mozilla.org/en-US/docs/Learn/CSS/Building_blocks/Selectors>; *"CSS includes a miniature language for selecting elements on a page called CSS selectors. CSS selectors define patterns for locating HTML elements, and are useful for scraping because they provide a concise way of describing which elements you want to extract."*, Quelle: <https://rvest.tidyverse.org/articles/rvest.html>

```{html}
<div class="wpd-thread-info" data-comments-count="150">
  <span class='wpdtc' title='150'>150</span> Kommentare </div>
```

Das Objekt `html` wird daher an den Befehl `html_elements("div")` √ºbergeben. Die anschlie√üende Angabe der CSS-Klasse `"wpd-thread-info"` erfolgt mit einem vorangestellten Punkt[^14] (`".wpd-thread-info"`) innerhalb des Befehls `html_elements(".wpd-thread-info")`:

[^14]: Die Angabe einer CSS-Klasse muss immer mit einem vorangestellten Punkt erfolgen

```{r}
html |>
  html_elements("div") |>
  html_elements(".wpd-thread-info")
```

Die eigentliche Extraktion der Anzahl der Kommentare erfolgt nun mit der Angabe des entsprechenden HTML-Attributes (`"data-comments-count"`) innerhalb des Befehls `html_attr("data-comments-count")`. Anschlie√üend wird die Anzahl der Kommentare mit dem Befehl `as.numeric()` in ein nummerisches Format √ºberf√ºhrt. Die Befehlskette gestaltet sich daher folgenderma√üen:

```{r}
html |>
  html_elements("div") |>
  html_elements(".wpd-thread-info") |>
  html_attr("data-comments-count") |>
  as.numeric()
```

#### Ort der Berichterstattung

Die Ortsangabe ist Bestandteil der Zusammenfassung (siehe @fig-struktur1) und die Zusammenfassung ist ein Absatz, i.d.R. der erste Absatz des Beitrages. F√ºr die Extraktion der Ortsangabe ist daher das HTML-Element f√ºr Abs√§tze notwendig (`p`[^15]; `p` steht f√ºr "paragraph"). Dieses HTML-Element (`p`) ist wie gewohnt innerhalb des HTML-Elements `article` geschachtelt. Zur Extraktion des ersten Absatzes wird diesmal der Befehl `html_element("p")` anstatt `html_elements("p")` genutzt. Der Befehl `html_elements("p")` w√ºrde alle Abs√§tze des Beitrages extrahieren. Wir ben√∂tigen aber nur den ersten Absatz mit der Ortsangabe und daher nutzen wir diesmal den Befehl `html_element("p")` anstatt `html_elements("p")`. Die Befehlskette gestaltet sich daher wie folgt:

[^15]: <https://developer.mozilla.org/en-US/docs/Web/HTML/Element/p>

```{r}
html |>
  html_elements("article") |>
  html_element("p") |> # html_element() anstatt html_elements()
  html_text() %>% 
  paste0("[", 1:length(.), "] ", '"', . ,'"') |> # don't run
  str_wrap(75) |>                                # don't run
  cat()                                          # don't run
```

Somit sehen wir den Absatz mit der Ortsangabe. Wir ben√∂tigen allerdings nur die Ortsangabe, also das erste Wort des Absatzes. Hinter der gew√ºnschten Ortsangabe steht ein Punkt (`GENF.`). Mit dem Befehl `str_extract("[^.]+")`[^16] extrahieren wir alle Zeichen vor dem ersten Punkt, also die Ortsangabe `GENF`. Die Befehlskette gestaltet sich daher wie folgt:

[^16]: Bei so einer kryptischen Formel (`"[^.]+"`) handelt es sich um eine sogenannte "*regular expression*" (*regex*). Eine Einf√ºhrung in diese Thematik findet man hier: <https://r4ds.hadley.nz/regexps> [@Wickham2023-hx, Kapitel 16].

```{r}
html |>
  html_elements("article") |>
  html_element("p") |> # html_element anstatt html_elements
  html_text() |>
  str_extract("[^\\.]+") # R-Zusatzpaket stringr (tidyverse)
```

#### Zusammenfassung des Beitrages

Wie soeben bei der Extraktion der Ortsangabe erw√§hnt, ist die Zusammenfassung des Beitrages der erste Absatz des Textes (siehe @fig-struktur1). Der erste Absatz wurde soeben folgenderma√üen extrahiert:

```{r}
html |>
  html_elements("article") |>
  html_element("p") |> # html_element() anstatt html_elements()
  html_text() %>% 
  paste0("[", 1:length(.), "] ", '"', . ,'"') |> # don't run
  str_wrap(75) |>                                # don't run
  cat()                                          # don't run
```

Somit erhalten wir die Zusammenfassung mit der Ortsangabe inkl. Punkt (`GENF.`). Nun wollen wir die √ºberfl√ºssige Ortsangabe entfernen und nur die eigentliche Zusammenfassung beibehalten. Dies erreichen wir mit dem Befehl `str_extract("\\.[\\s](.*)")`. Die regex-Formel `"\\.[\\s](.*)"` hat folgende Bedeutung:

-   `\\.` Suche und extrahiere Zeichen nach dem ersten Punkt (einschlie√ülich des ersten Punktes)
-   `[\\s]` Die extrahierten Zeichen k√∂nnen Leerzeichen sein ("*s*" steht f√ºr "*space*")
-   `(.*)` Extrahiere au√üerdem alle weiteren Zeichen

Die Befehlskette gestaltet sich daher wie folgt:

```{r}
html |>
  html_elements("article") |>
  html_element("p") |> # html_element anstatt html_elements
  html_text() |>
  str_extract("\\.[\\s](.*)") %>% # R-Zusatzpaket stringr (tidyverse) 
  paste0("[", 1:length(.), "] ", '"', . ,'"') |> # don't run
  str_wrap(75) |>                                # don't run
  cat()                                          # don't run
```

Die Ortsangabe (`GENF`) wurde erfolgreich entfernt. Der Punkt hinter der Ortsangabe (`GENF.`) wurde allerdings nicht entfernt und bleibt bestehen. Die Zusammenfassung beginnt daher nun mit einem Punkt (`.`) gefolgt von einem Leerzeichen. Wir entfernen den Punkt und das Leerzeichen (`". "`) mit dem Befehl `str_remove(". ")`. Die Befehlskette zur Extraktion der Zusammenfassung gestaltet sich daher folgenderma√üen:

```{r}
html |>
  html_elements("article") |>
  html_element("p") |> # html_element anstatt html_elements
  html_text() |>
  str_extract("\\.[\\s](.*)") |> # R-Zusatzpaket stringr (tidyverse)
  str_remove(". ") %>% # R-Zusatzpaket stringr (tidyverse) 
  paste0("[", 1:length(.), "] ", '"', . ,'"') |> # don't run
  str_wrap(75) |>                                # don't run
  cat()                                          # don't run
```

#### Haupttext

Kommen wir nun zum Filetst√ºck, also zum eigentlichen Haupttext des Beitrages. Der Beitragstext besteht aus Abs√§tzen. Also k√∂nnen wir, wie bereits gewohnt, das HTML-Element `p` ber√ºcksichtigen. Und dieses HTML-Element `p` ist bekannterweise innerhalb des HTML-Elements `article` geschachtelt. Bisher haben wir die eigentliche hierarchische Schachtelung des Haupttextes allerdings nie expliziert. Die Abs√§tze des Haupttextes (`p`) sind n√§mlich Bestandteil des HTML-Elements `div` und dieses HTML-Element ist wiederum Bestandteil des HTML-Elements `article` (`article -> div -> p`; @fig-schachtelung_4).

```{mermaid}
%%| label: fig-schachtelung_4
%%| fig-align: left
%%| fig-cap: Hierarchische Schachtelung der HTML-Elemente `article`, `div` und `p`
%%{init: {'theme':'forest'}}%%
flowchart TB
  id1(article) --> id2(div) --> id3(p)
```

Zur Abbildung dieser hierarchischen Schachtelung nutzen wir diesmal aus Gr√ºnden eine sogenannte *xpath*-Angabe[^17] (`"//article/div/p"`). Die *xpath*-Angabe (`"//article/div/p"`) erfolgt innerhalb des Befehls `html_elements(xpath = "//article/div/p")`:

[^17]: <https://developer.mozilla.org/en-US/docs/Web/XPath>

```{r}
html |>
  html_elements(xpath = "//article/div/p") |>
  html_text() |>
  str_wrap(75) %>%                                     # don't run
  paste0("\n", "[", 1:length(.), "] ", '"', . ,'"') |> # don't run
  cat()                                                # don't run
```

Mit der obigen Befehlskette werden alle relevanten Abs√§tze extrahiert, aber auch 3 Abs√§tze ohne jeglichen Inhalt (`[15]`, `[16]` und `[17]`). Diese Abs√§tze enthalten keine Zeichen oder ausschlie√ülich Leerzeichen. Wir wollen daher Abs√§tze, die keine Zeichen oder ausschlie√ülich Leerzeichen enthalten, entfernen. Hierf√ºr speichern wir vor√ºbergehend alle Abs√§tze im Textformat als Objekt `all_p` ab. Wir √úberpr√ºfen anschlie√üend, ob der Textinhalt der Abs√§tze keine Zeichen oder ausschlie√ülich Leerzeichen enth√§lt (`all_p |> str_detect("^\\s*$")`). Die regex-Formel `"^\\s*$"` hat folgende Bedeutung:

-   `^` Beginne die Suche am Anfang des Absatzes
-   `\\s*` Suche Leerzeichen bzw. keine Zeichen ("*s*" steht f√ºr "*space*")
-   `$` F√ºhre diese Suche bis zum Ende des Absatzes durch

Das Ergebnis der √úberpr√ºfung ist f√ºr jeden Absatz eine `TRUE`-`FALSE`-Aussage (`TRUE`: Keine Zeichen oder ausschlie√ülich Leerzeichen; `FALSE`: Andere Zeichen). Diese `TRUE`-`FALSE`-Aussage speichern wir als Objekt `spaces` ab und lassen uns abschlie√üend Abs√§tze anzeigen, welche nicht ausschlie√ülich aus Leerzeichen oder keinen Zeichen bestehen (`all_p[!spaces]`):

```{r}
all_p <-
  html |>
  html_elements(xpath = "//article/div/p") |>
  html_text()

all_p |>
  str_detect("^\\s*$") # R-Zusatzpaket stringr (tidyverse)

spaces <-
  all_p |>
  str_detect("^\\s*$") # R-Zusatzpaket stringr (tidyverse)

all_p[!spaces] |>
  str_wrap(75) %>%                                     # don't run
  paste0("\n", "[", 1:length(.), "] ", '"', . ,'"') |> # don't run
  cat()                                                # don't run
```

Fast geschafft! Wir m√ºssen nur noch den ersten Absatz entfernen (`all_p[!spaces] %>% .[-1]`), da der erste Absatz die Zusammenfassung mit Ortsangabe darstellt (`[1] "GENF. ‚ÄûSch√§mt Euch!‚Äú ‚Äì so hei√üt es auf einem Transparent..."`) und daher nicht zum Haupttext gez√§hlt werden kann. Der Punkt vor der eckigen Klammer (`.[-1]`) symbolisiert die Abs√§tze (`all_p[!spaces]`). Bei der √úbergabe dieser Abs√§tze nutzen wir, wie zuvor beim Auslesen des Links (@sec-link), die magrittr-Pipe (`%>%`), da nur diese Pipe, und nicht die base-Pipe (`|>`), eine √úbergabe an eine Bedingung in eckigen Klammern erm√∂glicht (`.[-1]`). Alle Abs√§tze werden anschlie√üend mit dem Befehl `list()` in eine Liste √ºberf√ºhrt. Dies ist daher die schlussendliche Befehlskette, welche zur Darstellung des Haupttextes f√ºhrt:

```{r eval = F}
all_p[!spaces] %>% # Pipe-Operator, R-Zusatzpaket magrittr (tidyverse)
  .[-1] |>
  list()

# R-Output wird aus Platzgr√ºnden nicht erneut dargestellt
```

### Datensatz

Die bisher erfolgreich ausgelesenen Informationen (@sec-schritte: ID, Link, Erscheinungsdatum, Anzahl der Kommentare, Ort der Berichterstattung, Titel, Zusammenfassung des Beitrages und Haupttext) k√∂nnen wir nun in einem Datensatz zusammenfassen (vgl. @tbl-idee). Hierf√ºr wiederholen wir pro forma und zwecks √úbersichtlichkeit alle bisherigen Arbeitsschritte und speichern jede einzelne Information als Objekt (`ID`, `link`, `datum`, `n_kommentare`, `ort`, `titel`, `zusammenfassung` und `text`):

```{r eval = F}
# Alle Informationen von der Webseite extrahieren

html <-
  read_html("https://www.news4teachers.de/2023/08/
            schaemt-euch-deutschland-steht-vor-den-vereinten-nationen-
            am-pranger-weil-es-die-inklusion-an-schulen-verweigert/")

# Achtung: Zeilenumbruch ist innerhalb des Links nicht erlaubt!
#          An dieser Stelle wurden Zeilenumbr√ºche nur aufgrund des
#          Platzmangels eingef√ºgt (zwecks Verbesserung der Darstellung).
#          Bitte entfernen Sie alle Zeilenumbr√ºche im Link, sodass der
#          Link bei Ihnen in einer Zeile erscheint.
```

```{r}
# ID

ID <-
  html |>
  html_elements("article") |>
  html_attr("id") |>
  str_remove("post-") |> # R-Zusatzpaket stringr (tidyverse)
  as.numeric()
```

```{r}
# Link

link <-
  html |>
  html_elements("article") |>
  html_attr("id") |>
  str_remove("post-") |> # R-Zusatzpaket stringr (tidyverse)
  as.numeric() %>% # Pipe-Operator, R-Zusatzpaket magrittr (tidyverse)
  paste0("https://www.news4teachers.de/?p=", .)
```

```{r}
# Erscheinungsdatum

datum <-
  html |>
  html_elements("article") |>
  html_elements("time") |>
  html_attr("datetime") |>
  str_sub(end = 10) # R-Zusatzpaket stringr (tidyverse)
```

```{r}
# Anzahl der Kommentare

n_kommentare <-
  html |>
  html_elements("div") |>
  html_elements(".wpd-thread-info") |>
  html_attr("data-comments-count") |>
  as.numeric()
```

```{r}
# Ort der Berichterstattung

ort <-
  html |>
  html_elements("article") |>
  html_element("p") |> # html_element anstatt html_elements
  html_text() |>
  str_extract("[^\\.]+") # R-Zusatzpaket stringr (tidyverse)
```

```{r}
# Titel

titel <-
  html |>
  html_elements("h1") |>
  html_text()
```

```{r}
# Zusammenfassung

zusammenfassung <-
  html |>
  html_elements("article") |>
  html_element("p") |> # html_element anstatt html_elements
  html_text() |>
  str_extract("\\.[\\s](.*)") |> # R-Zusatzpaket stringr (tidyverse)
  str_remove(". ")
```

```{r}
# Haupttext

all_p <-
  html |>
  html_elements(xpath = "//article/div/p") |>
  html_text()

spaces <-
  all_p |>
  str_detect("^\\s*$") # R-Zusatzpaket stringr (tidyverse)

text <-
  all_p[!spaces] %>% 
  .[-1] |>
  list()
```

Der Datensatz `n4t_data` wird mit dem Befehl `tibble()` erstellt (der Datensatz ist auch in @tbl-daten1 dargestellt):

```{r}
n4t_data <-
  tibble(ID, link, datum, n_kommentare, # R-Zusatzpaket tibble (tidyverse)
         ort, titel, zusammenfassung, text)

n4t_data
```

\blandscape

```{r echo = F}
#| label: tbl-daten1
#| tbl-cap: "Datensatz mit einem Beitrag"
n4t_data |>
  mutate(n_kommentare = as.character(n_kommentare)) |>
  mutate(ID = as.character(ID)) |>
  mutate(link = paste0(str_sub(link, start = 1, end = 8), "..."),
         titel = paste0(str_sub(titel, start = 1, end = 20), "..."),
         zusammenfassung =
           paste0(str_sub(zusammenfassung, start = 1, end = 20), "...")) |>
  flextable() |>
  bold(part = "header") |>
  rotate(rotation = "tbrl", align = "bottom", part = "header") |>
  height(height = 6, part = "header") |>
  width(j = 1:8, width = 1) |>
  footnote(i = 1, j = 8, value =
             as_paragraph(c("Abs√§tze des Haupttextes sind als Liste hinterlegt")),
           ref_symbols = c("#"), part = "body", inline = TRUE, sep = "")
```

\elandscape

Dieser Datensatz (@tbl-daten1) enth√§lt nun einen Beitrag von der Seite News4teachers. Wir wollen aber mehrere Beitr√§ge in einem Datensatz b√ºndeln. F√ºr den n√§chsten Beitrag mit dem Titel "*Sparpl√§ne: Inklusion h√§ngt in NRW am seidenen Faden - Lebenshilfe appelliert*"[^18] wiederholen wir daher die Arbeitsschritte der Extraktion:

[^18]: <https://www.news4teachers.de/2023/09/sparplaene-inklusion-haengt-in-nrw-am-seidenen-faden-lebenshilfe-appelliert/>

```{r echo=FALSE}
html <-
  read_html("https://www.news4teachers.de/2023/09/sparplaene-inklusion-haengt-in-nrw-am-seidenen-faden-lebenshilfe-appelliert/")
```

```{r eval = F}
# Alle Informationen von der Webseite extrahieren

html <-
  read_html("https://www.news4teachers.de/2023/09/
            sparplaene-inklusion-haengt-in-nrw-am-seidenen-faden-
            lebenshilfe-appelliert/")

# Achtung: Zeilenumbruch ist innerhalb des Links nicht erlaubt!
#          An dieser Stelle wurden Zeilenumbr√ºche nur aufgrund des
#          Platzmangels eingef√ºgt (zwecks Verbesserung der Darstellung).
#          Bitte entfernen Sie alle Zeilenumbr√ºche im Link, sodass der
#          Link bei Ihnen in einer Zeile erscheint.
```

```{r}
# ID

ID <-
  html |>
  html_elements("article") |>
  html_attr("id") |>
  str_remove("post-") |> # R-Zusatzpaket stringr (tidyverse)
  as.numeric()
```

```{r}
# Link

link <-
  html |>
  html_elements("article") |>
  html_attr("id") |>
  str_remove("post-") |> # R-Zusatzpaket stringr (tidyverse)
  as.numeric() %>% # Pipe-Operator, R-Zusatzpaket magrittr (tidyverse)
  paste0("https://www.news4teachers.de/?p=", .)
```

```{r}
# Erscheinungsdatum

datum <-
  html |>
  html_elements("article") |>
  html_elements("time") |>
  html_attr("datetime") |>
  str_sub(end = 10) # R-Zusatzpaket stringr (tidyverse)
```

```{r}
# Anzahl der Kommentare

n_kommentare <-
  html |>
  html_elements("div") |>
  html_elements(".wpd-thread-info") |>
  html_attr("data-comments-count") |>
  as.numeric()
```

```{r}
# Ort der Berichterstattung

ort <-
  html |>
  html_elements("article") |>
  html_element("p") |> # html_element anstatt html_elements
  html_text() |>
  str_extract("[^\\.]+") # R-Zusatzpaket stringr (tidyverse)
```

```{r}
# Titel

titel <-
  html |>
  html_elements("h1") |>
  html_text()
```

```{r}
# Zusammenfassung

zusammenfassung <-
  html |>
  html_elements("article") |>
  html_element("p") |> # html_element anstatt html_elements
  html_text() |>
  str_extract("\\.[\\s](.*)") |> # R-Zusatzpaket stringr (tidyverse)
  str_remove(". ")
```

```{r}
# Haupttext

all_p <-
  html |>
  html_elements(xpath = "//article/div/p") |>
  html_text()

spaces <-
  all_p |>
  str_detect("^\\s*$") # R-Zusatzpaket stringr (tidyverse)

text <-
  all_p[!spaces] %>% 
  .[-1] |>
  list()
```

Die Informationen aus dem neuen Beitrag werden dem bereits bestehenden Datensatz `n4t_data` mit dem Befehl `add_row()` hinzugef√ºgt:

```{r}
n4t_data <-
  n4t_data |>
  add_row(ID, link, datum, n_kommentare, ort, titel, zusammenfassung, text)
```

\blandscape

```{r echo = F}
#| label: tbl-daten2
#| tbl-cap: "Datensatz mit zwei Beitr√§gen"
n4t_data |>
  mutate(n_kommentare = as.character(n_kommentare)) |>
  mutate(ID = as.character(ID)) |>
  mutate(link = paste0(str_sub(link, start = 1, end = 8), "..."),
         titel = paste0(str_sub(titel, start = 1, end = 20), "..."),
         zusammenfassung =
           paste0(str_sub(zusammenfassung, start = 1, end = 20), "...")) |>
  flextable() |>
  bold(part = "header") |>
  rotate(rotation = "tbrl", align = "bottom", part = "header") |>
  height(height = 6, part = "header") |>
  width(j = 1:8, width = 1) |>
  footnote(i = 1:2, j = 8, value =
             as_paragraph(c("Abs√§tze des Haupttextes sind als Liste hinterlegt")),
           ref_symbols = c("#"), part = "body", inline = TRUE, sep = "")
```

\elandscape

Der neue Datensatz (@tbl-daten2) enth√§lt nun Informationen f√ºr zwei News4teachers-Beitr√§ge. Wir k√∂nnten die Arbeitsschritte der Extraktion nun immer wieder wiederholen und somit stetig neue Beitr√§ge zum Datensatz hinzuf√ºgen. Bei einer gro√üen Anzahl von Beitr√§gen w√§re dies allerdings ein sehr langwieriger Prozess. Daher m√ºssen wir f√ºr die Extraktion, also f√ºr das Web-Scraping der Beitr√§ge, eine automatisierte Routine entwickeln.

## Automatisiertes Web-Scraping

### Funktion

Beim Auslesen eines Beitrags von der Webseite News4teachers haben wir bisher stets folgende Arbeitsschritte get√§tigt (@sec-schritte):

1.  Alle Informationen von der Webseite (Beitragsseite) extrahieren
2.  ID auslesen
3.  Link auslesen
4.  Erscheinungsdatum auslesen
5.  Anzahl der Kommentare auslesen
6.  Ort der Berichterstattung auslesen
7.  Titel auslesen
8.  Zusammenfassung auslesen
9.  Haupttext auslesen
10. Alle Informationen (Arbeitsschritte 1 bis 9) in einem Datensatz zusammenfassen

Diese Arbeitsschritte k√∂nnen wir automatisieren, indem wir die jeweiligen Arbeitsschritte in einer Funktion b√ºndeln. Hierf√ºr schreiben wir eine eigene Funktion [@Wickham2023-hx, Kapitel 26]^[<https://r4ds.hadley.nz/function>]:

```{r}
web_scrape <-
  function(url = NULL) {
    
    # 1. Alle Informationen von der Webseite (Beitragsseite) extrahieren
    
    html <-
      read_html(url)
    
    # 2. ID auslesen
    
    ID <-
      html |>
      html_elements("article") |>
      html_attr("id") |>
      str_remove("post-") |> 
      as.numeric()
    
    # 3. Link auslesen
    
    link <-
      html |>
      html_elements("article") |>
      html_attr("id") |>
      str_remove("post-") |>
      as.numeric() %>%
      paste0("https://www.news4teachers.de/?p=", .)
    
    # 4. Erscheinungsdatum auslesen
    
    datum <-
      html |>
      html_elements("article") |>
      html_elements("time") |>
      html_attr("datetime") |>
      str_sub(end = 10)
    
    # 5. Anzahl der Kommentare auslesen
    
    n_kommentare <-
      html |>
      html_elements("div") |>
      html_elements(".wpd-thread-info") |>
      html_attr("data-comments-count") |>
      as.numeric()
    
    # 6. Ort der Berichterstattung auslesen
    
    ort <-
      html |>
      html_elements("article") |>
      html_element("p") |>
      html_text() |>
      str_extract("[^\\.]+")
    
    # 7. Titel auslesen
    
    titel <-
      html |>
      html_elements("h1") |>
      html_text()
    
    # 8. Zusammenfassung auslesen
    
    zusammenfassung <-
      html |>
      html_elements("article") |>
      html_element("p") |>
      html_text() |>
      str_extract("\\.[\\s](.*)") |>
      str_remove(". ")
    
    # 9. Haupttext auslesen

    all_p <-
      html |>
      html_elements(xpath = "//article/div/p") |>
      html_text()
    
    spaces <-
      all_p |>
      str_detect("^\\s*$")
    
    text <-
      all_p[!spaces] %>% 
      .[-1] |>
      list()
    
    # 10. Alle Informationen (Arbeitsschritte 1 bis 9)
    #     in einem Datensatz zusammenfassen
    
    tibble(ID, link, datum, n_kommentare, ort,
           titel, zusammenfassung, text)
  }
```

Wir k√∂nnen nun die neu erstellte Funktion `web_scrape()` nutzen und m√ºssen lediglich den Link eines Beitrags an die Funktion √ºbergeben. Dann werden die Arbeitsschritte 1 bis 10 ausgef√ºhrt und wir erhalten einen Datensatz f√ºr den neuen Beitrag (@tbl-daten3):

```{r eval = F}
web_scrape("https://www.news4teachers.de/2023/10/
           montessori-verband-zur-inklusionsdebatte-in-deutschland-ein-
           umdenken-ist-noetig/")

# Achtung: Zeilenumbruch ist innerhalb des Links nicht erlaubt!
#          An dieser Stelle wurden Zeilenumbr√ºche nur aufgrund des
#          Platzmangels eingef√ºgt (zwecks Verbesserung der Darstellung).
#          Bitte entfernen Sie alle Zeilenumbr√ºche im Link, sodass der
#          Link bei Ihnen in einer Zeile erscheint.
```

```{r echo = F}
web_scrape("https://www.news4teachers.de/2023/10/montessori-verband-zur-inklusionsdebatte-in-deutschland-ein-umdenken-ist-noetig/")
```

\blandscape

```{r echo = F}
#| label: tbl-daten3
#| tbl-cap: "Datensatz erstellt mit der Funktion `web_scrape()`"
web_scrape("https://www.news4teachers.de/2023/10/montessori-verband-zur-inklusionsdebatte-in-deutschland-ein-umdenken-ist-noetig/") |>
  mutate(n_kommentare = as.character(n_kommentare)) |>
  mutate(ID = as.character(ID)) |>
  mutate(link = paste0(str_sub(link, start = 1, end = 8), "..."),
         titel = paste0(str_sub(titel, start = 1, end = 20), "..."),
         zusammenfassung =
           paste0(str_sub(zusammenfassung, start = 1, end = 20), "...")) |>
  flextable() |>
  bold(part = "header") |>
  rotate(rotation = "tbrl", align = "bottom", part = "header") |>
  height(height = 6, part = "header") |>
  width(j = 1:8, width = 1) |>
  footnote(i = 1, j = 8, value =
             as_paragraph(c("Abs√§tze des Haupttextes sind als Liste hinterlegt")),
           ref_symbols = c("#"), part = "body", inline = TRUE, sep = "")
```

\elandscape

Ein mit der Funktion `web_scrape()` erzeugter Datensatz (@tbl-daten2) kann auch dem bereits erstellten Datensatz `n4t_data` (@tbl-daten3) hinzugef√ºgt werden (@tbl-daten4 = @tbl-daten2 + @tbl-daten3):

```{r eval = F}
n4t_data <-
  n4t_data |>
  bind_rows(web_scrape("https://www.news4teachers.de/2023/10/
                       montessori-verband-zur-inklusionsdebatte-in-
                       deutschland-ein-umdenken-ist-noetig/"))

# Achtung: Zeilenumbruch ist innerhalb des Links nicht erlaubt!
#          An dieser Stelle wurden Zeilenumbr√ºche nur aufgrund des
#          Platzmangels eingef√ºgt (zwecks Verbesserung der Darstellung).
#          Bitte entfernen Sie alle Zeilenumbr√ºche im Link, sodass der
#          Link bei Ihnen in einer Zeile erscheint.
```

```{r echo=FALSE}
n4t_data <-
  n4t_data |>
  bind_rows(web_scrape("https://www.news4teachers.de/2023/10/montessori-verband-zur-inklusionsdebatte-in-deutschland-ein-umdenken-ist-noetig/"))

n4t_data
```

\blandscape

```{r echo = F}
#| label: tbl-daten4
#| tbl-cap: "Datensatz mit drei Beitr√§gen"
n4t_data |>
  mutate(n_kommentare = as.character(n_kommentare)) |>
  mutate(ID = as.character(ID)) |>
  mutate(link = paste0(str_sub(link, start = 1, end = 8), "..."),
         titel = paste0(str_sub(titel, start = 1, end = 20), "..."),
         zusammenfassung =
           paste0(str_sub(zusammenfassung, start = 1, end = 20), "...")) |>
  flextable() |>
  bold(part = "header") |>
  rotate(rotation = "tbrl", align = "bottom", part = "header") |>
  height(height = 6, part = "header") |>
  width(j = 1:8, width = 1.1) |>
  footnote(i = 1:3, j = 8, value =
             as_paragraph(c("Abs√§tze des Haupttextes sind als Liste hinterlegt")),
           ref_symbols = c("#"), part = "body", inline = TRUE, sep = "")
```

\elandscape



# Weiteres

Selector Gadget

Urheber Gesetz

Politeness

Evtl. interessant f√ºr das Auslesen der Kommentare:

<https://www.news4teachers.de/2023/08/schaemt-euch-deutschland-steht-vor-den-vereinten-nationen-am-pranger-weil-es-die-inklusion-an-schulen-verweigert/feed/>

# Literatur
